{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q streamlit==1.46.0"
      ],
      "metadata": {
        "id": "gIhmNTJ5vGNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94384a84-bcea-4ecf-d079-50a00da1b51c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg3D5xQEvD3Y",
        "outputId": "475d0b63-9dbc-4b57-cd3b-c7dbfe4ac612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Optional\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import hashlib\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -------------------- Configuration --------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"ğŸ“§ Email Classification Explorer\",\n",
        "    page_icon=\"ğŸ“§\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS with Royal Green Theme\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* Main color palette */\n",
        "    :root {\n",
        "        --royal-dark: #003A00;\n",
        "        --royal-medium: #005500;\n",
        "        --royal-light: #007000;\n",
        "        --royal-pale: #228B22;\n",
        "        --royal-bg: #E8F5E8;\n",
        "        --royal-accent: #00AA00;\n",
        "    }\n",
        "\n",
        "    .main-header {\n",
        "        font-size: 2.5rem;\n",
        "        color: var(--royal-dark);\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "        text-shadow: 2px 2px 4px rgba(0, 58, 0, 0.2);\n",
        "        font-weight: 600;\n",
        "    }\n",
        "\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #005500 0%, #003A00 100%);\n",
        "        padding: 1.2rem;\n",
        "        border-radius: 12px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin: 0.5rem 0;\n",
        "        box-shadow: 0 4px 6px rgba(0, 58, 0, 0.3);\n",
        "        transition: transform 0.2s;\n",
        "    }\n",
        "\n",
        "    .metric-card:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: 0 6px 12px rgba(0, 58, 0, 0.4);\n",
        "    }\n",
        "\n",
        "    .stAlert > div {\n",
        "        background: linear-gradient(90deg, #228B22, #005500);\n",
        "        color: white;\n",
        "        border-radius: 8px;\n",
        "        border: none;\n",
        "    }\n",
        "\n",
        "    .email-result {\n",
        "        border-left: 4px solid var(--royal-medium);\n",
        "        padding: 1rem;\n",
        "        margin: 0.5rem 0;\n",
        "        background: var(--royal-bg);\n",
        "        border-radius: 0 10px 10px 0;\n",
        "        box-shadow: 0 2px 4px rgba(0, 58, 0, 0.1);\n",
        "    }\n",
        "\n",
        "    .similarity-score {\n",
        "        background: var(--royal-pale);\n",
        "        padding: 0.3rem 0.8rem;\n",
        "        border-radius: 20px;\n",
        "        font-weight: 600;\n",
        "        color: white;\n",
        "        font-size: 0.9rem;\n",
        "    }\n",
        "\n",
        "    /* Sidebar styling */\n",
        "    .css-1d391kg {\n",
        "        background-color: var(--royal-bg);\n",
        "    }\n",
        "\n",
        "    /* Button styling */\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(135deg, #005500 0%, #003A00 100%);\n",
        "        color: white;\n",
        "        border: none;\n",
        "        padding: 0.5rem 1rem;\n",
        "        font-weight: 500;\n",
        "        transition: all 0.3s;\n",
        "    }\n",
        "\n",
        "    .stButton > button:hover {\n",
        "        background: linear-gradient(135deg, #007000 0%, #005500 100%);\n",
        "        box-shadow: 0 4px 8px rgba(0, 58, 0, 0.3);\n",
        "    }\n",
        "\n",
        "    /* Success/Warning/Error messages */\n",
        "    .stSuccess {\n",
        "        background-color: var(--royal-pale);\n",
        "        color: white;\n",
        "        border-left: 4px solid var(--royal-medium);\n",
        "    }\n",
        "\n",
        "    .stWarning {\n",
        "        background-color: #F0E68C;\n",
        "        color: #5C4033;\n",
        "        border-left: 4px solid #DAA520;\n",
        "    }\n",
        "\n",
        "    .stError {\n",
        "        background-color: #FFE4E1;\n",
        "        color: #8B0000;\n",
        "        border-left: 4px solid #DC143C;\n",
        "    }\n",
        "\n",
        "    /* Tab styling */\n",
        "    .stTabs [data-baseweb=\"tab-list\"] {\n",
        "        background-color: var(--royal-bg);\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "\n",
        "    .stTabs [data-baseweb=\"tab\"] {\n",
        "        color: var(--royal-dark);\n",
        "        font-weight: 500;\n",
        "    }\n",
        "\n",
        "    .stTabs [aria-selected=\"true\"] {\n",
        "        background-color: var(--royal-medium);\n",
        "        color: white;\n",
        "        border-radius: 6px;\n",
        "    }\n",
        "\n",
        "    /* Expander styling */\n",
        "    .streamlit-expanderHeader {\n",
        "        background-color: var(--royal-pale);\n",
        "        color: white;\n",
        "        border-radius: 8px;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "\n",
        "    /* Input field styling */\n",
        "    .stTextInput > div > div > input,\n",
        "    .stTextArea > div > div > textarea {\n",
        "        border: 2px solid var(--royal-light);\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "\n",
        "    .stTextInput > div > div > input:focus,\n",
        "    .stTextArea > div > div > textarea:focus {\n",
        "        border-color: var(--royal-medium);\n",
        "        box-shadow: 0 0 0 3px rgba(0, 85, 0, 0.1);\n",
        "    }\n",
        "\n",
        "    /* Slider styling */\n",
        "    .stSlider > div > div > div {\n",
        "        background-color: var(--royal-light);\n",
        "    }\n",
        "\n",
        "    .stSlider > div > div > div > div {\n",
        "        background-color: var(--royal-medium);\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Royal Green color scheme for Plotly charts\n",
        "ROYAL_COLOR_SCALE = [\n",
        "    '#E8F5E8', '#A8D5A8', '#7FBF7F', '#228B22',\n",
        "    '#007000', '#005500', '#003A00', '#002600'\n",
        "]\n",
        "\n",
        "# -------------------- Cache Directory --------------------\n",
        "CACHE_DIR = Path(\"./.cache\")\n",
        "CACHE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main application function\"\"\"\n",
        "\n",
        "    # -------------------- Auto Download Sample Data --------------------\n",
        "    def install_and_import_gdown():\n",
        "        \"\"\"Install gdown if not available and import it\"\"\"\n",
        "        try:\n",
        "            import gdown\n",
        "            return gdown\n",
        "        except ImportError:\n",
        "            with st.spinner(\"ğŸ“¦ Installing gdown package...\"):\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"])\n",
        "            import gdown\n",
        "            return gdown\n",
        "\n",
        "    @st.cache_data\n",
        "    def download_sample_data():\n",
        "        \"\"\"Download sample data from Google Drive\"\"\"\n",
        "        sample_file_path = CACHE_DIR / \"sample_emails.csv\"\n",
        "\n",
        "        # Check if file already exists\n",
        "        if sample_file_path.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(sample_file_path)\n",
        "                df_processed = process_and_validate_dataframe(df)\n",
        "                if len(df_processed) > 0:\n",
        "                    return df_processed, \"âœ… Loaded cached sample data\"\n",
        "            except:\n",
        "                # If file is corrupted, download again\n",
        "                sample_file_path.unlink()\n",
        "\n",
        "        # Method 1: Try with gdown\n",
        "        try:\n",
        "            # Install gdown if needed (without caching this function)\n",
        "            try:\n",
        "                import gdown\n",
        "            except ImportError:\n",
        "                with st.spinner(\"ğŸ“¦ Installing gdown package...\"):\n",
        "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"])\n",
        "                import gdown\n",
        "\n",
        "            # Download file from Google Drive\n",
        "            file_id = \"1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R\"\n",
        "            url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "            with st.spinner(\"ğŸ“¥ Downloading sample data from Google Drive...\"):\n",
        "                gdown.download(url, str(sample_file_path), quiet=True)\n",
        "\n",
        "            # Load and process the downloaded file\n",
        "            df = pd.read_csv(sample_file_path)\n",
        "            df_processed = process_and_validate_dataframe(df)\n",
        "\n",
        "            if len(df_processed) > 0:\n",
        "                return df_processed, f\"âœ… Downloaded sample data: {len(df_processed)} emails\"\n",
        "            else:\n",
        "                raise ValueError(\"No valid data after processing\")\n",
        "\n",
        "        except Exception as gdown_error:\n",
        "            st.warning(f\"âš ï¸ gdown method failed: {str(gdown_error)}\")\n",
        "\n",
        "            # Method 2: Try with requests as fallback\n",
        "            try:\n",
        "                import requests\n",
        "\n",
        "                file_id = \"1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R\"\n",
        "                download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "                with st.spinner(\"ğŸ“¥ Trying alternative download method...\"):\n",
        "                    response = requests.get(download_url)\n",
        "                    response.raise_for_status()\n",
        "\n",
        "                    # Save the file\n",
        "                    with open(sample_file_path, 'wb') as f:\n",
        "                        f.write(response.content)\n",
        "\n",
        "                # Load and process\n",
        "                df = pd.read_csv(sample_file_path)\n",
        "                df_processed = process_and_validate_dataframe(df)\n",
        "\n",
        "                if len(df_processed) > 0:\n",
        "                    return df_processed, f\"âœ… Downloaded sample data (alternative method): {len(df_processed)} emails\"\n",
        "                else:\n",
        "                    raise ValueError(\"No valid data after processing\")\n",
        "\n",
        "            except Exception as requests_error:\n",
        "                st.error(f\"âŒ All download methods failed:\")\n",
        "                st.error(f\"â€¢ gdown error: {str(gdown_error)}\")\n",
        "                st.error(f\"â€¢ requests error: {str(requests_error)}\")\n",
        "                st.error(\"Please check your internet connection or the Google Drive file ID.\")\n",
        "                return pd.DataFrame(), \"âŒ Failed to load sample data\"\n",
        "\n",
        "    def process_and_validate_dataframe(df):\n",
        "        \"\"\"Process and validate dataframe, attempting to map columns to expected format\"\"\"\n",
        "        if df.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Debug: Show what we actually got\n",
        "        st.sidebar.markdown(\"### ğŸ” Debug Info\")\n",
        "        st.sidebar.write(\"**Downloaded columns:**\")\n",
        "        st.sidebar.write(list(df.columns))\n",
        "        st.sidebar.write(\"**Shape:**\", df.shape)\n",
        "        if len(df) > 0:\n",
        "            st.sidebar.write(\"**First few rows:**\")\n",
        "            st.sidebar.dataframe(df.head(3))\n",
        "\n",
        "        # Try to map columns intelligently\n",
        "        column_mapping = {}\n",
        "\n",
        "        # Look for ID column (optional)\n",
        "        id_candidates = ['id', 'ID', 'index', 'Index', 'message_id', 'email_id']\n",
        "        for col in df.columns:\n",
        "            if col.lower() in [c.lower() for c in id_candidates]:\n",
        "                column_mapping['id'] = col\n",
        "                break\n",
        "\n",
        "        # Look for email/text/message column\n",
        "        email_candidates = ['message', 'Message', 'email', 'Email', 'text', 'Text', 'content', 'Content', 'body', 'Body']\n",
        "        for col in df.columns:\n",
        "            if col.lower() in [c.lower() for c in email_candidates]:\n",
        "                column_mapping['email'] = col\n",
        "                break\n",
        "\n",
        "        # Look for label/category column\n",
        "        label_candidates = ['category', 'Category', 'label', 'Label', 'class', 'Class', 'type', 'Type', 'spam', 'Spam']\n",
        "        for col in df.columns:\n",
        "            if col.lower() in [c.lower() for c in label_candidates]:\n",
        "                column_mapping['label'] = col\n",
        "                break\n",
        "\n",
        "        # Show mapping results\n",
        "        st.sidebar.write(\"**Column mapping found:**\")\n",
        "        st.sidebar.write(column_mapping)\n",
        "\n",
        "        # Handle different scenarios based on available columns\n",
        "        processed_df = None\n",
        "\n",
        "        if len(df.columns) == 2:\n",
        "            # Perfect for Category, Message format\n",
        "            if 'email' in column_mapping and 'label' in column_mapping:\n",
        "                # We found both email and label columns\n",
        "                processed_df = pd.DataFrame({\n",
        "                    'id': range(1, len(df) + 1),\n",
        "                    'email': df[column_mapping['email']],\n",
        "                    'label': df[column_mapping['label']]\n",
        "                })\n",
        "                st.sidebar.success(\"âœ… Perfect! Auto-detected Category/Message format\")\n",
        "            else:\n",
        "                # Fallback: assume first column is label, second is email\n",
        "                processed_df = pd.DataFrame({\n",
        "                    'id': range(1, len(df) + 1),\n",
        "                    'email': df.iloc[:, 1],  # Second column as email/message\n",
        "                    'label': df.iloc[:, 0]   # First column as label/category\n",
        "                })\n",
        "                st.sidebar.warning(\"âš ï¸ Using columns as: Category (col 1) â†’ label, Message (col 2) â†’ email\")\n",
        "\n",
        "        elif len(df.columns) >= 3:\n",
        "            # 3+ columns: try to use mapping, fallback to first 3\n",
        "            if all(key in column_mapping for key in ['id', 'email', 'label']):\n",
        "                processed_df = pd.DataFrame({\n",
        "                    'id': df[column_mapping['id']],\n",
        "                    'email': df[column_mapping['email']],\n",
        "                    'label': df[column_mapping['label']]\n",
        "                })\n",
        "                st.sidebar.success(\"âœ… Found all required columns with mapping\")\n",
        "            elif 'email' in column_mapping and 'label' in column_mapping:\n",
        "                # Found email and label, generate ID\n",
        "                processed_df = pd.DataFrame({\n",
        "                    'id': range(1, len(df) + 1),\n",
        "                    'email': df[column_mapping['email']],\n",
        "                    'label': df[column_mapping['label']]\n",
        "                })\n",
        "                st.sidebar.success(\"âœ… Found email and label columns, generated IDs\")\n",
        "            else:\n",
        "                # Use first 3 columns as fallback\n",
        "                processed_df = pd.DataFrame({\n",
        "                    'id': df.iloc[:, 0],\n",
        "                    'email': df.iloc[:, 1],\n",
        "                    'label': df.iloc[:, 2]\n",
        "                })\n",
        "                st.sidebar.warning(\"âš ï¸ Using first 3 columns as id, email, label\")\n",
        "\n",
        "        else:\n",
        "            st.error(f\"âŒ Cannot process file with only {len(df.columns)} column(s)\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Clean and validate the data\n",
        "        if processed_df is not None:\n",
        "            try:\n",
        "                # Remove rows with missing values\n",
        "                initial_count = len(processed_df)\n",
        "                processed_df = processed_df.dropna()\n",
        "\n",
        "                # Convert to string and remove very short messages\n",
        "                processed_df['email'] = processed_df['email'].astype(str)\n",
        "                processed_df['label'] = processed_df['label'].astype(str)\n",
        "                processed_df = processed_df[processed_df['email'].str.len() > 5]  # At least 5 characters\n",
        "\n",
        "                # Reset IDs to be sequential\n",
        "                processed_df['id'] = range(1, len(processed_df) + 1)\n",
        "\n",
        "                # Show cleaning results\n",
        "                final_count = len(processed_df)\n",
        "                if final_count < initial_count:\n",
        "                    st.sidebar.info(f\"ğŸ§¹ Cleaned data: {initial_count} â†’ {final_count} rows\")\n",
        "\n",
        "                # Show sample of processed data\n",
        "                st.sidebar.write(\"**Processed sample:**\")\n",
        "                st.sidebar.dataframe(processed_df.head(3))\n",
        "\n",
        "                # Show label distribution\n",
        "                if len(processed_df) > 0:\n",
        "                    label_counts = processed_df['label'].value_counts()\n",
        "                    st.sidebar.write(\"**Label distribution:**\")\n",
        "                    st.sidebar.write(label_counts.to_dict())\n",
        "\n",
        "                st.sidebar.success(f\"âœ… Successfully processed {len(processed_df)} emails\")\n",
        "\n",
        "                return processed_df\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"âŒ Error processing dataframe: {str(e)}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # -------------------- Sidebar Configuration --------------------\n",
        "    st.sidebar.markdown(\"## âš™ï¸ Configuration\")\n",
        "    BATCH_SIZE = st.sidebar.number_input(\"Batch size\", min_value=1, max_value=100, value=10, help=\"Number of texts to process at once\")\n",
        "    CHUNK_SIZE = st.sidebar.number_input(\"Chunk size (tokens)\", min_value=100, max_value=2000, value=1000, help=\"Maximum tokens per chunk\")\n",
        "    OVERLAP = st.sidebar.number_input(\"Chunk overlap (tokens)\", min_value=0, max_value=500, value=200, help=\"Token overlap between chunks\")\n",
        "    DIM_REDUCTION = st.sidebar.selectbox(\"Dimensionality Reduction\", [\"PCA\", \"t-SNE\", \"UMAP\"], help=\"Method for visualizing embeddings\")\n",
        "    VISUALIZATION_3D = st.sidebar.checkbox(\"3D Visualization\", value=False, help=\"Enable 3D visualization for embeddings\")\n",
        "\n",
        "    st.sidebar.markdown(\"---\")\n",
        "    st.sidebar.markdown(\"## ğŸ“Š Model Info\")\n",
        "    EMBED_MODEL = \"text-embedding-ada-002 (Simulated)\"\n",
        "    st.sidebar.info(f\"**Model:** {EMBED_MODEL}\")\n",
        "\n",
        "    # -------------------- Utility Functions --------------------\n",
        "    @st.cache_data\n",
        "    def chunk_text(text: str, max_tokens: int, overlap: int) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        while start < len(words):\n",
        "            end = min(len(words), start + max_tokens)\n",
        "            chunk = words[start:end]\n",
        "            chunks.append(\" \".join(chunk))\n",
        "            if end >= len(words):\n",
        "                break\n",
        "            start += max_tokens - overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def simple_hash_embedding(text: str, dimensions: int = 384) -> np.ndarray:\n",
        "        \"\"\"Create a simple hash-based embedding for demonstration\"\"\"\n",
        "        # Create a hash of the text\n",
        "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
        "\n",
        "        # Convert hash to numbers and create embedding\n",
        "        embedding = []\n",
        "        for i in range(0, len(text_hash), 2):\n",
        "            hex_pair = text_hash[i:i+2]\n",
        "            embedding.append(int(hex_pair, 16) / 255.0)\n",
        "\n",
        "        # Pad or truncate to desired dimensions\n",
        "        while len(embedding) < dimensions:\n",
        "            embedding.extend(embedding[:min(len(embedding), dimensions - len(embedding))])\n",
        "\n",
        "        embedding = np.array(embedding[:dimensions])\n",
        "\n",
        "        # Add some randomness based on word count and common words\n",
        "        words = text.lower().split()\n",
        "        word_features = np.zeros(dimensions)\n",
        "\n",
        "        for i, word in enumerate(words[:50]):  # Use first 50 words\n",
        "            word_hash = abs(hash(word)) % dimensions\n",
        "            word_features[word_hash] += 1.0 / (i + 1)\n",
        "\n",
        "        # Combine hash and word features\n",
        "        final_embedding = 0.7 * embedding + 0.3 * word_features\n",
        "\n",
        "        # Normalize\n",
        "        norm = np.linalg.norm(final_embedding)\n",
        "        if norm > 0:\n",
        "            final_embedding = final_embedding / norm\n",
        "\n",
        "        return final_embedding\n",
        "\n",
        "    def perform_dimensionality_reduction(embeddings_array, method=\"PCA\", n_components=2):\n",
        "        \"\"\"Perform dimensionality reduction on embeddings\"\"\"\n",
        "        if method == \"PCA\":\n",
        "            from sklearn.decomposition import PCA\n",
        "            reducer = PCA(n_components=n_components, random_state=42)\n",
        "        elif method == \"t-SNE\":\n",
        "            from sklearn.manifold import TSNE\n",
        "            perplexity = min(30, len(embeddings_array) - 1)\n",
        "            reducer = TSNE(n_components=n_components, random_state=42, perplexity=perplexity)\n",
        "        elif method == \"UMAP\":\n",
        "            try:\n",
        "                import umap\n",
        "                reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
        "            except ImportError:\n",
        "                st.warning(\"âš ï¸ UMAP not installed, falling back to PCA\")\n",
        "                reducer = PCA(n_components=n_components, random_state=42)\n",
        "\n",
        "        return reducer.fit_transform(embeddings_array)\n",
        "\n",
        "    def create_smart_classifier(df_used, embeddings):\n",
        "        \"\"\"Create a smart classifier based on embeddings and labels from the actual used dataframe\"\"\"\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "        # Ensure we're using the same dataframe that was used to create embeddings\n",
        "        if len(df_used) != len(embeddings):\n",
        "            raise ValueError(f\"Mismatch: DataFrame has {len(df_used)} rows but {len(embeddings)} embeddings\")\n",
        "\n",
        "        # Encode labels\n",
        "        label_encoder = LabelEncoder()\n",
        "        encoded_labels = label_encoder.fit_transform(df_used['label'])\n",
        "\n",
        "        # Split data for training\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            embeddings, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
        "        )\n",
        "\n",
        "        # Train classifier\n",
        "        classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        return classifier, label_encoder, X_test, y_test\n",
        "\n",
        "    def predict_email_class(email_text, classifier, label_encoder, get_email_vector_func):\n",
        "        \"\"\"Predict class for a single email\"\"\"\n",
        "        # Get embedding for the email\n",
        "        email_vector = get_email_vector_func(email_text).reshape(1, -1)\n",
        "\n",
        "        # Predict\n",
        "        prediction_encoded = classifier.predict(email_vector)[0]\n",
        "        prediction_proba = classifier.predict_proba(email_vector)[0]\n",
        "\n",
        "        # Decode prediction\n",
        "        prediction_label = label_encoder.inverse_transform([prediction_encoded])[0]\n",
        "\n",
        "        # Get confidence scores for all classes\n",
        "        class_probabilities = {}\n",
        "        for i, class_name in enumerate(label_encoder.classes_):\n",
        "            class_probabilities[class_name] = prediction_proba[i]\n",
        "\n",
        "        return prediction_label, class_probabilities\n",
        "\n",
        "    def is_spam_classifier(email_text, df_used, embeddings, get_email_vector_func):\n",
        "        \"\"\"Special spam/ham binary classifier using the actual used dataframe\"\"\"\n",
        "        # Ensure we're using the same dataframe that was used to create embeddings\n",
        "        if len(df_used) != len(embeddings):\n",
        "            raise ValueError(f\"Mismatch: DataFrame has {len(df_used)} rows but {len(embeddings)} embeddings\")\n",
        "\n",
        "        # Create binary labels (spam vs non-spam)\n",
        "        binary_labels = ['spam' if label.lower() == 'spam' else 'ham' for label in df_used['label']]\n",
        "\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        import re\n",
        "\n",
        "        # Advanced spam detection features\n",
        "        def extract_spam_features(text):\n",
        "            features = []\n",
        "            text_lower = text.lower()\n",
        "\n",
        "            # 1. Capital letters ratio\n",
        "            if len(text) > 0:\n",
        "                caps_ratio = len([c for c in text if c.isupper()]) / len(text)\n",
        "                features.append(caps_ratio)\n",
        "            else:\n",
        "                features.append(0)\n",
        "\n",
        "            # 2. Exclamation marks count\n",
        "            features.append(text.count('!'))\n",
        "\n",
        "            # 3. Question marks count\n",
        "            features.append(text.count('?'))\n",
        "\n",
        "            # 4. Dollar signs count\n",
        "            features.append(text.count('$'))\n",
        "\n",
        "            # 5. URLs count\n",
        "            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "            urls = re.findall(url_pattern, text)\n",
        "            features.append(len(urls))\n",
        "\n",
        "            # 6. Email addresses count\n",
        "            email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "            emails = re.findall(email_pattern, text)\n",
        "            features.append(len(emails))\n",
        "\n",
        "            # 7. Phone numbers count\n",
        "            phone_pattern = r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'\n",
        "            phones = re.findall(phone_pattern, text)\n",
        "            features.append(len(phones))\n",
        "\n",
        "            # 8. Spam keywords score\n",
        "            spam_keywords = [\n",
        "                'win', 'winner', 'winning', 'won', 'prize', 'congratulations',\n",
        "                'click here', 'click now', 'urgent', 'limited time', 'act now',\n",
        "                'free', 'money', 'cash', 'discount', 'offer', 'deal', 'guarantee',\n",
        "                'viagra', 'pills', 'medication', 'weight loss', 'lose weight',\n",
        "                'million', 'thousand', 'hundred', 'dollars', 'pounds', 'euros',\n",
        "                'dear customer', 'dear friend', 'dear sir', 'dear madam',\n",
        "                'unsubscribe', 'remove', 'stop receiving', 'opt out',\n",
        "                'call now', 'call today', 'don\\'t hesitate', 'apply now',\n",
        "                'special offer', 'limited offer', 'exclusive offer', 'best price',\n",
        "                'no cost', 'no fees', 'no charges', 'no obligation',\n",
        "                'order now', 'buy now', 'purchase now', 'shop now',\n",
        "                'claim', 'redeem', 'collect', 'receive',\n",
        "                'guaranteed', '100%', 'risk free', 'satisfaction',\n",
        "                'increase', 'boost', 'enhance', 'improve',\n",
        "                'credit', 'loan', 'debt', 'mortgage'\n",
        "            ]\n",
        "\n",
        "            spam_score = sum(1 for keyword in spam_keywords if keyword in text_lower)\n",
        "            features.append(spam_score)\n",
        "\n",
        "            # 9. Length of text\n",
        "            features.append(len(text))\n",
        "\n",
        "            # 10. Number of words\n",
        "            features.append(len(text.split()))\n",
        "\n",
        "            # 11. Average word length\n",
        "            words = text.split()\n",
        "            if words:\n",
        "                avg_word_length = sum(len(word) for word in words) / len(words)\n",
        "                features.append(avg_word_length)\n",
        "            else:\n",
        "                features.append(0)\n",
        "\n",
        "            # 12. Numeric characters ratio\n",
        "            if len(text) > 0:\n",
        "                numeric_ratio = len([c for c in text if c.isdigit()]) / len(text)\n",
        "                features.append(numeric_ratio)\n",
        "            else:\n",
        "                features.append(0)\n",
        "\n",
        "            return np.array(features)\n",
        "\n",
        "        # Extract additional features for all emails\n",
        "        additional_features = []\n",
        "        for email in df_used['email']:\n",
        "            additional_features.append(extract_spam_features(email))\n",
        "        additional_features = np.array(additional_features)\n",
        "\n",
        "        # Extract features for the input email\n",
        "        input_additional_features = extract_spam_features(email_text)\n",
        "\n",
        "        # Combine embeddings with additional features\n",
        "        combined_features = np.hstack([embeddings, additional_features])\n",
        "        input_combined_features = np.hstack([\n",
        "            get_email_vector_func(email_text),\n",
        "            input_additional_features\n",
        "        ])\n",
        "\n",
        "        # Encode binary labels\n",
        "        label_encoder = LabelEncoder()\n",
        "        encoded_labels = label_encoder.fit_transform(binary_labels)\n",
        "\n",
        "        # Train binary classifier with combined features\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            combined_features, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
        "        )\n",
        "\n",
        "        classifier = RandomForestClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=20,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=42,\n",
        "            class_weight='balanced'  # Handle imbalanced classes\n",
        "        )\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # Predict for input email\n",
        "        input_combined_features = input_combined_features.reshape(1, -1)\n",
        "        prediction_encoded = classifier.predict(input_combined_features)[0]\n",
        "        prediction_proba = classifier.predict_proba(input_combined_features)[0]\n",
        "\n",
        "        prediction_label = label_encoder.inverse_transform([prediction_encoded])[0]\n",
        "        confidence = max(prediction_proba)\n",
        "\n",
        "        # Additional rule-based check for obvious spam\n",
        "        spam_score = extract_spam_features(email_text)[7]  # Get spam keywords score\n",
        "        if spam_score >= 5 and prediction_label == 'ham':\n",
        "            # Override if too many spam keywords\n",
        "            prediction_label = 'spam'\n",
        "            confidence = min(0.9, confidence + 0.2)\n",
        "\n",
        "        return prediction_label, confidence, classifier.score(X_test, y_test)\n",
        "\n",
        "    def plot_metrics_dashboard(y_true, y_pred, label_encoder):\n",
        "        \"\"\"Create comprehensive metrics dashboard\"\"\"\n",
        "        # Get the unique labels from label encoder\n",
        "        labels = label_encoder.classes_\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision, recall, f1_score, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, labels=np.arange(len(labels)), average=None\n",
        "        )\n",
        "\n",
        "        # Create metrics dataframe\n",
        "        metrics_df = pd.DataFrame({\n",
        "            'Label': labels,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-Score': f1_score,\n",
        "            'Support': support\n",
        "        })\n",
        "\n",
        "        # Create visualizations with royal green color scheme\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            # Metrics bar chart\n",
        "            fig_metrics = go.Figure()\n",
        "\n",
        "            fig_metrics.add_trace(go.Bar(\n",
        "                name='Precision',\n",
        "                x=labels,\n",
        "                y=precision,\n",
        "                marker_color='#228B22'\n",
        "            ))\n",
        "\n",
        "            fig_metrics.add_trace(go.Bar(\n",
        "                name='Recall',\n",
        "                x=labels,\n",
        "                y=recall,\n",
        "                marker_color='#005500'\n",
        "            ))\n",
        "\n",
        "            fig_metrics.add_trace(go.Bar(\n",
        "                name='F1-Score',\n",
        "                x=labels,\n",
        "                y=f1_score,\n",
        "                marker_color='#003A00'\n",
        "            ))\n",
        "\n",
        "            fig_metrics.update_layout(\n",
        "                title='Classification Metrics by Label',\n",
        "                xaxis_title='Labels',\n",
        "                yaxis_title='Score',\n",
        "                barmode='group',\n",
        "                height=400,\n",
        "                plot_bgcolor='#E8F5E8',\n",
        "                paper_bgcolor='#E8F5E8'\n",
        "            )\n",
        "\n",
        "            st.plotly_chart(fig_metrics, use_container_width=True)\n",
        "\n",
        "        with col2:\n",
        "            # Support pie chart\n",
        "            fig_support = px.pie(\n",
        "                values=support,\n",
        "                names=labels,\n",
        "                title='Support Distribution (Number of Samples)',\n",
        "                color_discrete_sequence=ROYAL_COLOR_SCALE[2:]\n",
        "            )\n",
        "            fig_support.update_layout(\n",
        "                height=400,\n",
        "                plot_bgcolor='#E8F5E8',\n",
        "                paper_bgcolor='#E8F5E8'\n",
        "            )\n",
        "            st.plotly_chart(fig_support, use_container_width=True)\n",
        "\n",
        "        # Metrics table\n",
        "        st.markdown(\"#### ğŸ“Š Detailed Metrics Table\")\n",
        "        st.dataframe(metrics_df.round(4), use_container_width=True)\n",
        "\n",
        "        # Overall metrics\n",
        "        overall_precision = np.average(precision, weights=support)\n",
        "        overall_recall = np.average(recall, weights=support)\n",
        "        overall_f1 = np.average(f1_score, weights=support)\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        with col1:\n",
        "            st.metric(\"Overall Precision\", f\"{overall_precision:.4f}\")\n",
        "        with col2:\n",
        "            st.metric(\"Overall Recall\", f\"{overall_recall:.4f}\")\n",
        "        with col3:\n",
        "            st.metric(\"Overall F1-Score\", f\"{overall_f1:.4f}\")\n",
        "\n",
        "        return metrics_df\n",
        "\n",
        "    def plot_confusion_matrix(y_true, y_pred, label_encoder):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        # Get label names\n",
        "        labels = label_encoder.classes_\n",
        "\n",
        "        # Create confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        # Create heatmap with royal green color scheme\n",
        "        fig = go.Figure(data=go.Heatmap(\n",
        "            z=cm,\n",
        "            x=labels,\n",
        "            y=labels,\n",
        "            colorscale=[[0, '#E8F5E8'], [0.5, '#228B22'], [1, '#003A00']],\n",
        "            text=cm,\n",
        "            texttemplate='%{text}',\n",
        "            textfont={\"size\": 12},\n",
        "            hovertemplate='True: %{y}<br>Predicted: %{x}<br>Count: %{z}<extra></extra>'\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Confusion Matrix',\n",
        "            xaxis_title='Predicted Label',\n",
        "            yaxis_title='True Label',\n",
        "            xaxis={'side': 'bottom'},\n",
        "            width=600,\n",
        "            height=500,\n",
        "            plot_bgcolor='#E8F5E8',\n",
        "            paper_bgcolor='#E8F5E8'\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    @st.cache_data\n",
        "    def get_email_vector(email: str, chunk_size: int, overlap: int) -> np.ndarray:\n",
        "        \"\"\"Get embedding vector for email\"\"\"\n",
        "        words = email.split()\n",
        "\n",
        "        if len(words) > chunk_size:\n",
        "            chunks = chunk_text(email, chunk_size, overlap)\n",
        "            vectors = [simple_hash_embedding(chunk) for chunk in chunks]\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return simple_hash_embedding(email)\n",
        "\n",
        "    # -------------------- Main App --------------------\n",
        "    st.markdown('<h1 class=\"main-header\">ğŸ‘‘ Email Classification & Embedding Explorer</h1>', unsafe_allow_html=True)\n",
        "\n",
        "    # Auto-load sample data on first run\n",
        "    if 'auto_data_loaded' not in st.session_state:\n",
        "        st.session_state['auto_data_loaded'] = False\n",
        "\n",
        "    if not st.session_state['auto_data_loaded']:\n",
        "        try:\n",
        "            sample_df, message = download_sample_data()\n",
        "            if len(sample_df) > 0:  # Only set if we actually got data\n",
        "                st.session_state['df'] = sample_df\n",
        "                st.session_state['sample_data_message'] = message\n",
        "            else:\n",
        "                st.session_state['sample_data_message'] = \"âŒ No data loaded - please upload a CSV file\"\n",
        "            st.session_state['auto_data_loaded'] = True\n",
        "        except Exception as e:\n",
        "            st.session_state['sample_data_message'] = f\"âŒ Error loading sample data: {str(e)}\"\n",
        "            st.session_state['auto_data_loaded'] = True  # Prevent infinite retry\n",
        "\n",
        "    # Show auto-load status\n",
        "    if 'sample_data_message' in st.session_state:\n",
        "        if \"âœ…\" in st.session_state['sample_data_message']:\n",
        "            st.success(st.session_state['sample_data_message'])\n",
        "        elif \"âš ï¸\" in st.session_state['sample_data_message']:\n",
        "            st.warning(st.session_state['sample_data_message'])\n",
        "        else:\n",
        "            st.info(st.session_state['sample_data_message'])\n",
        "\n",
        "    # File upload section\n",
        "    col1, col2, col3 = st.columns([2, 1, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"### ğŸ“ Upload Additional Data\")\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Upload your CSV file to add more data\",\n",
        "            type=['csv'],\n",
        "            help=\"CSV should contain columns: id, email, label. This will be added to existing data.\"\n",
        "        )\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"### ğŸ”„ Data Management\")\n",
        "        if st.button(\"ğŸ”„ Reload Sample Data\"):\n",
        "            # Force reload sample data\n",
        "            st.session_state['auto_data_loaded'] = False\n",
        "            try:\n",
        "                sample_df, message = download_sample_data()\n",
        "                if len(sample_df) > 0:\n",
        "                    st.session_state['df'] = sample_df\n",
        "                    st.session_state['auto_data_loaded'] = True\n",
        "                    st.success(\"âœ… Sample data reloaded!\")\n",
        "                else:\n",
        "                    st.error(\"âŒ Failed to reload sample data\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"âŒ Reload error: {str(e)}\")\n",
        "            st.rerun()\n",
        "\n",
        "    with col3:\n",
        "        st.markdown(\"### ğŸ—‘ï¸ Reset\")\n",
        "        if st.button(\"ğŸ—‘ï¸ Clear All Data\"):\n",
        "            # Clear all data and reload sample\n",
        "            for key in ['df', 'embeddings', 'email_data']:\n",
        "                if key in st.session_state:\n",
        "                    del st.session_state[key]\n",
        "            st.session_state['auto_data_loaded'] = False\n",
        "            st.success(\"âœ… Data cleared!\")\n",
        "            st.rerun()\n",
        "\n",
        "    # Handle file upload (append to existing data)\n",
        "    df = None\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            new_df = pd.read_csv(uploaded_file)\n",
        "\n",
        "            # Process and validate uploaded file\n",
        "            processed_new_df = process_and_validate_dataframe(new_df)\n",
        "\n",
        "            if len(processed_new_df) == 0:\n",
        "                st.error(\"âŒ Uploaded file contains no valid data after processing\")\n",
        "            else:\n",
        "                # Get existing data\n",
        "                existing_df = st.session_state.get('df', pd.DataFrame())\n",
        "\n",
        "                if len(existing_df) > 0:\n",
        "                    # Append new data to existing\n",
        "                    # Adjust IDs to avoid conflicts\n",
        "                    max_existing_id = existing_df['id'].max() if len(existing_df) > 0 else 0\n",
        "                    processed_new_df['id'] = processed_new_df['id'] + max_existing_id\n",
        "\n",
        "                    # Combine dataframes\n",
        "                    combined_df = pd.concat([existing_df, processed_new_df], ignore_index=True)\n",
        "                    st.session_state['df'] = combined_df\n",
        "                    st.success(f\"âœ… Added {len(processed_new_df)} new emails! Total: {len(combined_df)} emails\")\n",
        "                else:\n",
        "                    st.session_state['df'] = processed_new_df\n",
        "                    st.success(f\"âœ… Loaded {len(processed_new_df)} emails!\")\n",
        "\n",
        "                # Clear embeddings since data changed\n",
        "                if 'embeddings' in st.session_state:\n",
        "                    del st.session_state['embeddings']\n",
        "                if 'email_data' in st.session_state:\n",
        "                    del st.session_state['email_data']\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"âŒ Error loading file: {str(e)}\")\n",
        "            st.error(\"Please check that your file is a valid CSV\")\n",
        "\n",
        "    # Get current dataframe\n",
        "    if 'df' in st.session_state:\n",
        "        df = st.session_state['df']\n",
        "\n",
        "    # Display data overview\n",
        "    if df is not None and len(df) > 0:\n",
        "        st.markdown(\"### ğŸ“Š Data Overview\")\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(f'<div class=\"metric-card\"><h3>{len(df)}</h3><p>Total Emails</p></div>', unsafe_allow_html=True)\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(f'<div class=\"metric-card\"><h3>{df[\"label\"].nunique()}</h3><p>Unique Labels</p></div>', unsafe_allow_html=True)\n",
        "\n",
        "        with col3:\n",
        "            avg_length = df[\"email\"].str.split().str.len().mean()\n",
        "            st.markdown(f'<div class=\"metric-card\"><h3>{avg_length:.0f}</h3><p>Avg Words</p></div>', unsafe_allow_html=True)\n",
        "\n",
        "        with col4:\n",
        "            max_length = df[\"email\"].str.split().str.len().max()\n",
        "            st.markdown(f'<div class=\"metric-card\"><h3>{max_length}</h3><p>Max Words</p></div>', unsafe_allow_html=True)\n",
        "\n",
        "        # Label distribution\n",
        "        st.markdown(\"### ğŸ·ï¸ Label Distribution\")\n",
        "        label_counts = df[\"label\"].value_counts()\n",
        "\n",
        "        col1, col2 = st.columns([1, 1])\n",
        "\n",
        "        with col1:\n",
        "            fig_bar = px.bar(\n",
        "                x=label_counts.index,\n",
        "                y=label_counts.values,\n",
        "                title=\"Email Count by Label\",\n",
        "                color=label_counts.values,\n",
        "                color_continuous_scale=ROYAL_COLOR_SCALE\n",
        "            )\n",
        "            fig_bar.update_layout(\n",
        "                showlegend=False,\n",
        "                xaxis_title=\"Label\",\n",
        "                yaxis_title=\"Count\",\n",
        "                plot_bgcolor='#E8F5E8',\n",
        "                paper_bgcolor='#E8F5E8'\n",
        "            )\n",
        "            st.plotly_chart(fig_bar, use_container_width=True)\n",
        "\n",
        "        with col2:\n",
        "            fig_pie = px.pie(\n",
        "                values=label_counts.values,\n",
        "                names=label_counts.index,\n",
        "                title=\"Label Distribution\",\n",
        "                color_discrete_sequence=ROYAL_COLOR_SCALE[2:]\n",
        "            )\n",
        "            fig_pie.update_layout(\n",
        "                plot_bgcolor='#E8F5E8',\n",
        "                paper_bgcolor='#E8F5E8'\n",
        "            )\n",
        "            st.plotly_chart(fig_pie, use_container_width=True)\n",
        "\n",
        "        # Email length analysis\n",
        "        st.markdown(\"### ğŸ“ Email Length Analysis\")\n",
        "        lengths = df[\"email\"].str.split().str.len()\n",
        "\n",
        "        fig_hist = px.histogram(\n",
        "            x=lengths,\n",
        "            nbins=20,\n",
        "            title=\"Distribution of Email Lengths (Words)\",\n",
        "            color_discrete_sequence=[\"#005500\"]\n",
        "        )\n",
        "        fig_hist.update_layout(\n",
        "            xaxis_title=\"Number of Words\",\n",
        "            yaxis_title=\"Frequency\",\n",
        "            plot_bgcolor='#E8F5E8',\n",
        "            paper_bgcolor='#E8F5E8'\n",
        "        )\n",
        "        st.plotly_chart(fig_hist, use_container_width=True)\n",
        "\n",
        "        # Build embeddings\n",
        "        st.markdown(\"### ğŸ§  Build Embeddings\")\n",
        "\n",
        "        # Add sampling option for large datasets\n",
        "        if len(df) > 1000:\n",
        "            st.warning(f\"âš ï¸ Large dataset detected ({len(df)} emails). Consider sampling for faster processing.\")\n",
        "\n",
        "            col_sample1, col_sample2 = st.columns(2)\n",
        "            with col_sample1:\n",
        "                use_sampling = st.checkbox(\"Enable sampling for demo\", value=False)\n",
        "            with col_sample2:\n",
        "                if use_sampling:\n",
        "                    sample_size = st.slider(\"Sample size\", min_value=100, max_value=len(df), value=min(len(df), 5000))\n",
        "        else:\n",
        "            use_sampling = False\n",
        "            sample_size = len(df)\n",
        "\n",
        "        col1, col2 = st.columns([1, 1])\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"ğŸš€ Build Embeddings\", type=\"primary\"):\n",
        "                # Prepare data\n",
        "                if use_sampling and len(df) > sample_size:\n",
        "                    # Stratified sampling to maintain label distribution\n",
        "                    df_sample = df.groupby('label', group_keys=False).apply(\n",
        "                        lambda x: x.sample(min(len(x), sample_size // df['label'].nunique()))\n",
        "                    ).reset_index(drop=True)\n",
        "                    st.info(f\"ğŸ¯ Using stratified sample: {len(df_sample)} emails from {len(df)} total\")\n",
        "                else:\n",
        "                    df_sample = df.copy()\n",
        "\n",
        "                with st.spinner(\"Building embeddings...\"):\n",
        "                    # Create progress containers\n",
        "                    progress_bar = st.progress(0)\n",
        "                    progress_text = st.empty()\n",
        "                    status_text = st.empty()\n",
        "\n",
        "                    embeddings = []\n",
        "                    total_emails = len(df_sample)\n",
        "                    batch_size = BATCH_SIZE\n",
        "\n",
        "                    status_text.info(f\"ğŸ”„ Processing {total_emails} emails in batches of {batch_size}...\")\n",
        "\n",
        "                    # Process in batches for better performance\n",
        "                    for batch_start in range(0, total_emails, batch_size):\n",
        "                        batch_end = min(batch_start + batch_size, total_emails)\n",
        "                        batch_df = df_sample.iloc[batch_start:batch_end]\n",
        "\n",
        "                        # Process batch\n",
        "                        batch_embeddings = []\n",
        "                        for i, row in batch_df.iterrows():\n",
        "                            email_vector = get_email_vector(row[\"email\"], CHUNK_SIZE, OVERLAP)\n",
        "                            batch_embeddings.append(email_vector)\n",
        "\n",
        "                        embeddings.extend(batch_embeddings)\n",
        "\n",
        "                        # Update progress\n",
        "                        progress_value = len(embeddings) / total_emails\n",
        "                        progress_bar.progress(progress_value)\n",
        "                        progress_text.text(f\"Processed {len(embeddings)} of {total_emails} emails ({progress_value:.1%})\")\n",
        "\n",
        "                    # Store results\n",
        "                    st.session_state['embeddings'] = np.array(embeddings)\n",
        "                    st.session_state['email_data'] = df_sample.copy()\n",
        "\n",
        "                    # Clean up UI\n",
        "                    progress_bar.empty()\n",
        "                    progress_text.empty()\n",
        "                    status_text.empty()\n",
        "\n",
        "                    st.success(f\"âœ… Built embeddings for {len(embeddings)} emails!\")\n",
        "\n",
        "                    # Show embedding stats\n",
        "                    embedding_shape = st.session_state['embeddings'].shape\n",
        "                    st.info(f\"ğŸ“Š Embedding shape: {embedding_shape[0]} emails Ã— {embedding_shape[1]} dimensions\")\n",
        "\n",
        "        with col2:\n",
        "            if 'embeddings' in st.session_state:\n",
        "                st.info(f\"ğŸ“Š Embeddings ready: {st.session_state['embeddings'].shape}\")\n",
        "\n",
        "        # Similarity search and email classification\n",
        "        if 'embeddings' in st.session_state:\n",
        "            st.markdown(\"### ğŸ” Email Analysis & Prediction\")\n",
        "\n",
        "            # Create tabs for different analysis types\n",
        "            tab1, tab2, tab3 = st.tabs([\"ğŸ” Similarity Search\", \"ğŸ¯ Email Classification\", \"ğŸš¨ Spam Detection\"])\n",
        "\n",
        "            with tab1:\n",
        "                st.markdown(\"#### Find Similar Emails\")\n",
        "                query_text = st.text_area(\n",
        "                    \"Enter email text to find similar emails:\",\n",
        "                    placeholder=\"Type your email content here...\",\n",
        "                    height=100,\n",
        "                    key=\"similarity_search\"\n",
        "                )\n",
        "\n",
        "                col1, col2 = st.columns([1, 3])\n",
        "\n",
        "                with col1:\n",
        "                    top_k = st.slider(\"Number of results\", min_value=1, max_value=10, value=5, key=\"similarity_k\")\n",
        "\n",
        "                with col2:\n",
        "                    if st.button(\"ğŸ” Find Similar Emails\") and query_text:\n",
        "                        query_vector = get_email_vector(query_text, CHUNK_SIZE, OVERLAP).reshape(1, -1)\n",
        "                        similarities = cosine_similarity(query_vector, st.session_state['embeddings'])[0]\n",
        "\n",
        "                        # Get top k most similar\n",
        "                        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "                        st.markdown(\"#### ğŸ“‹ Search Results\")\n",
        "\n",
        "                        for i, idx in enumerate(top_indices):\n",
        "                            email_data = st.session_state['email_data'].iloc[idx]\n",
        "                            similarity_score = similarities[idx]\n",
        "\n",
        "                            st.markdown(f\"\"\"\n",
        "                            <div class=\"email-result\">\n",
        "                                <div style=\"display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;\">\n",
        "                                    <div>\n",
        "                                        <strong>#{i+1} - ID: {email_data['id']}</strong> |\n",
        "                                        <strong>Label:</strong> {email_data['label']}\n",
        "                                    </div>\n",
        "                                    <span class=\"similarity-score\">Similarity: {similarity_score:.4f}</span>\n",
        "                                </div>\n",
        "                                <div style=\"background: white; padding: 10px; border-radius: 5px; font-size: 14px;\">\n",
        "                                    {email_data['email']}\n",
        "                                </div>\n",
        "                            </div>\n",
        "                            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            with tab2:\n",
        "                st.markdown(\"#### Multi-Class Email Classification\")\n",
        "                classify_text = st.text_area(\n",
        "                    \"Enter email text for classification:\",\n",
        "                    placeholder=\"Type the email content you want to classify...\",\n",
        "                    height=100,\n",
        "                    key=\"classification_text\"\n",
        "                )\n",
        "\n",
        "                if st.button(\"ğŸ¯ Classify Email\") and classify_text:\n",
        "                    try:\n",
        "                        # Train classifier using the same dataframe used for embeddings\n",
        "                        with st.spinner(\"Training classifier...\"):\n",
        "                            classifier, label_encoder, X_test, y_test = create_smart_classifier(\n",
        "                                st.session_state['email_data'],\n",
        "                                st.session_state['embeddings']\n",
        "                            )\n",
        "\n",
        "                        # Make prediction - pass the modified get_email_vector function\n",
        "                        get_email_vector_lambda = lambda email: get_email_vector(email, CHUNK_SIZE, OVERLAP)\n",
        "                        prediction_label, class_probabilities = predict_email_class(\n",
        "                            classify_text, classifier, label_encoder, get_email_vector_lambda\n",
        "                        )\n",
        "\n",
        "                        # Display results\n",
        "                        col1, col2 = st.columns([1, 2])\n",
        "\n",
        "                        with col1:\n",
        "                            st.markdown(\"#### ğŸ¯ Prediction Result\")\n",
        "                            confidence = max(class_probabilities.values())\n",
        "\n",
        "                            if confidence > 0.7:\n",
        "                                st.success(f\"**Predicted Class:** {prediction_label}\")\n",
        "                                st.metric(\"Confidence\", f\"{confidence:.2%}\")\n",
        "                            elif confidence > 0.5:\n",
        "                                st.warning(f\"**Predicted Class:** {prediction_label}\")\n",
        "                                st.metric(\"Confidence\", f\"{confidence:.2%}\")\n",
        "                            else:\n",
        "                                st.error(f\"**Predicted Class:** {prediction_label}\")\n",
        "                                st.metric(\"Confidence (Low)\", f\"{confidence:.2%}\")\n",
        "\n",
        "                        with col2:\n",
        "                            st.markdown(\"#### ğŸ“Š Class Probabilities\")\n",
        "\n",
        "                            # Create probability dataframe\n",
        "                            prob_df = pd.DataFrame([\n",
        "                                {'Class': class_name, 'Probability': prob}\n",
        "                                for class_name, prob in class_probabilities.items()\n",
        "                            ]).sort_values('Probability', ascending=False)\n",
        "\n",
        "                            # Plot probabilities\n",
        "                            fig_prob = px.bar(\n",
        "                                prob_df,\n",
        "                                x='Class',\n",
        "                                y='Probability',\n",
        "                                title='Classification Probabilities',\n",
        "                                color='Probability',\n",
        "                                color_continuous_scale=ROYAL_COLOR_SCALE\n",
        "                            )\n",
        "                            fig_prob.update_layout(\n",
        "                                height=300,\n",
        "                                plot_bgcolor='#E8F5E8',\n",
        "                                paper_bgcolor='#E8F5E8'\n",
        "                            )\n",
        "                            st.plotly_chart(fig_prob, use_container_width=True)\n",
        "\n",
        "                            # Show probability table\n",
        "                            st.dataframe(prob_df, use_container_width=True, hide_index=True)\n",
        "\n",
        "                        # Model performance\n",
        "                        test_accuracy = classifier.score(X_test, y_test)\n",
        "                        st.info(f\"ğŸ“ˆ Model Test Accuracy: {test_accuracy:.2%}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"âŒ Classification error: {str(e)}\")\n",
        "\n",
        "            with tab3:\n",
        "                st.markdown(\"#### ğŸš¨ Spam/Ham Detection\")\n",
        "                spam_text = st.text_area(\n",
        "                    \"Enter email text for spam detection:\",\n",
        "                    placeholder=\"Type the email content to check if it's spam or legitimate...\",\n",
        "                    height=100,\n",
        "                    key=\"spam_detection_text\"\n",
        "                )\n",
        "\n",
        "                if st.button(\"ğŸš¨ Check for Spam\") and spam_text:\n",
        "                    try:\n",
        "                        with st.spinner(\"Analyzing email for spam...\"):\n",
        "                            get_email_vector_lambda = lambda email: get_email_vector(email, CHUNK_SIZE, OVERLAP)\n",
        "                            prediction, confidence, model_accuracy = is_spam_classifier(\n",
        "                                spam_text,\n",
        "                                st.session_state['email_data'],\n",
        "                                st.session_state['embeddings'],\n",
        "                                get_email_vector_lambda\n",
        "                            )\n",
        "\n",
        "                        # Display spam detection results\n",
        "                        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "                        with col1:\n",
        "                            if prediction == 'spam':\n",
        "                                st.error(f\"ğŸš¨ **SPAM DETECTED**\")\n",
        "                                st.markdown(\"âš ï¸ This email appears to be spam\")\n",
        "                            else:\n",
        "                                st.success(f\"âœ… **LEGITIMATE EMAIL**\")\n",
        "                                st.markdown(\"ğŸ“§ This email appears to be legitimate\")\n",
        "\n",
        "                        with col2:\n",
        "                            st.metric(\"Detection Confidence\", f\"{confidence:.2%}\")\n",
        "\n",
        "                            # Confidence interpretation\n",
        "                            if confidence > 0.8:\n",
        "                                st.success(\"High confidence\")\n",
        "                            elif confidence > 0.6:\n",
        "                                st.warning(\"Medium confidence\")\n",
        "                            else:\n",
        "                                st.error(\"Low confidence\")\n",
        "\n",
        "                        with col3:\n",
        "                            st.metric(\"Model Accuracy\", f\"{model_accuracy:.2%}\")\n",
        "                            st.info(\"Trained on your dataset\")\n",
        "\n",
        "                        # Additional analysis\n",
        "                        st.markdown(\"#### ğŸ” Email Analysis Details\")\n",
        "\n",
        "                        # Extract features for analysis\n",
        "                        email_lower = spam_text.lower()\n",
        "                        spam_indicators = []\n",
        "\n",
        "                        # Common spam indicators\n",
        "                        spam_words = ['win', 'prize', 'congratulations', 'click here', 'urgent', 'limited time',\n",
        "                                     'free', 'money', 'cash', 'discount', 'offer', 'deal', 'guarantee']\n",
        "\n",
        "                        found_indicators = [word for word in spam_words if word in email_lower]\n",
        "                        if found_indicators:\n",
        "                            spam_indicators.extend(found_indicators)\n",
        "\n",
        "                        # Check for suspicious patterns\n",
        "                        if len([c for c in spam_text if c.isupper()]) > len(spam_text) * 0.3:\n",
        "                            spam_indicators.append(\"Excessive capital letters\")\n",
        "\n",
        "                        if spam_text.count('!') > 3:\n",
        "                            spam_indicators.append(\"Multiple exclamation marks\")\n",
        "\n",
        "                        if spam_indicators:\n",
        "                            st.warning(\"âš ï¸ Potential spam indicators found:\")\n",
        "                            for indicator in spam_indicators:\n",
        "                                st.write(f\"â€¢ {indicator}\")\n",
        "                        else:\n",
        "                            st.success(\"âœ… No obvious spam indicators detected\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"âŒ Spam detection error: {str(e)}\")\n",
        "\n",
        "            # Visualization\n",
        "            st.markdown(\"### ğŸ“ˆ Embedding Visualization\")\n",
        "\n",
        "            col1, col2 = st.columns([1, 1])\n",
        "\n",
        "            with col1:\n",
        "                if st.button(\"ğŸ¨ Generate 2D Visualization\"):\n",
        "                    with st.spinner(\"Generating 2D visualization...\"):\n",
        "                        embeddings = st.session_state['embeddings']\n",
        "\n",
        "                        reduced_embeddings = perform_dimensionality_reduction(\n",
        "                            embeddings, method=DIM_REDUCTION, n_components=2\n",
        "                        )\n",
        "\n",
        "                        # Create visualization dataframe\n",
        "                        viz_df = pd.DataFrame({\n",
        "                            'x': reduced_embeddings[:, 0],\n",
        "                            'y': reduced_embeddings[:, 1],\n",
        "                            'label': st.session_state['email_data']['label'],\n",
        "                            'id': st.session_state['email_data']['id'],\n",
        "                            'email_preview': st.session_state['email_data']['email'].str[:100] + \"...\"\n",
        "                        })\n",
        "\n",
        "                        fig_scatter = px.scatter(\n",
        "                            viz_df,\n",
        "                            x='x',\n",
        "                            y='y',\n",
        "                            color='label',\n",
        "                            hover_data=['id', 'email_preview'],\n",
        "                            title=f\"Email Embeddings 2D Visualization ({DIM_REDUCTION})\",\n",
        "                            width=800,\n",
        "                            height=600,\n",
        "                            color_discrete_sequence=ROYAL_COLOR_SCALE[2:]\n",
        "                        )\n",
        "\n",
        "                        fig_scatter.update_traces(marker=dict(size=8, opacity=0.7))\n",
        "                        fig_scatter.update_layout(\n",
        "                            xaxis_title=f\"{DIM_REDUCTION} Component 1\",\n",
        "                            yaxis_title=f\"{DIM_REDUCTION} Component 2\",\n",
        "                            plot_bgcolor='#E8F5E8',\n",
        "                            paper_bgcolor='#E8F5E8'\n",
        "                        )\n",
        "\n",
        "                        st.plotly_chart(fig_scatter, use_container_width=True)\n",
        "\n",
        "            with col2:\n",
        "                if st.button(\"ğŸŒ Generate 3D Visualization\"):\n",
        "                    with st.spinner(\"Generating 3D visualization...\"):\n",
        "                        embeddings = st.session_state['embeddings']\n",
        "\n",
        "                        reduced_embeddings = perform_dimensionality_reduction(\n",
        "                            embeddings, method=DIM_REDUCTION, n_components=3\n",
        "                        )\n",
        "\n",
        "                        # Create 3D visualization dataframe\n",
        "                        viz_df_3d = pd.DataFrame({\n",
        "                            'x': reduced_embeddings[:, 0],\n",
        "                            'y': reduced_embeddings[:, 1],\n",
        "                            'z': reduced_embeddings[:, 2],\n",
        "                            'label': st.session_state['email_data']['label'],\n",
        "                            'id': st.session_state['email_data']['id'],\n",
        "                            'email_preview': st.session_state['email_data']['email'].str[:100] + \"...\"\n",
        "                        })\n",
        "\n",
        "                        # Create 3D scatter plot\n",
        "                        fig_3d = px.scatter_3d(\n",
        "                            viz_df_3d,\n",
        "                            x='x',\n",
        "                            y='y',\n",
        "                            z='z',\n",
        "                            color='label',\n",
        "                            hover_data=['id', 'email_preview'],\n",
        "                            title=f\"Email Embeddings 3D Visualization ({DIM_REDUCTION})\",\n",
        "                            width=800,\n",
        "                            height=700,\n",
        "                            color_discrete_sequence=ROYAL_COLOR_SCALE[2:]\n",
        "                        )\n",
        "\n",
        "                        fig_3d.update_traces(marker=dict(size=5, opacity=0.8))\n",
        "                        fig_3d.update_layout(\n",
        "                            scene=dict(\n",
        "                                xaxis_title=f\"{DIM_REDUCTION} Component 1\",\n",
        "                                yaxis_title=f\"{DIM_REDUCTION} Component 2\",\n",
        "                                zaxis_title=f\"{DIM_REDUCTION} Component 3\",\n",
        "                                camera=dict(\n",
        "                                    eye=dict(x=1.5, y=1.5, z=1.5)\n",
        "                                ),\n",
        "                                bgcolor='#E8F5E8'\n",
        "                            ),\n",
        "                            paper_bgcolor='#E8F5E8'\n",
        "                        )\n",
        "\n",
        "                        st.plotly_chart(fig_3d, use_container_width=True)\n",
        "\n",
        "            # Classification Metrics and Analysis\n",
        "            st.markdown(\"### ğŸ“Š Classification Metrics Analysis\")\n",
        "\n",
        "            if st.button(\"ğŸ“ˆ Generate Classification Report\"):\n",
        "                with st.spinner(\"Generating classification metrics...\"):\n",
        "                    # Train classifier for evaluation using the same dataframe used for embeddings\n",
        "                    classifier, label_encoder, X_test, y_test = create_smart_classifier(\n",
        "                        st.session_state['email_data'], st.session_state['embeddings']\n",
        "                    )\n",
        "\n",
        "                    # Get predictions on test set\n",
        "                    y_pred = classifier.predict(X_test)\n",
        "                    y_true = y_test\n",
        "\n",
        "                    st.markdown(\"#### ğŸ¯ Classification Performance\")\n",
        "                    st.info(f\"ğŸ“ Results based on trained Random Forest classifier with 80/20 train-test split on {len(st.session_state['email_data'])} emails.\")\n",
        "\n",
        "                    # Plot metrics dashboard\n",
        "                    metrics_df = plot_metrics_dashboard(y_true, y_pred, label_encoder)\n",
        "\n",
        "                    # Plot confusion matrix\n",
        "                    st.markdown(\"#### ğŸ”„ Confusion Matrix\")\n",
        "                    plot_confusion_matrix(y_true, y_pred, label_encoder)\n",
        "\n",
        "                    # Classification report\n",
        "                    st.markdown(\"#### ğŸ“‹ Detailed Classification Report\")\n",
        "                    # Convert encoded labels back for sklearn report\n",
        "                    y_true_labels = label_encoder.inverse_transform(y_true)\n",
        "                    y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "                    report = classification_report(y_true_labels, y_pred_labels, output_dict=True)\n",
        "                    report_df = pd.DataFrame(report).transpose()\n",
        "                    st.dataframe(report_df.round(4), use_container_width=True)\n",
        "\n",
        "                    # Model performance summary\n",
        "                    test_accuracy = classifier.score(X_test, y_test)\n",
        "                    st.success(f\"ğŸ¯ Overall Test Accuracy: {test_accuracy:.2%}\")\n",
        "\n",
        "            # Vector Database Visualization\n",
        "            st.markdown(\"### ğŸ—ƒï¸ Vector Database Analysis\")\n",
        "\n",
        "            if st.button(\"ğŸ” Analyze Vector Database\"):\n",
        "                with st.spinner(\"Analyzing vector database...\"):\n",
        "                    embeddings = st.session_state['embeddings']\n",
        "\n",
        "                    # Calculate similarity matrix\n",
        "                    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        # Similarity heatmap\n",
        "                        fig_sim = px.imshow(\n",
        "                            similarity_matrix[:20, :20],  # Show first 20x20 for readability\n",
        "                            title=\"Cosine Similarity Matrix (First 20 emails)\",\n",
        "                            color_continuous_scale=[[0, '#E8F5E8'], [0.5, '#228B22'], [1, '#003A00']],\n",
        "                            aspect=\"auto\"\n",
        "                        )\n",
        "                        fig_sim.update_layout(\n",
        "                            height=400,\n",
        "                            plot_bgcolor='#E8F5E8',\n",
        "                            paper_bgcolor='#E8F5E8'\n",
        "                        )\n",
        "                        st.plotly_chart(fig_sim, use_container_width=True)\n",
        "\n",
        "                    with col2:\n",
        "                        # Distribution of similarities\n",
        "                        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
        "\n",
        "                        fig_dist = px.histogram(\n",
        "                            x=upper_triangle,\n",
        "                            nbins=50,\n",
        "                            title=\"Distribution of Cosine Similarities\",\n",
        "                            labels={'x': 'Cosine Similarity', 'y': 'Frequency'},\n",
        "                            color_discrete_sequence=['#005500']\n",
        "                        )\n",
        "                        fig_dist.update_layout(\n",
        "                            height=400,\n",
        "                            plot_bgcolor='#E8F5E8',\n",
        "                            paper_bgcolor='#E8F5E8'\n",
        "                        )\n",
        "                        st.plotly_chart(fig_dist, use_container_width=True)\n",
        "\n",
        "                    # Vector statistics\n",
        "                    st.markdown(\"#### ğŸ“Š Vector Database Statistics\")\n",
        "\n",
        "                    col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "                    with col1:\n",
        "                        avg_sim = np.mean(upper_triangle)\n",
        "                        st.metric(\"Average Similarity\", f\"{avg_sim:.4f}\")\n",
        "\n",
        "                    with col2:\n",
        "                        std_sim = np.std(upper_triangle)\n",
        "                        st.metric(\"Similarity Std Dev\", f\"{std_sim:.4f}\")\n",
        "\n",
        "                    with col3:\n",
        "                        max_sim = np.max(upper_triangle)\n",
        "                        st.metric(\"Max Similarity\", f\"{max_sim:.4f}\")\n",
        "\n",
        "                    with col4:\n",
        "                        min_sim = np.min(upper_triangle)\n",
        "                        st.metric(\"Min Similarity\", f\"{min_sim:.4f}\")\n",
        "\n",
        "                    # Clustering analysis\n",
        "                    st.markdown(\"#### ğŸ¯ Clustering Analysis\")\n",
        "\n",
        "                    try:\n",
        "                        from sklearn.cluster import KMeans\n",
        "\n",
        "                        # Perform k-means clustering\n",
        "                        n_clusters = min(5, st.session_state['email_data']['label'].nunique())\n",
        "                        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "                        cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "                        # Create cluster visualization\n",
        "                        reduced_for_clustering = perform_dimensionality_reduction(\n",
        "                            embeddings, method=\"PCA\", n_components=2\n",
        "                        )\n",
        "\n",
        "                        cluster_df = pd.DataFrame({\n",
        "                            'x': reduced_for_clustering[:, 0],\n",
        "                            'y': reduced_for_clustering[:, 1],\n",
        "                            'cluster': cluster_labels,\n",
        "                            'true_label': st.session_state['email_data']['label'],\n",
        "                            'id': st.session_state['email_data']['id']\n",
        "                        })\n",
        "\n",
        "                        fig_cluster = px.scatter(\n",
        "                            cluster_df,\n",
        "                            x='x',\n",
        "                            y='y',\n",
        "                            color='cluster',\n",
        "                            symbol='true_label',\n",
        "                            hover_data=['id'],\n",
        "                            title=\"K-Means Clustering of Email Embeddings\",\n",
        "                            color_continuous_scale=ROYAL_COLOR_SCALE\n",
        "                        )\n",
        "\n",
        "                        fig_cluster.update_traces(marker=dict(size=8, opacity=0.7))\n",
        "                        fig_cluster.update_layout(\n",
        "                            plot_bgcolor='#E8F5E8',\n",
        "                            paper_bgcolor='#E8F5E8'\n",
        "                        )\n",
        "                        st.plotly_chart(fig_cluster, use_container_width=True)\n",
        "\n",
        "                    except ImportError:\n",
        "                        st.warning(\"âš ï¸ Scikit-learn clustering not available\")\n",
        "\n",
        "        # Data preview\n",
        "        if st.checkbox(\"ğŸ‘€ Show Raw Data\"):\n",
        "            st.markdown(\"### ğŸ“‹ Raw Data Preview\")\n",
        "            st.dataframe(df, use_container_width=True)\n",
        "\n",
        "    else:\n",
        "        # Show information when no data is loaded\n",
        "        st.warning(\"âš ï¸ No data loaded. Please upload a CSV file to get started.\")\n",
        "\n",
        "        # Show expected format examples\n",
        "        st.markdown(\"### ğŸ“ Supported CSV Formats\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(\"**Format 1: Category/Message**\")\n",
        "            st.code(\"\"\"\n",
        "Category,Message\n",
        "spam,\"You won $1000! Click here to claim\"\n",
        "ham,\"Hello team, meeting at 2PM tomorrow\"\n",
        "spam,\"URGENT! Your account will be suspended\"\n",
        "ham,\"Thanks for your email, I'll reply soon\"\n",
        "            \"\"\")\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(\"**Format 2: Standard 3-column**\")\n",
        "            st.code(\"\"\"\n",
        "id,email,label\n",
        "1,\"You won $1000! Click here\",spam\n",
        "2,\"Hello team, meeting at 2PM\",ham\n",
        "3,\"URGENT! Account suspended\",spam\n",
        "4,\"Thanks for your email\",ham\n",
        "            \"\"\")\n",
        "\n",
        "    # Footer\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <div style='text-align: center; color: #003A00; padding: 20px;'>\n",
        "            ğŸ‘‘ Email Classification & Embedding Explorer |\n",
        "            Built with Streamlit & Python |\n",
        "            ğŸš€ Powered by Machine Learning\n",
        "        </div>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9yRjECLL0bE",
        "outputId": "79e13779-2176-4735-c6e0-cbd84632a955"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.148.227.158"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3HWJ8D5vqpC",
        "outputId": "9f8693bb-c68c-4df8-aaa1-8b1cf98afcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.227.158:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kyour url is: https://petite-mice-chew.loca.lt\n",
            "Websocket message size limit exceeded. Showing error to the user: **Data of size 290.0 MB exceeds the message size limit of\n",
            "200.0 MB.**\n",
            "\n",
            "This is often caused by a large chart or dataframe. Please decrease the amount of data sent\n",
            "to the browser, or increase the limit by setting the config option `server.maxMessageSize`.\n",
            "[Click here to learn more about config options](https://docs.streamlit.io/develop/api-reference/configuration/config.toml).\n",
            "\n",
            "_Note that increasing the limit may lead to long loading times and large memory consumption\n",
            "of the client's browser and the Streamlit server._\n",
            "2025-07-26 13:38:08.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}