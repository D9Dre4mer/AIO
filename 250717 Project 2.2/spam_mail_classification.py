"""
H·ªá th·ªëng ph√¢n lo·∫°i Email Spam

H·ªá th·ªëng ph√¢n lo·∫°i email th√†nh spam ho·∫∑c ham s·ª≠ d·ª•ng
c√°c thu·∫≠t to√°n Naive Bayes v·ªõi x·ª≠ l√Ω vƒÉn b·∫£n v√† tr·ª±c quan h√≥a.
"""

import os
from typing import Optional, Tuple, Any
import logging

import gdown
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import string
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix
)

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def DownloadFile(
    file_id: str,
    output_dir: str = "dataset",
    filename: str = "default.csv"
) -> Optional[str]:
    """
    T·∫£i file t·ª´ Google Drive b·∫±ng ID file.

    Tham s·ªë:
        file_id: ID file Google Drive
        output_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u file
        filename: T√™n file ƒë·∫ßu ra

    Tr·∫£ v·ªÅ:
        ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ƒë√£ t·∫£i ho·∫∑c None n·∫øu th·∫•t b·∫°i
    """
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)

    if os.path.exists(output_path):
        logger.info(f"‚úÖ File ƒë√£ t·ªìn t·∫°i: {output_path}")
        return output_path

    url = f"https://drive.google.com/uc?id={file_id}"

    try:
        gdown.download(url, output_path, quiet=False)
        logger.info(f"üì• T·∫£i file th√†nh c√¥ng: {output_path}")
        return output_path
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·∫£i file: {e}")
        return None


def create_spam_ham_visualization(df: pd.DataFrame) -> None:
    """
    T·∫°o bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a ph√¢n b·ªë Spam vs Ham.

    Tham s·ªë:
        df: DataFrame ch·ª©a c·ªôt 'Category' v·ªõi gi√° tr·ªã ƒë√£ m√£ h√≥a
            (0=Ham, 1=Spam)
    """
    plt.figure(figsize=(12, 6))
    plt.style.use('seaborn-v0_8')

    # B·∫£ng m√†u hi·ªán ƒë·∫°i
    colors = ['#FF6B9D', '#4ECDC4']

    # T·∫°o bi·ªÉu ƒë·ªì s·ªë l∆∞·ª£ng
    ax = sns.countplot(
        data=df,
        x='Category',
        palette=colors,
        edgecolor='white',
        linewidth=1.5
    )

    # T√πy ch·ªânh ti√™u ƒë·ªÅ v√† nh√£n
    plt.title(
        'üìß Ph√¢n B·ªë Spam vs Ham trong Dataset',
        fontsize=20,
        fontweight='bold',
        pad=20,
        color='#2C3E50'
    )
    plt.xlabel(
        'Lo·∫°i Email',
        fontsize=14,
        fontweight='bold',
        color='#34495E'
    )
    plt.ylabel(
        'S·ªë l∆∞·ª£ng Email',
        fontsize=14,
        fontweight='bold',
        color='#34495E'
    )

    # T√πy ch·ªânh nh√£n tr·ª•c x
    plt.xticks(
        [0, 1],
        ['üì© Ham (B√¨nh th∆∞·ªùng)', '‚ö†Ô∏è Spam (R√°c)'],
        fontsize=12,
        fontweight='bold'
    )

    # Th√™m th√¥ng tin s·ªë l∆∞·ª£ng
    category_counts = df['Category'].value_counts().sort_index()
    max_count = max(category_counts)

    for i, count in enumerate(category_counts):
        # Th√™m s·ªë l∆∞·ª£ng
        ax.text(
            i, count + max_count * 0.02,
            f'{count:,}',
            ha='center', va='bottom',
            fontsize=10, fontweight='bold',
            color='#2C3E50'
        )

        # Th√™m ph·∫ßn trƒÉm
        percentage = (count / len(df)) * 100
        ax.text(
            i, count + max_count * 0.08,
            f'({percentage:.1f}%)',
            ha='center', va='bottom',
            fontsize=10, style='italic',
            color='#7F8C8D'
        )

    # T√πy ch·ªânh l∆∞·ªõi v√† n·ªÅn
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_facecolor('#FAFAFA')

    # ƒê·ªãnh d·∫°ng ƒë∆∞·ªùng vi·ªÅn
    for spine in ax.spines.values():
        spine.set_color('#BDC3C7')
        spine.set_linewidth(1)

    plt.tight_layout()
    plt.show()


def plot_confusion_matrices(
    y_true: Any,
    y_pred_gauss: Any,
    y_pred_multi: Any,
    figsize: Tuple[int, int] = (15, 6)
) -> None:
    """
    V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n cho c·∫£ hai m√¥ h√¨nh c·∫°nh nhau.

    Tham s·ªë:
        y_true: Nh√£n th·ª±c t·∫ø
        y_pred_gauss: D·ª± ƒëo√°n t·ª´ GaussianNB
        y_pred_multi: D·ª± ƒëo√°n t·ª´ MultinomialNB
        figsize: K√≠ch th∆∞·ªõc h√¨nh
    """
    # T√≠nh ma tr·∫≠n nh·∫ßm l·∫´n
    cm_gauss = confusion_matrix(y_true, y_pred_gauss)
    cm_multi = confusion_matrix(y_true, y_pred_multi)

    # T·∫°o subplot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

    # Tham s·ªë chung cho heatmap
    heatmap_kwargs = {
        'annot': True,
        'fmt': 'd',
        'cbar_kws': {'label': 'S·ªë l∆∞·ª£ng'},
        'annot_kws': {'size': 12, 'weight': 'bold'}
    }

    # Heatmap cho GaussianNB
    sns.heatmap(cm_gauss, cmap='Blues', ax=ax1, **heatmap_kwargs)
    ax1.set_title(
        'GaussianNB - Ma tr·∫≠n Nh·∫ßm l·∫´n',
        fontsize=14,
        fontweight='bold'
    )
    ax1.set_xlabel('D·ª± ƒëo√°n', fontsize=12)
    ax1.set_ylabel('Th·ª±c t·∫ø', fontsize=12)
    ax1.set_xticklabels(['Ham', 'Spam'])
    ax1.set_yticklabels(['Ham', 'Spam'])

    # Heatmap cho MultinomialNB
    sns.heatmap(cm_multi, cmap='Reds', ax=ax2, **heatmap_kwargs)
    ax2.set_title(
        'MultinomialNB - Ma tr·∫≠n Nh·∫ßm l·∫´n',
        fontsize=14,
        fontweight='bold'
    )
    ax2.set_xlabel('D·ª± ƒëo√°n', fontsize=12)
    ax2.set_ylabel('Th·ª±c t·∫ø', fontsize=12)
    ax2.set_xticklabels(['Ham', 'Spam'])
    ax2.set_yticklabels(['Ham', 'Spam'])

    plt.tight_layout()
    plt.show()


def preprocess_text(text: str) -> str:
    """
    X·ª≠ l√Ω tr∆∞·ªõc vƒÉn b·∫£n b·∫±ng c√°ch chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè d·∫•u c√¢u,
    t·ª´ d·ª´ng v√† √°p d·ª•ng stemming.

    Tham s·ªë:
        text: VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn x·ª≠ l√Ω

    Tr·∫£ v·ªÅ:
        VƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω d∆∞·ªõi d·∫°ng chu·ªói
    """
    ps = PorterStemmer()
    stop_words = set(stopwords.words('english'))

    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng v√† lo·∫°i b·ªè d·∫•u c√¢u
    text = text.lower().translate(str.maketrans('', '', string.punctuation))

    # T√°ch t·ª´, lo·∫°i b·ªè t·ª´ d·ª´ng v√† √°p d·ª•ng stemming
    return ' '.join([
        ps.stem(word) for word in text.split()
        if word not in stop_words
    ])


def predict(
    text: str,
    model: Any,
    vectorizer: CountVectorizer,
    dense: bool = False
) -> int:
    """
    D·ª± ƒëo√°n vƒÉn b·∫£n l√† spam hay ham.

    Tham s·ªë:
        text: VƒÉn b·∫£n ƒë·∫ßu v√†o ƒë·ªÉ ph√¢n lo·∫°i
        model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán (GaussianNB ho·∫∑c MultinomialNB)
        vectorizer: CountVectorizer ƒë√£ fit
        dense: C√≥ chuy·ªÉn sang m·∫£ng d√†y kh√¥ng (cho GaussianNB)

    Tr·∫£ v·ªÅ:
        D·ª± ƒëo√°n (0 cho ham, 1 cho spam)
    """
    processed_text = preprocess_text(text)
    features = vectorizer.transform([processed_text])

    if dense:
        features = features.toarray()

    prediction = model.predict(features)
    return prediction[0]


def print_section_header(title: str, width: int = 80) -> None:
    """In ti√™u ƒë·ªÅ ph·∫ßn v·ªõi ƒë·ªãnh d·∫°ng."""
    print("=" * width)
    print(f"üîç {title}")
    print("=" * width)


def print_dataset_info(df: pd.DataFrame) -> None:
    """In th√¥ng tin to√†n di·ªán v·ªÅ dataset."""
    print_section_header("TH√îNG TIN DATASET")

    print("üìä Th·ªëng k√™ c∆° b·∫£n v·ªÅ Dataset:")
    print(f"   ‚Ä¢ T·ªïng s·ªë email: {len(df):,}")
    print(f"   ‚Ä¢ C√°c c·ªôt: {list(df.columns)}")
    print(f"   ‚Ä¢ S·ª≠ d·ª•ng b·ªô nh·ªõ: \
        {df.memory_usage(deep=True).sum() / 1024:.2f} KB")

    print("\nüìã D·ªØ li·ªáu m·∫´u (5 d√≤ng ƒë·∫ßu):")
    print(df.head().to_string())

    print("\nüìà M√¥ t·∫£ Dataset:")
    print(df.describe())


def print_data_analysis(df: pd.DataFrame) -> None:
    """In ph√¢n t√≠ch d·ªØ li·ªáu chi ti·∫øt."""
    print_section_header("PH√ÇN T√çCH D·ªÆ LI·ªÜU")

    ham_count = (df['Category'] == 0).sum()
    spam_count = (df['Category'] == 1).sum()
    total = len(df)

    print("üìä Ph√¢n t√≠ch Ph√¢n b·ªë:")
    print(f"   ‚Ä¢ T·ªïng s·ªë email: {total:,}")
    print(f"   ‚Ä¢ Email Ham: {ham_count:,} ({ham_count/total*100:.2f}%)")
    print(f"   ‚Ä¢ Email Spam: {spam_count:,} ({spam_count/total*100:.2f}%)")
    print(f"   ‚Ä¢ T·ª∑ l·ªá Ham:Spam: {ham_count/spam_count:.2f}:1")


def print_model_results(
    model_name: str,
    val_accuracy: float,
    test_accuracy: float,
    y_val: Any,
    y_val_pred: Any,
    y_test: Any,
    y_test_pred: Any
) -> None:
    """In k·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh to√†n di·ªán."""
    print_section_header(f"K·∫æT QU·∫¢ M√î H√åNH {model_name.upper()}")

    print("üéØ ƒêi·ªÉm ƒë·ªô ch√≠nh x√°c:")
    print(f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c Validation: {val_accuracy:.4f} \
          ({val_accuracy*100:.2f}%)")
    print(f"   ‚Ä¢ ƒê·ªô ch√≠nh x√°c Test: {test_accuracy:.4f} \
          ({test_accuracy*100:.2f}%)")

    print("\nüìä B√°o c√°o Ph√¢n lo·∫°i Chi ti·∫øt (Validation):")
    print(classification_report(y_val, y_val_pred))

    print("\nüìä B√°o c√°o Ph√¢n lo·∫°i Chi ti·∫øt (Test):")
    print(classification_report(y_test, y_test_pred))


def main() -> None:
    """H√†m th·ª±c thi ch√≠nh."""
    print_section_header("H·ªÜ TH·ªêNG PH√ÇN LO·∫†I EMAIL SPAM", 80)

    # T·∫£i dataset
    logger.info("üì• ƒêang t·∫£i dataset...")
    file_path = DownloadFile(
        file_id="1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R",
        output_dir="dataset",
        filename="mails.csv"
    )

    if file_path is None:
        logger.error("‚ùå Kh√¥ng th·ªÉ t·∫£i file. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return

    # T·∫£i v√† kh√°m ph√° d·ªØ li·ªáu
    logger.info("üìÇ Truy xu·∫•t d·ªØ li·ªáu trong dataset...")
    mails_df = pd.read_csv(file_path)

    print_dataset_info(mails_df)

    # M√£ h√≥a nh√£n
    logger.info("üîÑ ƒêang m√£ h√≥a nh√£n...")
    mails_df['Category'] = mails_df['Category'].map({'ham': 0, 'spam': 1})

    print_data_analysis(mails_df)

    # T·∫°o tr·ª±c quan h√≥a
    logger.info("üìä ƒêang t·∫°o tr·ª±c quan h√≥a...")
    # create_spam_ham_visualization(mails_df)

    # X·ª≠ l√Ω tr∆∞·ªõc vƒÉn b·∫£n
    logger.info("üîß ƒêang x·ª≠ l√Ω tr∆∞·ªõc d·ªØ li·ªáu vƒÉn b·∫£n...")
    mails_df['processed_message'] = [
        preprocess_text(msg) for msg in mails_df['Message']
    ]

    # Vector h√≥a vƒÉn b·∫£n
    logger.info("üî§ ƒêang vector h√≥a vƒÉn b·∫£n...")
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(mails_df['processed_message'])
    y = mails_df['Category'].values

    print_section_header("K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG")
    print(f"üìè K√≠ch th∆∞·ªõc Ma tr·∫≠n ƒê·∫∑c tr∆∞ng: {X.shape}")
    print(f"üìè K√≠ch th∆∞·ªõc Nh√£n: {y.shape}")
    print(f"üìè K√≠ch th∆∞·ªõc T·ª´ v·ª±ng: {len(vectorizer.vocabulary_):,}")

    # Chia d·ªØ li·ªáu
    logger.info("‚úÇÔ∏è ƒêang chia dataset...")
    VAL_SIZE, TEST_SIZE, SEED = 0.2, 0.125, 0

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=VAL_SIZE, shuffle=True, random_state=SEED
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X_train, y_train, test_size=TEST_SIZE, shuffle=True, random_state=SEED
    )

    print_section_header("CHIA D·ªÆ LI·ªÜU")
    print(f"üìä T·∫≠p Hu·∫•n luy·ªán: {X_train.shape[0]:,} m·∫´u")
    print(f"üìä T·∫≠p Validation: {X_val.shape[0]:,} m·∫´u")
    print(f"üìä T·∫≠p Test: {X_test.shape[0]:,} m·∫´u")

    # Chuy·ªÉn ma tr·∫≠n th∆∞a sang d√†y cho GaussianNB
    logger.info("üîÑ ƒêang chuy·ªÉn ma tr·∫≠n cho GaussianNB...")
    X_train_dense = X_train.toarray()
    X_val_dense = X_val.toarray()
    X_test_dense = X_test.toarray()

    # Hu·∫•n luy·ªán GaussianNB
    logger.info("ü§ñ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh GaussianNB...")
    model_gauss = GaussianNB()
    model_gauss.fit(X_train_dense, y_train)

    # Hu·∫•n luy·ªán MultinomialNB
    logger.info("ü§ñ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh MultinomialNB...")
    model_multi = MultinomialNB()
    model_multi.fit(X_train, y_train)

    # ƒê√°nh gi√° m√¥ h√¨nh
    logger.info("üìä ƒêang ƒë√°nh gi√° m√¥ h√¨nh...")

    # ƒê√°nh gi√° GaussianNB
    y_val_pred_gauss = model_gauss.predict(X_val_dense)
    y_test_pred_gauss = model_gauss.predict(X_test_dense)
    val_acc_gauss = accuracy_score(y_val, y_val_pred_gauss)
    test_acc_gauss = accuracy_score(y_test, y_test_pred_gauss)

    print_model_results(
        "GaussianNB", val_acc_gauss, test_acc_gauss,
        y_val, y_val_pred_gauss, y_test, y_test_pred_gauss
    )

    # ƒê√°nh gi√° MultinomialNB
    y_val_pred_multi = model_multi.predict(X_val)
    y_test_pred_multi = model_multi.predict(X_test)
    val_acc_multi = accuracy_score(y_val, y_val_pred_multi)
    test_acc_multi = accuracy_score(y_test, y_test_pred_multi)

    print_model_results(
        "MultinomialNB", val_acc_multi, test_acc_multi,
        y_val, y_val_pred_multi, y_test, y_test_pred_multi
    )

    # V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n
    logger.info("üìä ƒêang t·∫°o ma tr·∫≠n nh·∫ßm l·∫´n...")
    plot_confusion_matrices(y_val, y_val_pred_gauss, y_val_pred_multi)

    # Ki·ªÉm th·ª≠ d·ª± ƒëo√°n
    print_section_header("KI·ªÇM TH·ª¨ D·ª∞ ƒêO√ÅN")
    test_input = 'I am actually thinking a way of doing something useful'
    label_map = {0: 'ham', 1: 'spam'}

    logger.info("üîÆ ƒêang ki·ªÉm th·ª≠ d·ª± ƒëo√°n...")

    prediction_gauss = predict(test_input, model_gauss, vectorizer, dense=True)
    prediction_multi = predict(test_input, model_multi, vectorizer)

    print(f"üìß Email ki·ªÉm th·ª≠: '{test_input}'")
    print(f"ü§ñ D·ª± ƒëo√°n GaussianNB: {label_map[prediction_gauss]} \
        (m√£: {prediction_gauss})")
    print(f"ü§ñ D·ª± ƒëo√°n MultinomialNB: {label_map[prediction_multi]} \
        (m√£: {prediction_multi})")

    # T√≥m t·∫Øt so s√°nh m√¥ h√¨nh
    print_section_header("T√ìM T·∫ÆT SO S√ÅNH M√î H√åNH")
    print("üèÜ So s√°nh Hi·ªáu su·∫•t:")
    print(f"   ‚Ä¢ GaussianNB    - Val: {val_acc_gauss:.4f},  \
        Test: {test_acc_gauss:.4f}")
    print(f"   ‚Ä¢ MultinomialNB - Val: {val_acc_multi:.4f},  \
        Test: {test_acc_multi:.4f}")

    models = {True: "GaussianNB", False: "MultinomialNB"}
    best_model = models[test_acc_gauss > test_acc_multi]
    print(f"ü•á M√¥ h√¨nh Hi·ªáu su·∫•t T·ªët nh·∫•t: {best_model}")

    logger.info("‚úÖ Ch∆∞∆°ng tr√¨nh ho√†n th√†nh th√†nh c√¥ng!")


if __name__ == "__main__":
    main()
