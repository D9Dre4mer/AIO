"""
Pipeline ch√≠nh cho spam classification.
"""
import numpy as np
import os
import logging
import json
from typing import Dict, Any
from config import SpamClassifierConfig
from data_loader import DataLoader
from embedding_generator import EmbeddingGenerator
from knn_classifier import KNNClassifier
from tfidf_classifier import TFIDFClassifier

logger = logging.getLogger(__name__)

class SpamClassifierPipeline:
    """Pipeline ho√†n ch·ªânh cho spam classification."""
    
    def __init__(self,
                 config: SpamClassifierConfig = None,
                 classifier_type: str = 'knn'):
        """
        Kh·ªüi t·∫°o SpamClassifierPipeline.
        
        Args:
            config: C·∫•u h√¨nh h·ªá th·ªëng
        """
        self.config = config or SpamClassifierConfig()
        self.data_loader = DataLoader(self.config)
        self.embedding_generator = (
            EmbeddingGenerator(self.config)
            if classifier_type == 'knn' else None
        )
        self.classifier = None
        self.classifier_type = classifier_type
        
    def load_corrections(self) -> Dict[str, Any]:
        """Load correction data t·ª´ file JSON"""
        correction_file = "./cache/corrections.json"
        if os.path.exists(correction_file):
            try:
                with open(correction_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"L·ªói load corrections: {e}")
                return {}
        return {}
        
    def train_with_corrections(self) -> Dict[str, Any]:
        """
        üÜï Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu g·ªëc + corrections.
        
        Returns:
            Dict ch·ª©a th√¥ng tin training v·ªõi corrections
        """
        logger.info("ƒêang t·∫£i d·ªØ li·ªáu g·ªëc...")
        messages, labels = self.data_loader.load_data()
        
        corrections = self.load_corrections()
        logger.info(f"T√¨m th·∫•y {len(corrections)} corrections")
        
        if corrections:
            logger.info("ƒêang merge corrections v√†o dataset...")
            
            extended_messages = messages.copy()
            extended_labels = labels.copy()
            
            for email_id, correction in corrections.items():
                subject = correction.get('subject', '')
                sender = correction.get('sender', '')
                snippet = correction.get('snippet', '')
                
                correction_text = (
                    f"Subject: {subject}\nFrom: {sender}\n{snippet}"
                )
                
                corrected_label = correction.get('corrected_label', 'ham')
                extended_messages.append(correction_text)
                extended_labels.append(corrected_label)
                
                logger.info(f"Th√™m correction: {email_id} -> {corrected_label}")
            
            messages = extended_messages
            labels = extended_labels
            
            logger.info(f"Dataset sau merge: {len(messages)} samples")
        
        return self._train_with_data(messages, labels, use_corrections=True)
        
    def train(self) -> None:
        """Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu g·ªëc."""
        logger.info("ƒêang t·∫£i d·ªØ li·ªáu...")
        messages, labels = self.data_loader.load_data()
        
        self._train_with_data(messages, labels, use_corrections=False)
        
    def _train_with_data(self, messages: list, labels: list,
                         use_corrections: bool = False) -> Dict[str, Any]:
        """
        üÜï Internal method ƒë·ªÉ train v·ªõi data ƒë∆∞·ª£c cung c·∫•p.
        
        Args:
            messages: List messages
            labels: List labels
            use_corrections: C√≥ s·ª≠ d·ª•ng corrections kh√¥ng
            
        Returns:
            Dict ch·ª©a th√¥ng tin training
        """
        training_info = {
            'total_samples': len(messages),
            'original_samples': (
                len(messages) - len(self.load_corrections())
                if use_corrections else len(messages)
            ),
            'correction_samples': (
                len(self.load_corrections()) if use_corrections else 0
            ),
            'use_corrections': use_corrections
        }
        
        if self.classifier_type == 'knn':
            # Ki·ªÉm tra s·ªë d√≤ng dataset so v·ªõi embeddings cache
            emb_file = os.path.join(
                'cache', 'embeddings',
                f"embeddings_{self.config.model_name.replace('/', '_')}.npy"
            )
            dataset_count = len(messages)

            if os.path.exists(emb_file):
                try:
                    cache = np.load(emb_file)
                    cache_count = cache.shape[0]
                    if (cache_count != dataset_count
                            and not self.config.regenerate_embeddings):
                        msg = (
                            f"S·ªë d√≤ng trong dataset ({dataset_count}) "
                            f"kh√¥ng kh·ªõp v·ªõi embeddings cache ({cache_count}). "
                            "Ch·∫°y l·∫°i v·ªõi --regenerate ƒë·ªÉ c·∫≠p nh·∫≠t."
                        )
                        raise ValueError(msg)
                    if (cache_count != dataset_count
                            and self.config.regenerate_embeddings):
                        logger.info(
                            f"X√≥a cache c≈©: {emb_file} "
                            "(flag regenerate_embeddings=True)"
                        )
                        os.remove(emb_file)
                except Exception as e:
                    logger.error(f"L·ªói khi ki·ªÉm tra cache: {e}")
                    raise

        logger.info(f"C√°c l·ªõp: {self.data_loader.get_class_names()}")

        # Chia d·ªØ li·ªáu
        train_idx, _, y_train, _ = (
            self.data_loader.split_data(messages, labels)
        )
        train_msgs = [messages[i] for i in train_idx]
        train_lbls = [labels[i] for i in train_idx]
        
        if self.classifier_type == 'knn':
            # T·∫°o embeddings
            logger.info(f"T·∫°o embeddings cho {len(messages)} tin nh·∫Øn...")
            embeddings = self.embedding_generator.generate_embeddings(messages)
            logger.info(f"K√≠ch th∆∞·ªõc embeddings: {embeddings.shape}")

            # T·∫°o metadata
            encoded = self.data_loader.label_encoder.transform(labels)
            metadata = self.data_loader.create_metadata(
                messages, labels, encoded
            )

            # Chia embeddings v√† metadata
            train_emb = embeddings[train_idx]
            train_meta = [metadata[i] for i in train_idx]

            logger.info(f"K√≠ch th∆∞·ªõc t·∫≠p train: {len(train_emb)}")
            logger.info(f"Ph√¢n b·ªë nh√£n train: {np.bincount(y_train)}")

            # T·∫°o v√† hu·∫•n luy·ªán classifier
            self.classifier = KNNClassifier(train_emb.shape[1])
            self.classifier.fit(train_emb, train_meta)

        elif self.classifier_type == 'tfidf':
            self.classifier = TFIDFClassifier()
            self.classifier.fit(train_msgs, train_lbls)

        else:
            raise ValueError(
                f"Lo·∫°i classifier kh√¥ng h·ª£p l·ªá: {self.classifier_type}"
            )
            
        training_info['train_samples'] = len(train_msgs)
        training_info['label_distribution'] = {
            label: train_lbls.count(label) for label in set(train_lbls)
        }
        
        logger.info(f"Training ho√†n t·∫•t: {training_info}")
        return training_info
            
    def predict(self, text: str, k: int = None) -> Dict[str, Any]:
        """
        Ph√¢n lo·∫°i m·ªôt vƒÉn b·∫£n.
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i
            k: S·ªë l∆∞·ª£ng neighbors (m·∫∑c ƒë·ªãnh t·ª´ config)
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n lo·∫°i
        """
        if self.classifier is None:
            raise ValueError("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. "
                           "H√£y g·ªçi train() tr∆∞·ªõc.")

        pre_text = self.data_loader.preprocess_text(text)

        if self.classifier_type == 'knn':
            k = k or self.config.default_k
            
            logger.info(f"\n***ƒêang ph√¢n lo·∫°i: '{text}'")
            logger.info(f"\n***S·ª≠ d·ª•ng top-{k} nearest neighbors")
            
            # T·∫°o query embedding
            query_embedding = self.embedding_generator.generate_query_embedding(
                text
            )
            
            # Ph√¢n lo·∫°i
            prediction, neighbors = self.classifier.predict(
                query_embedding, k=k
            )
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            logger.info(f"\n***D·ª± ƒëo√°n: {prediction.upper()}")
            logger.info("\n***Top neighbors:")
            for i, neighbor in enumerate(neighbors, 1):
                logger.info(f"{i}. Nh√£n: {neighbor['label']} | "
                            f"ƒêi·ªÉm: {neighbor['score']:.4f}")
                logger.info(f"Tin nh·∫Øn: {neighbor['message']}")
            
            # ƒê·∫øm ph√¢n b·ªë nh√£n
            labels = [n['label'] for n in neighbors]
            label_counts = {
                label: labels.count(label) for label in set(labels)
            }
            
            
            return {
                'prediction': prediction,
                'neighbors': neighbors,
                'label_distribution': label_counts
            }
            
        if self.classifier_type == 'tfidf':
            return self.classifier.predict(pre_text, return_proba=True)

        raise ValueError(
            f"Lo·∫°i classifier kh√¥ng h·ª£p l·ªá: {self.classifier_type}"
        )