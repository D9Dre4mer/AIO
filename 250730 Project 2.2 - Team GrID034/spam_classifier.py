"""
Pipeline ch√≠nh cho spam classification.
"""
import numpy as np
import os
import logging
import json
import pickle
from typing import Dict, Any, List, Optional, Tuple
from config import SpamClassifierConfig
from data_loader import DataLoader
from embedding_generator import EmbeddingGenerator
from knn_classifier import KNNClassifier
from tfidf_classifier import TFIDFClassifier

logger = logging.getLogger(__name__)


class SpamClassifierPipeline:
    """Pipeline ho√†n ch·ªânh cho spam classification."""
    
    def __init__(self,
                 config: SpamClassifierConfig = None,
                 classifier_type: str = 'knn'):
        """
        Kh·ªüi t·∫°o SpamClassifierPipeline.
        
        Args:
            config: C·∫•u h√¨nh h·ªá th·ªëng
        """
        self.config = config or SpamClassifierConfig()
        self.data_loader = DataLoader(self.config)
        self.embedding_generator = (
            EmbeddingGenerator(self.config)
            if classifier_type == 'knn' else None
        )
        self.classifier = None
        self.classifier_type = classifier_type
        
    def load_corrections(self) -> Dict[str, Any]:
        """Load correction data t·ª´ file JSON"""
        correction_file = "./cache/corrections.json"
        if os.path.exists(correction_file):
            try:
                with open(correction_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"L·ªói load corrections: {e}")
                return {}
        return {}

    # Corrections dataset cache helpers
    def _get_corrections_dataset_path(self) -> str:
        """ƒê∆∞·ªùng d·∫´n file cache cho merged corrections dataset."""
        cache_dir = os.path.join('cache', 'datasets')
        os.makedirs(cache_dir, exist_ok=True)
        # L∆∞u theo model ƒë·ªÉ tr√°nh xung ƒë·ªôt gi·ªØa c√°c model kh√°c nhau
        model_name_safe = self.config.model_name.replace('/', '_')
        return os.path.join(
            cache_dir, f"with_corrections_dataset_{model_name_safe}.pkl"
        )

    def _save_corrections_dataset(
        self, messages: List[str], labels: List[str]
    ) -> None:
        """L∆∞u merged corrections dataset (ƒë√£ preprocess) v√†o cache."""
        dataset_path = self._get_corrections_dataset_path()
        try:
            with open(dataset_path, 'wb') as f:
                pickle.dump({'messages': messages, 'labels': labels}, f)
            logger.info(
                f"ƒê√£ l∆∞u merged corrections dataset: {dataset_path} | "
                f"samples: {len(messages)}"
            )
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ l∆∞u corrections dataset: {e}")

    def _load_corrections_dataset(
        self,
    ) -> Optional[Tuple[List[str], List[str]]]:
        """Th·ª≠ load merged corrections dataset t·ª´ cache, n·∫øu c√≥."""
        dataset_path = self._get_corrections_dataset_path()
        if not os.path.exists(dataset_path):
            return None
        try:
            with open(dataset_path, 'rb') as f:
                data = pickle.load(f)
            messages = data.get('messages', [])
            labels = data.get('labels', [])
            if len(messages) != len(labels):
                logger.warning(
                    "Corrections dataset kh√¥ng ƒë·ªìng nh·∫•t (messages != labels). B·ªè qua cache n√†y."
                )
                return None
            logger.info(
                "ƒê√£ load merged corrections dataset t·ª´ cache: %s | samples: %d",
                dataset_path,
                len(messages),
            )
            return messages, labels
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ load corrections dataset: {e}")
            return None
        
    def train_with_corrections(self) -> Dict[str, Any]:
        """
        üÜï Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu g·ªëc + corrections.
        
        Returns:
            Dict ch·ª©a th√¥ng tin training v·ªõi corrections
        """
        logger.info("ƒêang t·∫£i d·ªØ li·ªáu g·ªëc...")
        messages, labels = self.data_loader.load_data()
        
        corrections = self.load_corrections()
        logger.info(f"T√¨m th·∫•y {len(corrections)} corrections")
        
        if corrections:
            logger.info("ƒêang merge corrections v√†o dataset...")
            
            extended_messages = messages.copy()
            extended_labels = labels.copy()
            
            for email_id, correction in corrections.items():
                subject = correction.get('subject', '')
                sender = correction.get('sender', '')
                snippet = correction.get('snippet', '')
                
                correction_text = (
                    f"Subject: {subject}\nFrom: {sender}\n{snippet}"
                )
                
                corrected_label = correction.get('corrected_label', 'ham')
                # B·∫£o ƒë·∫£m preprocess cho corrections ƒë·ªÉ th·ªëng nh·∫•t v·ªõi CSV
                preprocessed_text = self.data_loader.preprocess_text(correction_text)
                extended_messages.append(preprocessed_text)
                extended_labels.append(corrected_label)
                
                logger.info(f"Th√™m correction: {email_id} -> {corrected_label}")
            
            messages = extended_messages
            labels = extended_labels
            
            logger.info(f"Dataset sau merge: {len(messages)} samples")

            # L∆∞u merged corrections dataset ƒë·ªÉ d√πng ·ªïn ƒë·ªãnh gi·ªØa c√°c l·∫ßn train
            self._save_corrections_dataset(messages, labels)
        
        return self._train_with_data(messages, labels, use_corrections=True)
        
    def train(self) -> None:
        """Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu g·ªëc."""
        logger.info("ƒêang t·∫£i d·ªØ li·ªáu...")
        messages, labels = self.data_loader.load_data()
        
        self._train_with_data(messages, labels, use_corrections=False)
        
    def _train_with_data(self, messages: list, labels: list,
                         use_corrections: bool = False) -> Dict[str, Any]:
        """
        üÜï Internal method ƒë·ªÉ train v·ªõi data ƒë∆∞·ª£c cung c·∫•p.
        
        Args:
            messages: List messages
            labels: List labels
            use_corrections: C√≥ s·ª≠ d·ª•ng corrections kh√¥ng
            
        Returns:
            Dict ch·ª©a th√¥ng tin training
        """
        # N·∫øu train v·ªõi corrections v√† c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ b√™n ngo√†i,
        # th√¨ s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√≥ thay v√¨ load t·ª´ cache
        original_messages_count = None
        if use_corrections:
            # Ch·ªâ load cache n·∫øu messages r·ªóng (t·ª©c l√† ƒë∆∞·ª£c g·ªçi tr·ª±c ti·∫øp)
            if len(messages) == 0:
                cached = self._load_corrections_dataset()
                if cached is not None:
                    messages, labels = cached
                    logger.info("ƒê√£ load merged corrections dataset t·ª´ cache")
            
            # Ghi nh·∫≠n s·ªë m·∫´u g·ªëc ƒë·ªÉ th·ªëng k√™
            try:
                orig_msgs, _ = self.data_loader.load_data()
                original_messages_count = len(orig_msgs)
            except Exception:
                original_messages_count = None

        total_samples = len(messages)
        training_info = {
            'total_samples': total_samples,
            'original_samples': (
                original_messages_count if (use_corrections and original_messages_count is not None)
                else (len(messages) if not use_corrections else None)
            ),
            'correction_samples': (
                (total_samples - original_messages_count)
                if (use_corrections and original_messages_count is not None)
                else (0 if not use_corrections else None)
            ),
            'use_corrections': use_corrections
        }
        
        if self.classifier_type == 'knn':
            # üÜï T·∫°o t√™n file cache d·ª±a tr√™n lo·∫°i training
            cache_suffix = "_with_corrections" if use_corrections else "_original"
            emb_file = os.path.join(
                'cache', 'embeddings',
                f"embeddings_{self.config.model_name.replace('/', '_')}{cache_suffix}.npy"
            )
            dataset_count = len(messages)
            
            logger.info(f"Training type: {'with corrections' if use_corrections else 'original'}")
            logger.info(f"Dataset count: {dataset_count}")
            logger.info(f"Cache file: {emb_file}")

            if os.path.exists(emb_file):
                try:
                    cache = np.load(emb_file)
                    cache_count = cache.shape[0]
                    logger.info(f"Cache count: {cache_count}")
                    
                    if (cache_count != dataset_count
                            and not self.config.regenerate_embeddings):
                        msg = (
                            f"S·ªë d√≤ng trong dataset ({dataset_count}) "
                            f"kh√¥ng kh·ªõp v·ªõi embeddings cache ({cache_count}). "
                            f"Training type: {'with corrections' if use_corrections else 'original'}. "
                            "Ch·∫°y l·∫°i v·ªõi --regenerate ƒë·ªÉ c·∫≠p nh·∫≠t."
                        )
                        raise ValueError(msg)
                    if (cache_count != dataset_count
                            and self.config.regenerate_embeddings):
                        logger.info(
                            f"X√≥a cache c≈©: {emb_file} "
                            f"(flag regenerate_embeddings=True, "
                            f"training type: {'with corrections' if use_corrections else 'original'})"
                        )
                        os.remove(emb_file)
                except Exception as e:
                    logger.error(f"L·ªói khi ki·ªÉm tra cache: {e}")
                    raise

        logger.info(f"C√°c l·ªõp: {self.data_loader.get_class_names()}")

        # Chia d·ªØ li·ªáu
        train_idx, _, y_train, _ = (
            self.data_loader.split_data(messages, labels)
        )
        train_msgs = [messages[i] for i in train_idx]
        train_lbls = [labels[i] for i in train_idx]
        
        if self.classifier_type == 'knn':
            # T·∫°o embeddings
            logger.info(f"T·∫°o embeddings cho {len(messages)} tin nh·∫Øn...")
            cache_suffix = "_with_corrections" if use_corrections else "_original"
            embeddings = self.embedding_generator.generate_embeddings(
                messages, cache_suffix=cache_suffix
            )
            logger.info(f"K√≠ch th∆∞·ªõc embeddings: {embeddings.shape}")

            # T·∫°o metadata
            encoded = self.data_loader.label_encoder.transform(labels)
            metadata = self.data_loader.create_metadata(
                messages, labels, encoded
            )

            # Chia embeddings v√† metadata
            train_emb = embeddings[train_idx]
            train_meta = [metadata[i] for i in train_idx]

            logger.info(f"K√≠ch th∆∞·ªõc t·∫≠p train: {len(train_emb)}")
            logger.info(f"Ph√¢n b·ªë nh√£n train: {np.bincount(y_train)}")

            # üÜï T·∫°o v√† hu·∫•n luy·ªán classifier v·ªõi cache FAISS index
            self.classifier = KNNClassifier(train_emb.shape[1])
            
            # Th·ª≠ load FAISS index t·ª´ cache tr∆∞·ªõc
            cache_suffix = "_with_corrections" if use_corrections else "_original"
            if not self.classifier.load_index(cache_suffix):
                # N·∫øu kh√¥ng c√≥ cache, train m·ªõi v√† l∆∞u cache
                self.classifier.fit(train_emb, train_meta)
                self.classifier.save_index(cache_suffix)

        elif self.classifier_type == 'tfidf':
            self.classifier = TFIDFClassifier()
            self.classifier.fit(train_msgs, train_lbls)

        else:
            raise ValueError(
                f"Lo·∫°i classifier kh√¥ng h·ª£p l·ªá: {self.classifier_type}"
            )
            
        training_info['train_samples'] = len(train_msgs)
        training_info['label_distribution'] = {
            label: train_lbls.count(label) for label in set(train_lbls)
        }
        
        logger.info(f"Training ho√†n t·∫•t: {training_info}")
        return training_info
            
    def predict(self, text: str, k: int = None) -> Dict[str, Any]:
        """
        Ph√¢n lo·∫°i m·ªôt vƒÉn b·∫£n.
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i
            k: S·ªë l∆∞·ª£ng neighbors (m·∫∑c ƒë·ªãnh t·ª´ config)
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n lo·∫°i
        """
        if self.classifier is None:
            raise ValueError("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. "
                           "H√£y g·ªçi train() tr∆∞·ªõc.")

        pre_text = self.data_loader.preprocess_text(text)

        if self.classifier_type == 'knn':
            k = k or self.config.default_k
            
            logger.info(f"\n***ƒêang ph√¢n lo·∫°i: '{text}'")
            logger.info(f"\n***S·ª≠ d·ª•ng top-{k} nearest neighbors")
            
            # T·∫°o query embedding
            query_embedding = self.embedding_generator.generate_query_embedding(
                text
            )
            
            # Ph√¢n lo·∫°i
            prediction, neighbors = self.classifier.predict(
                query_embedding, k=k
            )
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            logger.info(f"\n***D·ª± ƒëo√°n: {prediction.upper()}")
            logger.info("\n***Top neighbors:")
            for i, neighbor in enumerate(neighbors, 1):
                logger.info(f"{i}. Nh√£n: {neighbor['label']} | "
                            f"ƒêi·ªÉm: {neighbor['score']:.4f}")
                logger.info(f"Tin nh·∫Øn: {neighbor['message']}")
            
            # ƒê·∫øm ph√¢n b·ªë nh√£n
            labels = [n['label'] for n in neighbors]
            label_counts = {
                label: labels.count(label) for label in set(labels)
            }
            
            
            return {
                'prediction': prediction,
                'neighbors': neighbors,
                'label_distribution': label_counts
            }
            
        if self.classifier_type == 'tfidf':
            return self.classifier.predict(pre_text, return_proba=True)

        raise ValueError(
            f"Lo·∫°i classifier kh√¥ng h·ª£p l·ªá: {self.classifier_type}"
        )