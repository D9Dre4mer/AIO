"""
File ch·∫°y ch√≠nh cho pipeline ph√¢n lo·∫°i email spam.
"""
import numpy as np
import time
import signal
import pandas as pd
import logging
import os
import argparse
from spam_classifier import SpamClassifierPipeline
from config import SpamClassifierConfig
from email_handler import GmailHandler
from evaluator import ModelEvaluator

# Thi·∫øt l·∫≠p logging
log_dir = 'logs'
os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s: %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'spam_classifier.log')),
        logging.StreamHandler()
    ]
)
logger = logging.info

# Bi·∫øn ki·ªÉm so√°t tho√°t an to√†n
running = True


def signal_handler(sig: int, frame: str) -> None:
    """
    X·ª≠ l√Ω t√≠n hi·ªáu Ctrl+C ƒë·ªÉ tho√°t ch∆∞∆°ng tr√¨nh an to√†n.

    Args:
        sig: ID c·ªßa t√≠n hi·ªáu.
        frame: Frame hi·ªán t·∫°i.
    """
    global running
    running = False
    logger("Nh·∫≠n t√≠n hi·ªáu Ctrl+C. ƒêang d·ª´ng ch∆∞∆°ng tr√¨nh an to√†n...")


def prepare_evaluation_data(evaluator: ModelEvaluator,
                           config: SpamClassifierConfig) -> tuple:
    """
    T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu cho ƒë√°nh gi√° m√¥ h√¨nh.

    Args:
        evaluator: ƒê·ªëi t∆∞·ª£ng ModelEvaluator ƒë·ªÉ t·∫£i d·ªØ li·ªáu v√† t·∫°o embedding.
        config: ƒê·ªëi t∆∞·ª£ng c·∫•u h√¨nh ch·ª©a th√¥ng tin dataset v√† m√¥ h√¨nh.

    Returns:
        Tuple ch·ª©a (test_embeddings, test_metadata) cho ƒë√°nh gi√°.
    """
    # T·∫£i d·ªØ li·ªáu
    logger("ƒêang t·∫£i d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°...")
    messages, labels = evaluator.data_loader.load_data()

    # T·∫°o embedding
    logger(f"ƒêang t·∫°o embedding cho {len(messages)} tin nh·∫Øn...")
    # üÜï S·ª≠ d·ª•ng cache v·ªõi suffix _original cho evaluation
    embeddings = evaluator.embedding_generator.generate_embeddings(
        messages, cache_suffix="_original"
    )

    # Chia d·ªØ li·ªáu th√†nh t·∫≠p train/test
    logger("ƒêang chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test...")
    train_idx, test_idx, _, _ = evaluator.data_loader.split_data(messages, labels)

    # Chu·∫©n b·ªã embedding cho t·∫≠p test
    test_embeddings = embeddings[test_idx]

    # T·∫°o metadata cho t·∫≠p test
    logger("ƒêang t·∫°o metadata cho t·∫≠p test...")
    encoded_labels = evaluator.data_loader.label_encoder.transform(labels)
    metadata = evaluator.data_loader.create_metadata(messages, labels, encoded_labels)
    test_metadata = [metadata[i] for i in test_idx]

    return test_embeddings, test_metadata


def main():
    """
    H√†m ch√≠nh ƒë·ªÉ ch·∫°y pipeline ph√¢n lo·∫°i email spam.
    """
    parser = argparse.ArgumentParser(description="Ch·∫°y pipeline ph√¢n lo·∫°i email spam.")
    parser.add_argument('--regenerate', action='store_true',
                        help='T√°i t·∫°o embedding (m·∫∑c ƒë·ªãnh: False)')
    parser.add_argument('--run-email-classifier', action='store_true',
                        help='Ch·∫°y ch·∫ø ƒë·ªô ph√¢n lo·∫°i email qua Gmail API (m·∫∑c ƒë·ªãnh: False)')
    parser.add_argument('--merge-emails', action='store_true',
                        help='G·ªôp email t·ª´ th∆∞ m·ª•c inbox/spam v√†o dataset (m·∫∑c ƒë·ªãnh: False)')
    parser.add_argument('--evaluate', action='store_true',
                        help='Ch·∫°y ƒë√°nh gi√° m√¥ h√¨nh v·ªõi bi·ªÉu ƒë·ªì tr·ª±c quan (m·∫∑c ƒë·ªãnh: False)')
    parser.add_argument('--k-values', type=str,
                        help='Danh s√°ch gi√° tr·ªã k cho ƒë√°nh gi√°, ph√¢n t√°ch b·∫±ng d·∫•u ph·∫©y (v√≠ d·ª•: "1,3,5")')
    parser.add_argument('--classifier', type=str, default='knn', choices=['knn', 'tfidf'],
                        help='Ch·ªçn b·ªô ph√¢n lo·∫°i: knn (m·∫∑c ƒë·ªãnh) ho·∫∑c tfidf')
    args = parser.parse_args()

    try:
        # Kh·ªüi t·∫°o c·∫•u h√¨nh
        config = SpamClassifierConfig()
        config.regenerate_embeddings = args.regenerate
        if args.k_values:
            config.k_values = [int(k) for k in args.k_values.split(',')]
        logger(f"T√°i t·∫°o embedding: {config.regenerate_embeddings}")
        logger(f"Gi√° tr·ªã k: {config.k_values}")

        # T·∫°o pipeline
        logger("ƒêang kh·ªüi t·∫°o pipeline...")
        pipeline = SpamClassifierPipeline(config, classifier_type=args.classifier)

        # G·ªôp email n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if args.merge_emails:
            logger("ƒêang g·ªôp email t·ª´ th∆∞ m·ª•c inbox/spam v√†o dataset...")
            pipeline.data_loader.merge_emails_to_dataset()

            # üÜï Ki·ªÉm tra t√≠nh nh·∫•t qu√°n gi·ªØa dataset v√† cache embedding
            # S·ª≠ d·ª•ng cache cho dataset g·ªëc (kh√¥ng c√≥ corrections)
            dataset_path = config.dataset_path
            embeddings_file = os.path.join('cache', 'embeddings',
                                          f"embeddings_{config.model_name.replace('/', '_')}_original.npy")
            if os.path.exists(dataset_path) and os.path.exists(embeddings_file):
                df = pd.read_csv(dataset_path)
                dataset_count = len(df)
                embeddings = np.load(embeddings_file)
                cache_count = embeddings.shape[0]
                
                logger(f"Dataset count: {dataset_count}")
                logger(f"Cache count: {cache_count}")
                logger(f"Cache file: {embeddings_file}")
                
                if cache_count != dataset_count and not args.regenerate:
                    logger(
                        f"C·∫¢NH B√ÅO: S·ªë d√≤ng trong dataset ({dataset_count}) "
                        f"kh√¥ng kh·ªõp v·ªõi cache embedding ({cache_count}). "
                        "Ch·∫°y l·∫°i v·ªõi --regenerate ƒë·ªÉ c·∫≠p nh·∫≠t embedding."
                    )
                    return
                elif cache_count != dataset_count and args.regenerate:
                    logger("Ph√°t hi·ªán s·ªë d√≤ng kh√¥ng kh·ªõp. "
                           "ƒêang t√°i t·∫°o embedding...")

                    # üÜï Ki·ªÉm tra v√† ∆∞u ti√™n cache v·ªõi corrections cho Gmail classification
            if args.run_email_classifier:
                # Ki·ªÉm tra xem c√≥ cache _with_corrections kh√¥ng
                model_name_safe = config.model_name.replace('/', '_')
                corrections_cache_file = os.path.join(
                    'cache', 'embeddings',
                    f"embeddings_{model_name_safe}_with_corrections.npy"
                )
                original_cache_file = os.path.join(
                    'cache', 'embeddings',
                    f"embeddings_{model_name_safe}_original.npy"
                )
                
                # Ki·ªÉm tra s·ª± t·ªìn t·∫°i
                corrections_emb_exists = os.path.exists(corrections_cache_file)
                original_emb_exists = os.path.exists(original_cache_file)
                
                # Logic ∆∞u ti√™n cache - concise logging
                if corrections_emb_exists:
                    print(f"EMAIL SCAN: Using cache _with_corrections for Gmail classification")
                    print(f"FAISS INDEX: Loading from cache _with_corrections")
                    pipeline.train_with_corrections()
                elif original_emb_exists:
                    print(f"EMAIL SCAN: Using cache _original for Gmail classification")
                    print(f"FAISS INDEX: Loading from cache _original")
                    pipeline.train()
                else:
                    print(f"EMAIL SCAN: No cache found, training new model")
                    print(f"FAISS INDEX: Creating new index from original data")
                    pipeline.train()
        else:
            # Hu·∫•n luy·ªán m√¥ h√¨nh (ch·ªâ m·ªôt l·∫ßn cho pipeline ch√≠nh)
            logger("ƒêang b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...")
            pipeline.train()

        # ƒê√°nh gi√° m√¥ h√¨nh n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if args.evaluate:
            logger("ƒêang b·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh...")
            evaluator = ModelEvaluator(config)
            test_embeddings, test_metadata = prepare_evaluation_data(evaluator, config)
            
            # Init TF-IDF pipeline ri√™ng (train ch·ªâ m·ªôt l·∫ßn)
            tfidf_pipeline = SpamClassifierPipeline(config, classifier_type='tfidf')
            tfidf_pipeline.train()
            
            evaluator.evaluate_accuracy(
                test_embeddings, test_metadata, 
                pipeline.classifier, tfidf_pipeline.classifier, 
                config.k_values
            )
            return

        # Ch·∫°y ph√¢n lo·∫°i email n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if args.run_email_classifier:
            logger("ƒêang kh·ªüi ƒë·ªông ch·∫ø ƒë·ªô ph√¢n lo·∫°i email qua Gmail API "
                   "·ªü ch·∫ø ƒë·ªô n·ªÅn...")
            
            # Ki·ªÉm tra file credentials.json tr∆∞·ªõc khi kh·ªüi t·∫°o GmailHandler
            credentials_path = './cache/input/credentials.json'
            if not os.path.exists(credentials_path):
                logger(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file credentials.json t·∫°i: "
                       f"{credentials_path}")
                return
            
            try:
                handler = GmailHandler(pipeline, config)
                
                # Kh·ªüi t·∫°o Gmail service
                if not handler.initialize_for_main():
                    logger("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Gmail service. D·ª´ng ch∆∞∆°ng tr√¨nh.")
                    logger("üí° G·ª£i √Ω: Ki·ªÉm tra l·∫°i file credentials.json v√† "
                           "th·ª≠ x√≥a file token.json n·∫øu c√≥")
                    return
                
                last_page_token = None

                # ƒêƒÉng k√Ω tr√¨nh x·ª≠ l√Ω t√≠n hi·ªáu Ctrl+C
                signal.signal(signal.SIGINT, signal_handler)

                while running:
                    try:
                        # L·∫•y danh s√°ch email m·ªõi
                        results = handler.service.users().messages().list(
                            userId='me',
                            q='is:unread',
                            maxResults=10,
                            includeSpamTrash=True,
                            pageToken=last_page_token
                        ).execute()
                        messages = results.get('messages', [])

                        if messages:
                            logger(f"Ph√°t hi·ªán {len(messages)} email m·ªõi. "
                                   "ƒêang x·ª≠ l√Ω...")
                            handler.process_emails(max_results=10)
                            last_page_token = results.get('nextPageToken')
                        else:
                            logger("Kh√¥ng c√≥ email m·ªõi. Ch·ªù 30 gi√¢y...")

                        time.sleep(30)
                    except Exception as e:
                        logger(f"L·ªói khi x·ª≠ l√Ω email: {str(e)}")
                        time.sleep(60)

                logger("Ch∆∞∆°ng tr√¨nh ƒë√£ d·ª´ng an to√†n.")
                return
                
            except FileNotFoundError as e:
                logger(f"‚ùå L·ªói: {str(e)}")
                return
            except Exception as e:
                logger(f"‚ùå L·ªói kh√¥ng mong mu·ªën: {str(e)}")
                return

        # Ki·ªÉm tra v·ªõi c√°c v√≠ d·ª• m·∫´u
        logger("ƒêang ki·ªÉm tra pipeline v·ªõi c√°c v√≠ d·ª• m·∫´u...")
        test_examples = [
            "I am actually thinking a way of doing something useful",
            "FREE!! Click here to win $1000 NOW! Limited time offer!"
        ]
        for i, example in enumerate(test_examples, 1):
            logger(f"V√≠ d·ª• {i}: {example}")
            result = pipeline.predict(example, k=3)
            logger(f"D·ª± ƒëo√°n cho v√≠ d·ª• {i}: {result['prediction']}")

    except Exception as e:
        logger(f"L·ªói trong qu√° tr√¨nh ch·∫°y ch√≠nh: {str(e)}")
        raise


if __name__ == "__main__":
    main()