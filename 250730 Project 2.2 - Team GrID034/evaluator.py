"""
Module ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh.
"""
import os
import numpy as np
from typing import List, Dict, Any, Tuple
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
from tqdm import tqdm
import json
from datetime import datetime
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from matplotlib.gridspec import GridSpec
from knn_classifier import KNNClassifier
from config import SpamClassifierConfig
from data_loader import DataLoader
from embedding_generator import EmbeddingGenerator
from spam_classifier import SpamClassifierPipeline
from tfidf_classifier import TFIDFClassifier  # Th√™m import cho TF-IDF

logger = logging.getLogger(__name__)

class ModelEvaluator:
    """Class ƒë√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh."""
    
    def __init__(self, config: SpamClassifierConfig):
        """
        Kh·ªüi t·∫°o ModelEvaluator.
        
        Args:
            config: C·∫•u h√¨nh h·ªá th·ªëng
        """
        self.config = config
        self.data_loader = DataLoader(config)
        self.embedding_generator = EmbeddingGenerator(config)
    
    def evaluate_accuracy(self, 
                          test_embeddings: np.ndarray = None,  # Ch·ªâ cho KNN
                          test_metadata: List[Dict[str, Any]] = None,  # Ch·ªâ cho KNN
                          knn_classifier: KNNClassifier = None,  # KNN classifier
                          tfidf_classifier: TFIDFClassifier = None,  # Th√™m TF-IDF classifier
                          k_values: List[int] = None) -> Tuple[Dict, Dict]:
        """
        ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c cho c·∫£ KNN v√† TF-IDF.
        
        Args:
            test_embeddings: Embeddings c·ªßa test set (cho KNN)
            test_metadata: Metadata c·ªßa test set (cho KNN)
            knn_classifier: KNN classifier ƒë√£ train
            tfidf_classifier: TFIDF classifier ƒë√£ train
            k_values: Danh s√°ch c√°c gi√° tr·ªã k c·∫ßn test
            
        Returns:
            Tuple ch·ª©a k·∫øt qu·∫£ c√°c ch·ªâ s·ªë v√† errors
        """
        if k_values is None:
            k_values = self.config.k_values
            
        knn_results = {}
        knn_errors = {}
        knn_confusion_matrices = {}
        
        true_labels = [meta['label'] for meta in test_metadata]
        
        for k in k_values:
            correct = 0
            predictions = []
            errors = []
            
            for i in tqdm(range(len(test_embeddings)), desc=f"ƒê√°nh gi√° k={k}"):
                query_embedding = test_embeddings[i:i+1].astype('float32')
                true_label = test_metadata[i]['label']
                true_message = test_metadata[i]['message']
                
                pred, neighbors = knn_classifier.predict(query_embedding, k=k)
                predictions.append(pred)
                
                if pred == true_label:
                    correct += 1
                else:
                    error_info = {
                        'index': i,
                        'original_index': test_metadata[i]['index'],
                        'message': true_message,
                        'true_label': true_label,
                        'predicted_label': pred,
                        'neighbors': neighbors,
                        'label_distribution': {
                            label: sum(1 for n in neighbors if n['label'] == label)
                            for label in set(n['label'] for n in neighbors)
                        }
                    }
                    errors.append(error_info)
            
            # T√≠nh to√°n c√°c ch·ªâ s·ªë
            accuracy = correct / len(test_embeddings)
            precision, recall, f1, _ = precision_recall_fscore_support(
                true_labels, predictions, average='weighted', labels=self.data_loader.get_class_names()
            )
            cm = confusion_matrix(true_labels, predictions, labels=self.data_loader.get_class_names())
            
            knn_results[k] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            knn_errors[k] = errors
            knn_confusion_matrices[k] = cm
            
            logger.info(f"ƒê·ªô ch√≠nh x√°c v·ªõi k={k}: {accuracy:.4f}")
            logger.info(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}")
            logger.info(f"S·ªë l·ªói v·ªõi k={k}: {len(errors)}/{len(test_embeddings)} "
                        f"({(len(errors)/len(test_embeddings))*100:.2f}%)")
        
        # Ch·ªçn best K d·ª±a tr√™n max weighted F1
        best_k = max(knn_results, key=lambda k: knn_results[k]['f1'])
        logger.info(f"Best K: {best_k} d·ª±a tr√™n F1-score cao nh·∫•t ({knn_results[best_k]['f1']:.4f})")
        
        # TF-IDF evaluation
        tfidf_results = {}
        tfidf_predictions = []
        tfidf_errors = []
        messages, labels = self.data_loader.load_data()
        train_indices, test_indices, _, _ = self.data_loader.split_data(messages, labels)
        test_messages = [messages[i] for i in test_indices]
        true_labels = [labels[i] for i in test_indices]  # Reuse true_labels
        
        for i, msg in tqdm(enumerate(test_messages), desc="ƒê√°nh gi√° TF-IDF"):
            pred = tfidf_classifier.predict(msg)['prediction']
            tfidf_predictions.append(pred)
            if pred != true_labels[i]:
                tfidf_errors.append({'index': i, 'message': msg, 'true_label': true_labels[i], 'predicted_label': pred})
        
        tfidf_accuracy = sum(p == t for p, t in zip(tfidf_predictions, true_labels)) / len(true_labels)
        tfidf_precision, tfidf_recall, tfidf_f1, _ = precision_recall_fscore_support(
            true_labels, tfidf_predictions, average='weighted', labels=self.data_loader.get_class_names()
        )
        tfidf_cm = confusion_matrix(true_labels, tfidf_predictions, labels=self.data_loader.get_class_names())
        
        tfidf_results = {'accuracy': tfidf_accuracy, 'precision': tfidf_precision, 'recall': tfidf_recall, 'f1': tfidf_f1}
        logger.info(f"TF-IDF: Accuracy {tfidf_accuracy:.4f}, Precision {tfidf_precision:.4f}, Recall {tfidf_recall:.4f}, F1 {tfidf_f1:.4f}")
        
        # Combine cho viz
        combined_results = {'knn': knn_results, 'best_k': best_k, 'tfidf': tfidf_results}
        combined_cms = {'knn': knn_confusion_matrices, 'tfidf': tfidf_cm}
        
        # L∆∞u error analysis (th√™m TF-IDF)
        self.save_error_analysis(knn_results, knn_errors, len(true_labels), tfidf_results, tfidf_errors)
        
        # Visualize
        self._visualize_results(combined_results, combined_cms, messages, labels, k_values)
        
        return combined_results, knn_errors, combined_cms
    
    def save_error_analysis(self, 
                            knn_results: Dict,
                            knn_errors: Dict,
                            test_size: int,
                            tfidf_results: Dict = None,
                            tfidf_errors: List = None) -> None:
        """
        L∆∞u ph√¢n t√≠ch l·ªói v√†o file JSON.
        
        Args:
            knn_results: K·∫øt qu·∫£ KNN
            knn_errors: Errors KNN
            test_size: K√≠ch th∆∞·ªõc test set
            tfidf_results: K·∫øt qu·∫£ TF-IDF (optional)
            tfidf_errors: Errors TF-IDF (optional)
        """
        error_analysis = {
            'timestamp': datetime.now().isoformat(),
            'model': self.config.model_name,
            'test_size': test_size,
            'knn_results': knn_results,
            'errors_by_k': {
                f'k_{k}': {
                    'total_errors': len(errors),
                    'error_rate': len(errors) / test_size,
                    'errors': errors
                } for k, errors in knn_errors.items()
            }
        }
        
        if tfidf_results:
            error_analysis['tfidf_results'] = tfidf_results
            error_analysis['tfidf_errors'] = {
                'total_errors': len(tfidf_errors),
                'error_rate': len(tfidf_errors) / test_size,
                'errors': tfidf_errors
            }
        
        try:
            with open(self.config.output_file, 'w', encoding='utf-8') as f:
                json.dump(error_analysis, f, ensure_ascii=False, indent=2)
            logger.info(f"Ph√¢n t√≠ch l·ªói ƒë√£ l∆∞u v√†o: {self.config.output_file}")
            logger.info("T√≥m t·∫Øt:")
            for k in error_analysis['errors_by_k']:
                logger.info(f"   {k}: {error_analysis['errors_by_k'][k]['total_errors']} l·ªói")
            if tfidf_results:
                logger.info(f"   TF-IDF: {error_analysis['tfidf_errors']['total_errors']} l·ªói")
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u ph√¢n t√≠ch l·ªói: {str(e)}")
            raise
    
    def _visualize_results(self, results: Dict, confusion_matrices: Dict, messages: List[str], labels: List[str], k_values: List[int]) -> None:
        """
        T·∫°o v√† l∆∞u c√°c bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a.
        
        Args:
            results: K·∫øt qu·∫£ combined (knn, tfidf)
            confusion_matrices: Ma tr·∫≠n nh·∫ßm l·∫´n combined
            messages: Danh s√°ch tin nh·∫Øn
            labels: Danh s√°ch nh√£n
            k_values: Danh s√°ch c√°c gi√° tr·ªã k
        """
        # Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o
        if not results or not confusion_matrices or not labels:
            logger.error("D·ªØ li·ªáu ƒë·∫ßu v√†o cho tr·ª±c quan h√≥a r·ªóng.")
            return

        # S·ª≠ d·ª•ng style h·ª£p l·ªá c·ªßa Matplotlib
        plt.style.use('seaborn-v0_8')
        
        # T·∫°o l∆∞·ªõi subplot v·ªõi height_ratios ƒë·ªÉ gi√£n h√†ng v√† hspace cho padding
        n_k = len(k_values)
        fig = plt.figure(figsize=(6 * n_k, 22))  # TƒÉng height t·ªïng ƒë·ªÉ h·ªó tr·ª£ gi√£n
        gs = GridSpec(5, n_k, figure=fig, height_ratios=[1.2, 1.2, 1.2, 1.2, 1.2], hspace=0.5)  # height_ratios=1.2 cho m·ªói h√†ng, hspace=0.5
        
        # === H√†ng 1: Lineplot cho hi·ªáu su·∫•t c·ªßa top K (KNN) ===
        ax1 = fig.add_subplot(gs[0, :])  # H√†ng 0, t·∫•t c·∫£ c·ªôt
        knn_metrics_df = pd.DataFrame([
            {'k': k, 'Metric': metric, 'Value': results['knn'][k][metric]}
            for k in k_values
            for metric in ['accuracy', 'precision', 'recall', 'f1']
        ])
        sns.lineplot(data=knn_metrics_df, x='Metric', y='Value', hue='k', marker='o', ax=ax1)
        ax1.set_title("So s√°nh c√°c ch·ªâ s·ªë theo t·ª´ng k (KNN)")
        ax1.set_xlabel("Metric")
        ax1.set_ylabel("Score")
        values = knn_metrics_df['Value'].values
        mean_val = np.mean(values)
        std_val = np.std(values)
        y_min = max(0.9, mean_val - 1.5 * std_val)
        y_max = min(1.0, mean_val + 1.5 * std_val)
        ax1.set_ylim(y_min, y_max)
        ax1.legend(title='k')
        
        # === H√†ng 2: Heatmaps cho t·ª´ng top K (KNN) ===
        for idx, k in enumerate(k_values):
            ax = fig.add_subplot(gs[1, idx])  # H√†ng 1, c·ªôt idx
            sns.heatmap(confusion_matrices['knn'][k], annot=True, fmt='d', cmap='YlOrRd', cbar=False, ax=ax)
            ax.set_title(f'Confusion Matrix KNN (k={k})')
            ax.set_xlabel('Predicted')
            ax.set_ylabel('Actual')
            ax.set_xticklabels(self.data_loader.get_class_names())
            ax.set_yticklabels(self.data_loader.get_class_names())
        
        # === H√†ng 3: Barplot cho ph√¢n b·ªë nh√£n ===
        ax3 = fig.add_subplot(gs[2, :])  # H√†ng 2, t·∫•t c·∫£ c·ªôt
        df_bar = pd.DataFrame({'label': self.data_loader.get_class_names(), 'count': [sum(1 for label in labels if label == cls) for cls in self.data_loader.get_class_names()]})
        sns.barplot(data=df_bar, x='label', y='count', palette='Set2', ax=ax3)
        ax3.set_title("T·ªïng s·ªë email theo t·ª´ng nh√£n")
        ax3.set_xlabel("Label")
        ax3.set_ylabel("S·ªë l∆∞·ª£ng email")
        for p in ax3.patches:
            ax3.annotate(f'{p.get_height():.0f}', 
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='center', xytext=(0, 5), textcoords='offset points')
        
        # === H√†ng 4: Grouped barplot so s√°nh TF-IDF vs Best KNN ===
        ax4 = fig.add_subplot(gs[3, :])
        best_k = results['best_k']
        comp_df = pd.DataFrame([
            {'Classifier': 'TF-IDF', 'Metric': m, 'Value': results['tfidf'][m]} for m in ['accuracy', 'precision', 'recall', 'f1']
        ] + [
            {'Classifier': f'Best KNN (k={best_k})', 'Metric': m, 'Value': results['knn'][best_k][m]} for m in ['accuracy', 'precision', 'recall', 'f1']
        ])
        sns.barplot(data=comp_df, x='Metric', y='Value', hue='Classifier', palette='Set1', ax=ax4)
        ax4.set_title("So s√°nh TF-IDF vs Best KNN")
        ax4.set_ylabel("Score")
        ax4.legend(title='Classifier')
        values = comp_df['Value'].values
        mean_val = np.mean(values)
        std_val = np.std(values)
        ax4.set_ylim(max(0.9, mean_val - 1.5 * std_val), min(1.0, mean_val + 1.5 * std_val))
        
        # === H√†ng 5: Heatmap cho TF-IDF ===
        ax5 = fig.add_subplot(gs[4, :])
        sns.heatmap(confusion_matrices['tfidf'], annot=True, fmt='d', cmap='YlOrRd', cbar=False, ax=ax5)
        ax5.set_title('Confusion Matrix (TF-IDF)')
        ax5.set_xlabel('Predicted')
        ax5.set_ylabel('Actual')
        ax5.set_xticklabels(self.data_loader.get_class_names())
        ax5.set_yticklabels(self.data_loader.get_class_names())
        
        # L∆∞u bi·ªÉu ƒë·ªì g·ªôp v·ªõi DPI cao
        summary_file = os.path.join(self.config.output_dir, 'evaluation_summary.png')
        plt.savefig(summary_file, dpi=300)
        logger.info(f"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì g·ªôp v√†o: {summary_file}")
        plt.close()

    def plot_knn_metrics(self, knn_results: Dict, k_values: List[int]) -> plt.Figure:
        """Tr·∫£ v·ªÅ fig lineplot metrics KNN theo k."""
        fig, ax = plt.subplots(figsize=(8, 5))
        knn_metrics_df = pd.DataFrame([
            {'k': k, 'Metric': metric, 'Value': knn_results[k][metric]}
            for k in k_values
            for metric in ['accuracy', 'precision', 'recall', 'f1']
        ])
        sns.lineplot(data=knn_metrics_df, x='Metric', y='Value', hue='k', marker='o', ax=ax)
        ax.set_title("So s√°nh c√°c ch·ªâ s·ªë theo t·ª´ng k (KNN)")
        ax.set_xlabel("Metric")
        ax.set_ylabel("Score")
        values = knn_metrics_df['Value'].values
        mean_val = np.mean(values)
        std_val = np.std(values)
        ax.set_ylim(max(0.9, mean_val - 1.5 * std_val), min(1.0, mean_val + 1.5 * std_val))
        ax.legend(title='k')
        return fig

    def plot_knn_confusion(self, cm: np.ndarray, k: int) -> plt.Figure:
        """Tr·∫£ v·ªÅ fig heatmap confusion matrix cho KNN t·∫°i k."""
        fig, ax = plt.subplots(figsize=(5, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', cbar=False, ax=ax)
        ax.set_title(f'Confusion Matrix KNN (k={k})')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        ax.set_xticklabels(self.data_loader.get_class_names())
        ax.set_yticklabels(self.data_loader.get_class_names())
        return fig

    def plot_label_distribution(self, labels: List[str]) -> plt.Figure:
        """Tr·∫£ v·ªÅ fig barplot ph√¢n b·ªë labels."""
        fig, ax = plt.subplots(figsize=(6, 4))
        df_bar = pd.DataFrame({'label': self.data_loader.get_class_names(), 
                            'count': [sum(1 for label in labels if label == cls) 
                                        for cls in self.data_loader.get_class_names()]})
        sns.barplot(data=df_bar, x='label', y='count', palette='Set2', ax=ax)
        ax.set_title("T·ªïng s·ªë email theo t·ª´ng nh√£n")
        ax.set_xlabel("Label")
        ax.set_ylabel("S·ªë l∆∞·ª£ng email")
        for p in ax.patches:
            ax.annotate(f'{p.get_height():.0f}', 
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='center', xytext=(0, 5), textcoords='offset points')
        return fig

    def plot_comparison(self, knn_best: Dict, tfidf_results: Dict, best_k: int) -> plt.Figure:
        """Tr·∫£ v·ªÅ fig grouped bar so s√°nh TF-IDF vs best KNN."""
        fig, ax = plt.subplots(figsize=(8, 5))
        comp_df = pd.DataFrame([
            {'Classifier': 'TF-IDF', 'Metric': m, 'Value': tfidf_results[m]} 
            for m in ['accuracy', 'precision', 'recall', 'f1']
        ] + [
            {'Classifier': f'Best KNN (k={best_k})', 'Metric': m, 'Value': knn_best[m]} 
            for m in ['accuracy', 'precision', 'recall', 'f1']
        ])
        sns.barplot(data=comp_df, x='Metric', y='Value', hue='Classifier', palette='Set1', ax=ax)
        ax.set_title("So s√°nh TF-IDF vs Best KNN")
        ax.set_ylabel("Score")
        ax.legend(title='Classifier')
        values = comp_df['Value'].values
        mean_val = np.mean(values)
        std_val = np.std(values)
        ax.set_ylim(max(0.9, mean_val - 1.5 * std_val), min(1.0, mean_val + 1.5 * std_val))
        return fig

    def plot_tfidf_confusion(self, cm: np.ndarray) -> plt.Figure:
        """Tr·∫£ v·ªÅ fig heatmap confusion matrix cho TF-IDF."""
        fig, ax = plt.subplots(figsize=(5, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', cbar=False, ax=ax)
        ax.set_title('Confusion Matrix (TF-IDF)')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        ax.set_xticklabels(self.data_loader.get_class_names())
        ax.set_yticklabels(self.data_loader.get_class_names())
        return fig


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y ƒë√°nh gi√° m√¥ h√¨nh qua command line."""
    import argparse
    parser = argparse.ArgumentParser(description="Run model evaluation with visualization.")
    parser.add_argument('--evaluate', action='store_true', default=False,
                        help='Run model evaluation with visualization')
    parser.add_argument('--k-values', type=str, default=None,
                        help='Comma-separated list of k values to evaluate (e.g., "1,3,5")')
    parser.add_argument('--regenerate', action='store_true', default=False,
                        help='Regenerate embeddings before evaluation')
    args = parser.parse_args()
    
    if not args.evaluate:
        logger.info("Kh√¥ng ch·∫°y ƒë√°nh gi√°. S·ª≠ d·ª•ng --evaluate ƒë·ªÉ k√≠ch ho·∫°t.")
        return
    
    # Kh·ªüi t·∫°o c·∫•u h√¨nh v√† pipeline
    config = SpamClassifierConfig()
    config.regenerate_embeddings = args.regenerate
    if args.k_values:
        config.k_values = [int(k) for k in args.k_values.split(',')]
    
    # T·∫£i v√† hu·∫•n luy·ªán m√¥ h√¨nh
    pipeline = SpamClassifierPipeline(config)
    pipeline.train()
    
    # Ch·∫°y ƒë√°nh gi√°
    evaluator = ModelEvaluator(config)
    messages, labels = evaluator.data_loader.load_data()
    # üÜï S·ª≠ d·ª•ng cache v·ªõi suffix _original cho evaluation
    embeddings = evaluator.embedding_generator.generate_embeddings(
        messages, cache_suffix="_original"
    )
    train_indices, test_indices, y_train, y_test = evaluator.data_loader.split_data(messages, labels)
    test_embeddings = embeddings[test_indices]
    test_metadata = [evaluator.data_loader.create_metadata(messages, labels, evaluator.data_loader.label_encoder.transform(labels))[i] for i in test_indices]
    
    evaluator.evaluate_accuracy(test_embeddings, test_metadata, pipeline.classifier, config.k_values)

if __name__ == "__main__":
    main()