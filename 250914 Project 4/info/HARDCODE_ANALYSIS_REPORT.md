# PhÃ¢n TÃ­ch CÃ¡c Hardcode CÃ³ Thá»ƒ áº¢nh HÆ°á»Ÿng Äáº¿n Káº¿t Quáº£

## Tá»•ng Quan

Sau khi nghiÃªn cá»©u toÃ n bá»™ codebase, Ä‘Ã£ phÃ¡t hiá»‡n nhiá»u giÃ¡ trá»‹ hardcode cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ training vÃ  testing. BÃ¡o cÃ¡o nÃ y phÃ¢n tÃ­ch tá»«ng loáº¡i hardcode vÃ  Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tÃ¡c Ä‘á»™ng.

## 1. Hardcode LiÃªn Quan Äáº¿n Random State

### ğŸ”´ **Má»©c Äá»™ TÃ¡c Äá»™ng: CAO**

#### **CÃ¡c chá»— hardcode `random_state=42`:**

**a) Data Splitting:**
```python
# Trong táº¥t cáº£ comprehensive files
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Trong app.py
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**b) Cross-Validation:**
```python
# Trong táº¥t cáº£ comprehensive files
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

**c) Data Sampling:**
```python
# Trong comprehensive files
df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
```

**d) Ensemble Models:**
```python
# Trong app.py
final_estimator = LogisticRegression(random_state=42, max_iter=1000)
final_estimator = RandomForestClassifier(n_estimators=50, random_state=42)
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Reproducibility**: Káº¿t quáº£ luÃ´n giá»‘ng nhau
- **Bias**: CÃ³ thá»ƒ táº¡o bias do luÃ´n chá»n cÃ¹ng má»™t subset
- **Overfitting**: Model cÃ³ thá»ƒ overfit vá»›i split cá»¥ thá»ƒ
- **Evaluation**: KhÃ´ng Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c Ä‘á»™ á»•n Ä‘á»‹nh cá»§a model

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- Sá»­ dá»¥ng multiple random seeds Ä‘á»ƒ test
- Táº¡o config cho random_state thay vÃ¬ hardcode
- Cháº¡y experiment vá»›i nhiá»u random_state khÃ¡c nhau

## 2. Hardcode LiÃªn Quan Äáº¿n Optuna Configuration

### ğŸŸ¡ **Má»©c Äá»™ TÃ¡c Äá»™ng: TRUNG BÃŒNH**

#### **CÃ¡c chá»— hardcode Optuna:**

**a) Trials vÃ  Timeout:**
```python
# Trong táº¥t cáº£ comprehensive files
config = {
    'trials': 2,  # Ráº¤T THáº¤P - chá»‰ Ä‘á»ƒ test nhanh
    'timeout': 30,
    'direction': 'maximize'
}
```

**b) Default Values trong app.py:**
```python
# Trong app.py UI
n_trials = st.number_input("Number of Trials", min_value=10, max_value=200, value=50)
timeout = st.number_input("Timeout (minutes)", min_value=5, max_value=120, value=30)
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Underoptimization**: `trials=2` quÃ¡ tháº¥p, khÃ´ng tÃ¬m Ä‘Æ°á»£c optimal hyperparameters
- **Inconsistent**: Comprehensive files dÃ¹ng 2 trials, app.py dÃ¹ng 50 trials
- **Poor Performance**: Model performance khÃ´ng optimal do hyperparameter tuning kÃ©m

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- TÄƒng `trials` lÃªn Ã­t nháº¥t 20-50 cho comprehensive testing
- Táº¡o config file cho Optuna parameters
- Sá»­ dá»¥ng adaptive trials dá»±a trÃªn dataset size

## 3. Hardcode LiÃªn Quan Äáº¿n Data Split

### ğŸŸ¡ **Má»©c Äá»™ TÃ¡c Äá»™ng: TRUNG BÃŒNH**

#### **CÃ¡c chá»— hardcode test_size:**

```python
# Trong táº¥t cáº£ files
test_size=0.2  # 80-20 split
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Fixed Split**: LuÃ´n dÃ¹ng 80-20 split
- **Small Datasets**: Vá»›i dataset nhá», 20% test cÃ³ thá»ƒ quÃ¡ Ã­t
- **Large Datasets**: Vá»›i dataset lá»›n, 20% test cÃ³ thá»ƒ thá»«a

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- Dynamic test_size dá»±a trÃªn dataset size
- Smaller datasets: 70-30 split
- Larger datasets: 90-10 split

## 4. Hardcode LiÃªn Quan Äáº¿n Vectorization

### ğŸŸ¡ **Má»©c Äá»™ TÃ¡c Äá»™ng: TRUNG BÃŒNH**

#### **CÃ¡c chá»— hardcode vectorization parameters:**

**a) TF-IDF vÃ  BoW:**
```python
# Trong comprehensive files - QUÃ THáº¤P
'max_features': 1000,
'ngram_range': (1, 2),
'min_df': 2

# Trong app.py - TÆ¯Æ NG Äá»I Tá»T
'max_features': 10000,  # Default value
'ngram_range': (1, 2),
'min_df': 2
```

**b) Text Encoders:**
```python
# Trong text_encoders.py
min_df=2,           # Ignore words appearing in < 2 documents
max_df=0.95,        # Ignore words appearing in > 95% documents
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Under-representation**: `max_features=1000` quÃ¡ tháº¥p cÃ³ thá»ƒ máº¥t thÃ´ng tin
- **Inconsistency**: Comprehensive files (1000) vs app.py (10000)
- **Poor Text Representation**: CÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n accuracy cá»§a text classification

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- TÄƒng `max_features` trong comprehensive files lÃªn 5000-10000
- Táº¡o adaptive max_features dá»±a trÃªn dataset size
- Test vá»›i nhiá»u ngram_range khÃ¡c nhau

## 5. Hardcode LiÃªn Quan Äáº¿n Cross-Validation

### ğŸŸ¡ **Má»©c Äá»™ TÃ¡c Äá»™ng: TRUNG BÃŒNH**

#### **CÃ¡c chá»— hardcode CV:**

```python
# Trong táº¥t cáº£ files
n_splits=5  # 5-fold CV
cv_folds=5
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Fixed Folds**: LuÃ´n dÃ¹ng 5-fold
- **Small Datasets**: 5-fold cÃ³ thá»ƒ táº¡o fold quÃ¡ nhá»
- **Large Datasets**: 5-fold cÃ³ thá»ƒ Ä‘á»§ nhÆ°ng cÃ³ thá»ƒ tá»‘i Æ°u hÆ¡n

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- Dynamic CV folds dá»±a trÃªn dataset size
- Small datasets (n<500): 3-fold CV
- Medium datasets (500-5000): 5-fold CV
- Large datasets (>5000): 10-fold CV

## 6. Hardcode LiÃªn Quan Äáº¿n Sample Size

### ğŸŸ¡ **Má»©c Äá»™ TÃ¡c Äá»™ng: TRUNG BÃŒNH**

#### **CÃ¡c chá»— hardcode sample_size:**

```python
# Trong comprehensive files
def load_large_dataset(sample_size: int = 1000)
def load_spam_dataset(sample_size: int = 1000)

# Trong main functions
df, text_column, label_column = load_large_dataset(sample_size=1000)
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Limited Testing**: Chá»‰ test trÃªn 1000 samples tá»« 300K dataset
- **Unrepresentative**: 1000 samples cÃ³ thá»ƒ khÃ´ng Ä‘áº¡i diá»‡n cho toÃ n bá»™ dataset
- **Performance Bias**: Káº¿t quáº£ cÃ³ thá»ƒ khÃ¡c khi cháº¡y trÃªn full dataset

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- TÄƒng sample_size lÃªn 5000-10000 cho large dataset
- Táº¡o multiple sample sizes Ä‘á»ƒ test
- So sÃ¡nh performance giá»¯a cÃ¡c sample sizes

## 7. Hardcode Trong Model Parameters

### ğŸŸ¢ **Má»©c Äá»™ TÃ¡c Äá»™ng: THáº¤P**

#### **CÃ¡c chá»— hardcode model params:**

```python
# Trong app.py ensemble
final_estimator = LogisticRegression(random_state=42, max_iter=1000)
final_estimator = RandomForestClassifier(n_estimators=50, random_state=42)
```

#### **ğŸš¨ TÃ¡c Äá»™ng:**
- **Suboptimal**: CÃ¡c parameters nÃ y cÃ³ thá»ƒ khÃ´ng optimal
- **Limited**: KhÃ´ng test cÃ¡c configuration khÃ¡c

#### **ğŸ’¡ Khuyáº¿n Nghá»‹:**
- Äá»ƒ Optuna tá»± optimize cÃ¡c parameters nÃ y
- KhÃ´ng hardcode model-specific parameters

## 8. CÃ¡c Hardcode KhÃ¡c

### ğŸŸ¢ **Má»©c Äá»™ TÃ¡c Äá»™ng: THáº¤P - KHÃ”NG ÄÃNG Ká»‚**

#### **Formatting vÃ  Display:**
```python
# CÃ¡c hardcode khÃ´ng áº£nh hÆ°á»Ÿng káº¿t quáº£
print(f"   Success rate: {len(successful_results)/len(all_results)*100:.1f}%")
'max_iter': 1000  # Trong LogisticRegression - Ä‘á»§ cho háº§u háº¿t trÆ°á»ng há»£p
```

## ÄÃ¡nh GiÃ¡ Tá»•ng Thá»ƒ

### **ğŸ”´ Critical Issues (Cáº§n Sá»­a Ngay):**

1. **`trials=2` trong comprehensive files**
   - QuÃ¡ tháº¥p, khÃ´ng Ä‘á»§ Ä‘á»ƒ tÃ¬m optimal hyperparameters
   - TÃ¡c Ä‘á»™ng trá»±c tiáº¿p Ä‘áº¿n model performance

2. **`max_features=1000` trong vectorization**
   - QuÃ¡ tháº¥p cho text data, cÃ³ thá»ƒ máº¥t thÃ´ng tin quan trá»ng
   - TÃ¡c Ä‘á»™ng Ä‘áº¿n text classification accuracy

3. **`sample_size=1000` cho large dataset**
   - KhÃ´ng Ä‘áº¡i diá»‡n cho 300K samples
   - Káº¿t quáº£ khÃ´ng reflect performance thá»±c táº¿

### **ğŸŸ¡ Medium Issues (NÃªn Cáº£i Thiá»‡n):**

1. **`random_state=42` á»Ÿ má»i nÆ¡i**
   - Cáº§n test vá»›i multiple random seeds
   - ÄÃ¡nh giÃ¡ model stability

2. **Fixed `test_size=0.2`**
   - NÃªn adaptive dá»±a trÃªn dataset size

3. **Fixed `n_splits=5` cho CV**
   - NÃªn adaptive dá»±a trÃªn dataset size

### **ğŸŸ¢ Low Issues (CÃ³ Thá»ƒ Bá» Qua):**

1. Formatting parameters
2. Display configurations
3. Reasonable default values (nhÆ° max_iter=1000)

## Khuyáº¿n Nghá»‹ Cáº£i Thiá»‡n

### **1. Táº¡o Configuration System:**

```python
# config.py
DEFAULT_CONFIG = {
    'data_split': {
        'test_size': 0.2,
        'random_state': 42,
        'stratify': True
    },
    'optuna': {
        'trials': 50,  # TÄƒng tá»« 2
        'timeout': 300,  # 5 phÃºt
        'direction': 'maximize'
    },
    'vectorization': {
        'max_features': 10000,  # TÄƒng tá»« 1000
        'ngram_range': (1, 2),
        'min_df': 2
    },
    'cross_validation': {
        'n_splits': 5,
        'shuffle': True,
        'random_state': 42
    }
}
```

### **2. Dynamic Parameters:**

```python
def get_adaptive_config(dataset_size):
    if dataset_size < 500:
        return {'cv_folds': 3, 'test_size': 0.3}
    elif dataset_size < 5000:
        return {'cv_folds': 5, 'test_size': 0.2}
    else:
        return {'cv_folds': 10, 'test_size': 0.1}
```

### **3. Multiple Random Seeds Testing:**

```python
def test_with_multiple_seeds(seeds=[42, 123, 456, 789, 999]):
    results = []
    for seed in seeds:
        result = train_model(random_state=seed)
        results.append(result)
    return analyze_stability(results)
```

### **4. Increased Optuna Trials:**

```python
# Comprehensive files
config = {
    'trials': 50,  # TÄƒng tá»« 2
    'timeout': 600,  # 10 phÃºt thay vÃ¬ 30 giÃ¢y
    'direction': 'maximize'
}
```

### **5. Improved Vectorization:**

```python
# Comprehensive files
'max_features': 10000,  # TÄƒng tá»« 1000
'ngram_range': [(1,1), (1,2), (1,3)],  # Test multiple ranges
```

## Káº¿t Luáº­n

### **TÃ¡c Äá»™ng LÃªn Káº¿t Quáº£ Hiá»‡n Táº¡i:**

1. **Heart Dataset**: Ãt bá»‹ áº£nh hÆ°á»Ÿng vÃ¬ dataset dá»… classify
2. **Text Datasets**: Bá»‹ áº£nh hÆ°á»Ÿng nhiá»u hÆ¡n do `max_features=1000` vÃ  `trials=2`
3. **Overall Performance**: CÃ³ thá»ƒ chÆ°a optimal do underoptimization

### **Priority Actions:**

1. **Immediately**: TÄƒng `trials` tá»« 2 lÃªn 50 trong comprehensive files
2. **High Priority**: TÄƒng `max_features` tá»« 1000 lÃªn 10000
3. **Medium Priority**: Test vá»›i multiple random seeds
4. **Low Priority**: Táº¡o adaptive configuration system

### **Expected Improvements:**

- **Text Classification**: +2-5% accuracy vá»›i max_features tÄƒng
- **All Models**: +1-3% accuracy vá»›i trials tÄƒng
- **Stability**: Better understanding vá»›i multiple seeds
- **Reliability**: More robust evaluation vá»›i improved configuration

---

*BÃ¡o cÃ¡o nÃ y dá»±a trÃªn phÃ¢n tÃ­ch comprehensive toÃ n bá»™ codebase vÃ  Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a tá»«ng hardcode Ä‘áº¿n model performance.*
