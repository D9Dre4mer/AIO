# ğŸ“Š Heart vs Text Dataset Comparison Report

## ğŸ¯ Tá»•ng Quan

BÃ¡o cÃ¡o nÃ y so sÃ¡nh káº¿t quáº£ test Optuna giá»¯a hai loáº¡i dá»¯ liá»‡u:
- **Heart Dataset** (Numeric): 1025 samples, 13 features
- **Text Dataset** (Spam): 1000 samples, TF-IDF vectorized to 1000 features

## ğŸ“ˆ Káº¿t Quáº£ So SÃ¡nh

### ğŸ† **Performance Rankings**

#### **Heart Dataset (Numeric)**
```
ğŸ¥‡ Perfect Scores (100%):
- Random Forest: 100.0%
- XGBoost: 100.0% 
- LightGBM: 100.0%
- CatBoost: 100.0%
- Gradient Boosting: 100.0%

ğŸ¥ˆ Excellent Scores (95%+):
- Decision Tree: 98.5%

ğŸ¥‰ Good Scores (80-95%):
- AdaBoost: 89.8%
- Logistic Regression: 81.0%
- Linear SVC: 80.5%
- KNN: 86.3%
- Naive Bayes: 82.9%
- SVM: 77.6%
```

#### **Text Dataset (TF-IDF)**
```
ğŸ¥‡ Excellent Scores (94%+):
- Linear SVC: 94.0%
- AdaBoost: 94.0%

ğŸ¥ˆ Very Good Scores (90-94%):
- CatBoost: 93.5%
- LightGBM: 93.0%
- Gradient Boosting: 93.0%
- Random Forest: 91.0%
- Decision Tree: 91.5%
- Logistic Regression: 91.5%

ğŸ¥‰ Good Scores (80-90%):
- XGBoost: 89.0%
- SVM: 81.5%
- KNN: 82.5%
- Naive Bayes: 65.5%
```

## ğŸ” **PhÃ¢n TÃ­ch Chi Tiáº¿t**

### 1. **Tree-based Models Performance**

| Model | Heart (Numeric) | Text (TF-IDF) | Difference |
|-------|----------------|---------------|------------|
| Random Forest | 100.0% | 91.0% | -9.0% |
| XGBoost | 100.0% | 89.0% | -11.0% |
| LightGBM | 100.0% | 93.0% | -7.0% |
| CatBoost | 100.0% | 93.5% | -6.5% |
| Gradient Boosting | 100.0% | 93.0% | -7.0% |
| Decision Tree | 98.5% | 91.5% | -7.0% |

**ğŸ“Š Nháº­n xÃ©t**: Tree-based models hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n vá»›i numeric data, cÃ³ thá»ƒ do:
- Numeric features cÃ³ cáº¥u trÃºc rÃµ rÃ ng hÆ¡n cho splitting
- Text features sau TF-IDF cÃ³ thá»ƒ cÃ³ noise hoáº·c sparsity

### 2. **Linear Models Performance**

| Model | Heart (Numeric) | Text (TF-IDF) | Difference |
|-------|----------------|---------------|------------|
| Logistic Regression | 81.0% | 91.5% | +10.5% |
| Linear SVC | 80.5% | 94.0% | +13.5% |
| SVM | 77.6% | 81.5% | +3.9% |

**ğŸ“Š Nháº­n xÃ©t**: Linear models hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n vá»›i text data:
- TF-IDF features phÃ¹ há»£p vá»›i linear models
- Text classification thÆ°á»ng cÃ³ linear decision boundaries

### 3. **Ensemble Models Performance**

| Model | Heart (Numeric) | Text (TF-IDF) | Difference |
|-------|----------------|---------------|------------|
| AdaBoost | 89.8% | 94.0% | +4.2% |

**ğŸ“Š Nháº­n xÃ©t**: AdaBoost hoáº¡t Ä‘á»™ng tá»‘t vá»›i cáº£ hai loáº¡i data, Ä‘áº·c biá»‡t tá»‘t vá»›i text.

### 4. **Other Models Performance**

| Model | Heart (Numeric) | Text (TF-IDF) | Difference |
|-------|----------------|---------------|------------|
| KNN | 86.3% | 82.5% | -3.8% |
| Naive Bayes | 82.9% | 65.5% | -17.4% |

**ğŸ“Š Nháº­n xÃ©t**: 
- KNN: TÆ°Æ¡ng Ä‘á»‘i á»•n Ä‘á»‹nh
- Naive Bayes: Hoáº¡t Ä‘á»™ng kÃ©m vá»›i text data (cÃ³ thá»ƒ do TF-IDF khÃ´ng phÃ¹ há»£p vá»›i Gaussian assumption)

## âš¡ **Training Time Comparison**

### **Heart Dataset (Numeric)**
```
Fastest: Decision Tree (0.01s)
Slowest: LightGBM (9.67s)
Average: ~2.5s per model
```

### **Text Dataset (TF-IDF)**
```
Fastest: Linear SVC (0.01s)
Slowest: CatBoost (12.31s)
Average: ~3.5s per model
```

**ğŸ“Š Nháº­n xÃ©t**: Text processing máº¥t thá»i gian hÆ¡n do:
- TF-IDF vectorization overhead
- Higher dimensionality (1000 vs 13 features)
- Sparse matrix operations

## ğŸ¯ **Key Insights**

### 1. **Data Type Suitability**
- **Numeric Data**: Tree-based models excel (100% accuracy)
- **Text Data**: Linear models excel (94% accuracy)

### 2. **Model Selection Strategy**
- **For Numeric**: Prioritize Random Forest, XGBoost, LightGBM
- **For Text**: Prioritize Linear SVC, AdaBoost, CatBoost

### 3. **Feature Engineering Impact**
- TF-IDF vectorization táº¡o ra 1000 features tá»« text
- Numeric data chá»‰ cÃ³ 13 features nhÆ°ng hiá»‡u quáº£ hÆ¡n
- Quality > Quantity trong features

### 4. **GPU Acceleration**
- XGBoost, LightGBM, CatBoost Ä‘á»u sá»­ dá»¥ng GPU thÃ nh cÃ´ng
- GPU acceleration hoáº¡t Ä‘á»™ng tá»‘t vá»›i cáº£ numeric vÃ  text data

## ğŸ† **Best Practices Recommendations**

### **For Numeric Data**
1. **Primary**: Random Forest, XGBoost, LightGBM
2. **Secondary**: Gradient Boosting, Decision Tree
3. **Avoid**: SVM (77.6% accuracy)

### **For Text Data**
1. **Primary**: Linear SVC, AdaBoost
2. **Secondary**: CatBoost, LightGBM, Gradient Boosting
3. **Avoid**: Naive Bayes (65.5% accuracy)

### **Universal Models**
- **AdaBoost**: Hoáº¡t Ä‘á»™ng tá»‘t vá»›i cáº£ hai loáº¡i data
- **Logistic Regression**: Reliable baseline
- **KNN**: Stable performance across data types

## ğŸ“Š **Success Rate Summary**

| Dataset Type | Models Tested | Success Rate | Best Accuracy |
|--------------|---------------|--------------|---------------|
| **Heart (Numeric)** | 12/12 | 100% | 100.0% (Multiple) |
| **Text (TF-IDF)** | 12/12 | 100% | 94.0% (Linear SVC) |

## ğŸ‰ **Conclusion**

âœ… **Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng hoÃ n háº£o** vá»›i cáº£ hai loáº¡i dá»¯ liá»‡u:
- **100% success rate** cho táº¥t cáº£ models
- **Optuna optimization** hoáº¡t Ä‘á»™ng tá»‘t
- **GPU acceleration** Ä‘Æ°á»£c sá»­ dá»¥ng hiá»‡u quáº£
- **Automatic data detection** vÃ  processing hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c

ğŸš€ **Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng** Ä‘á»ƒ xá»­ lÃ½ cáº£ numeric vÃ  text data má»™t cÃ¡ch tá»± Ä‘á»™ng vÃ  hiá»‡u quáº£!

---
*Report generated on: 2025-09-25*
*Test environment: PJ3.1 conda environment*
*Datasets: heart.csv (numeric) + 2cls_spam_text_cls.csv (text)*
