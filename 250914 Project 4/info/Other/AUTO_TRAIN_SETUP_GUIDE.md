# Auto Train Scripts Setup Guide

## T·ªïng Quan

C√°c file auto_train scripts (`auto_train_spam_ham.py`, `auto_train_large_dataset.py`, `auto_train_heart_dataset.py`) ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ s·ª≠ d·ª•ng c√πng cache system v·ªõi `app.py`, ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n v√† kh·∫£ nƒÉng chia s·∫ª cache gi·ªØa c√°c script.

## C·∫•u H√¨nh Cache System

### 1. Cache Format Th·ªëng Nh·∫•t

T·∫•t c·∫£ auto_train scripts s·ª≠ d·ª•ng c√πng cache format v·ªõi `app.py`:

```
cache/models/{model_name}/{dataset_id}/{config_hash}/
‚îú‚îÄ‚îÄ model.pkl
‚îú‚îÄ‚îÄ params.json
‚îú‚îÄ‚îÄ metrics.json
‚îú‚îÄ‚îÄ config.json
‚îî‚îÄ‚îÄ fingerprint.json
```

### 2. Dataset ID Format

- **Text Data**: `{dataset_name}_{vectorization_method}`
  - V√≠ d·ª•: `spam_ham_dataset_TF-IDF`, `large_dataset_TF-IDF`
- **Numerical Data**: `{dataset_name}_{preprocessing_method}`
  - V√≠ d·ª•: `heart_dataset_StandardScaler`

### 3. Config Hash Structure

```json
{
  "model": "random_forest",
  "vectorization": "TF-IDF",  // ho·∫∑c "preprocessing": "StandardScaler"
  "trials": 10,
  "random_state": 42,
  "test_size": 0.2
}
```

## Chi Ti·∫øt T·ª´ng Script

### 1. auto_train_spam_ham.py

**Dataset**: Spam/Ham Text Classification
- **File**: `data/2cls_spam_text_cls.csv`
- **Text Column**: `Message`
- **Label Column**: `Category`
- **Vectorization Methods**: TF-IDF, BoW, Word Embeddings
- **Cache ID**: `spam_ham_dataset_{method}`

**Setup Parameters**:
```python
step3_data = {
    'optuna_config': {
        'trials': 10,           # Aligned with app.py default
        'timeout': 30,
        'direction': 'maximize'
    },
    'selected_models': get_all_models(),
    'vectorization_config': {
        'selected_methods': ['TF-IDF', 'BoW', 'Word Embeddings']
    },
    'selected_vectorization': ['TF-IDF', 'BoW', 'Word Embeddings']
}
```

### 2. auto_train_large_dataset.py

**Dataset**: Large Text Dataset (ArXiv Papers)
- **File**: `data/20250822-004129_sample-300_000Samples.csv`
- **Text Column**: `title`
- **Label Column**: `label`
- **Vectorization Methods**: TF-IDF, BoW, Word Embeddings
- **Cache ID**: `large_dataset_{method}`

**Setup Parameters**:
```python
step3_data = {
    'optuna_config': {
        'trials': 10,           # Aligned with app.py default
        'timeout': 30,
        'direction': 'maximize'
    },
    'selected_models': get_all_models(),
    'vectorization_config': {
        'selected_methods': ['TF-IDF', 'BoW', 'Word Embeddings']
    },
    'selected_vectorization': ['TF-IDF', 'BoW', 'Word Embeddings']
}
```

### 3. auto_train_heart_dataset.py

**Dataset**: Heart Disease Classification
- **File**: `data/heart.csv`
- **Feature Columns**: All numerical columns except target
- **Label Column**: `target`
- **Preprocessing Methods**: StandardScaler, MinMaxScaler, NoScaling
- **Cache ID**: `heart_dataset_{method}`

**Setup Parameters**:
```python
step3_data = {
    'optuna_config': {
        'trials': 10,           # Aligned with app.py default
        'timeout': 30,
        'direction': 'maximize'
    },
    'selected_models': get_all_models(),
    'preprocessing_config': {
        'selected_methods': ['StandardScaler', 'MinMaxScaler', 'NoScaling']
    }
}
```

## Cache Manager Integration

### Direct Cache Manager Usage

T·∫•t c·∫£ auto_train scripts s·ª≠ d·ª•ng `CacheManager` tr·ª±c ti·∫øp thay v√¨ `ComprehensiveEvaluator` ƒë·ªÉ ƒë·∫£m b·∫£o cache format t∆∞∆°ng ƒë·ªìng v·ªõi `app.py`:

```python
from cache_manager import CacheManager

# Generate cache identifiers using app.py format
cache_manager = CacheManager()
model_key = model_name
dataset_id = f"{dataset_name}_{method}"
config_hash = cache_manager.generate_config_hash({
    'model': model_name,
    'vectorization': method,  # ho·∫∑c 'preprocessing'
    'trials': 10,
    'random_state': 42,
    'test_size': 0.2
})
dataset_fingerprint = cache_manager.generate_dataset_fingerprint(
    dataset_path=dataset_path,
    dataset_size=os.path.getsize(dataset_path),
    num_rows=len(X_train)
)
```

### Cache Check v√† Save

```python
# Check if cache exists
cache_exists, cached_data = cache_manager.check_cache_exists(
    model_key, dataset_id, config_hash, dataset_fingerprint
)

if cache_exists:
    print(f"üíæ Cache hit! Loading cached results for {model_name}")
    return {'status': 'success', 'cache_hit': True}
else:
    # Train model and save cache
    cache_manager.save_model_cache(
        model_key=model_key,
        dataset_id=dataset_id,
        config_hash=config_hash,
        dataset_fingerprint=dataset_fingerprint,
        model=model,
        params=model.get_params(),
        metrics=metrics,
        config=config
    )
```

## Text Vectorization Methods

### TF-IDF Vectorization

```python
from text_encoders import TextVectorizer
vectorizer = TextVectorizer()

# Create TF-IDF features
X_train_tfidf = vectorizer.fit_transform_tfidf(X_train)
X_test_tfidf = vectorizer.transform_tfidf(X_test)
```

### Bag of Words (BoW)

```python
# Create BoW features
X_train_bow = vectorizer.fit_transform_bow(X_train)
X_test_bow = vectorizer.transform_bow(X_test)
```

### Word Embeddings

```python
from text_encoders import EmbeddingVectorizer
vectorizer = EmbeddingVectorizer(
    model_name='all-MiniLM-L6-v2',
    device='cpu'
)
X_train_embeddings = vectorizer.fit_transform(X_train)
X_test_embeddings = vectorizer.transform(X_test)
```

## Numerical Preprocessing

### StandardScaler

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### MinMaxScaler

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

## Data Splitting

T·∫•t c·∫£ scripts s·ª≠ d·ª•ng c√πng data splitting strategy:

```python
from sklearn.model_selection import train_test_split

# Split data: 80% train, 10% val, 10% test
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)
```

## Model Training v√† Evaluation

### Model Training

```python
from models import model_registry

# Get model class
model_class = model_registry.get_model(model_name)
model = model_class()

# Train model
model.fit(X_train_processed, y_train)

# Evaluate
y_pred = model.predict(X_test_processed)
```

### Metrics Calculation

```python
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

metrics = {
    'accuracy': accuracy_score(y_test, y_pred),
    'f1_score': f1_score(y_test, y_pred, average='weighted'),
    'precision': precision_score(y_test, y_pred, average='weighted'),
    'recall': recall_score(y_test, y_pred, average='weighted')
}
```

## Cache Reuse v·ªõi app.py

### Cache Hit Scenario

Khi auto_train scripts ch·∫°y l·∫ßn th·ª© 2, ch√∫ng s·∫Ω s·ª≠ d·ª•ng cache t·ª´ l·∫ßn ch·∫°y ƒë·∫ßu ti√™n:

```
üíæ Cache hit! Loading cached results for random_forest
```

### Cache Compatibility

- ‚úÖ **Same Dataset ID Format**: `{dataset_name}_{method}`
- ‚úÖ **Same Config Structure**: Model, method, trials, random_state, test_size
- ‚úÖ **Same Cache Directory**: `cache/models/{model_name}/{dataset_id}/`
- ‚úÖ **Same File Structure**: model.pkl, params.json, metrics.json, config.json, fingerprint.json

## L∆∞u √ù Quan Tr·ªçng

### 1. Environment Setup

ƒê·∫£m b·∫£o ch·∫°y trong m√¥i tr∆∞·ªùng conda `PJ3.1`:

```bash
conda activate PJ3.1
```

### 2. Required Imports

```python
from datetime import datetime  # Required for timestamp
from cache_manager import CacheManager
from models import model_registry
from text_encoders import TextVectorizer, EmbeddingVectorizer
```

### 3. Error Handling

T·∫•t c·∫£ scripts c√≥ error handling ƒë·ªÉ tr√°nh crash:

```python
try:
    # Training logic
    pass
except Exception as e:
    print(f"‚ùå Error: {e}")
    return {'status': 'failed', 'error': str(e)}
```

## K·∫øt Lu·∫≠n

C√°c auto_train scripts ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ho√†n h·∫£o ƒë·ªÉ:

1. **S·ª≠ d·ª•ng c√πng cache system** v·ªõi `app.py`
2. **Chia s·∫ª cache** gi·ªØa c√°c script v√† app.py
3. **ƒê·∫£m b·∫£o t√≠nh nh·∫•t qu√°n** trong training v√† evaluation
4. **T·ªëi ∆∞u h√≥a performance** th√¥ng qua cache reuse
5. **Kh√¥ng ·∫£nh h∆∞·ªüng** ƒë·∫øn ho·∫°t ƒë·ªông c·ªßa `app.py`

Vi·ªác setup n√†y cho ph√©p ng∆∞·ªùi d√πng c√≥ th·ªÉ train models th√¥ng qua auto_train scripts v√† sau ƒë√≥ s·ª≠ d·ª•ng k·∫øt qu·∫£ trong `app.py` m√† kh√¥ng c·∫ßn retrain.
