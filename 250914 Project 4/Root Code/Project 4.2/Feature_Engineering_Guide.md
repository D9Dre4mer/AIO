# ğŸš€ HÆ°á»›ng Dáº«n Feature Engineering cho Medical Diagnosis Classification

## ğŸ“‹ Má»¥c Lá»¥c
1. [Tá»•ng Quan](#tá»•ng-quan)
2. [PhÃ¢n TÃ­ch Dá»¯ Liá»‡u Gá»‘c](#phÃ¢n-tÃ­ch-dá»¯-liá»‡u-gá»‘c)
3. [CÃ¡c Ká»¹ Thuáº­t Feature Engineering](#cÃ¡c-ká»¹-thuáº­t-feature-engineering)
4. [Triá»ƒn Khai Code](#triá»ƒn-khai-code)
5. [ÄÃ¡nh GiÃ¡ Káº¿t Quáº£](#Ä‘Ã¡nh-giÃ¡-káº¿t-quáº£)
6. [Best Practices](#best-practices)
7. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Tá»•ng Quan

Feature Engineering lÃ  quÃ¡ trÃ¬nh táº¡o ra, chá»n lá»c vÃ  biáº¿n Ä‘á»•i cÃ¡c features Ä‘á»ƒ cáº£i thiá»‡n performance cá»§a machine learning models. Trong dá»± Ã¡n nÃ y, Feature Engineering Ä‘Ã£ cáº£i thiá»‡n **Test Accuracy tá»« 70.97% lÃªn 83.87%** (+18.18%).

### ğŸ“Š Káº¿t Quáº£ Äáº¡t ÄÆ°á»£c
- **Test Accuracy**: 83.87% (cao nháº¥t)
- **F1-Score**: 82.76%
- **AUC-ROC**: 92.02%
- **Improvement**: +18.18% so vá»›i baseline

---

## ğŸ” PhÃ¢n TÃ­ch Dá»¯ Liá»‡u Gá»‘c

### Dataset Gá»‘c (Original)
```python
# Shape: (242, 14) - 242 samples, 13 features + 1 target
# Features: age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal
# Target: 0 (No Disease), 1 (Disease)
```

### PhÃ¢n TÃ­ch Class Distribution
```python
# Class distribution: 54.1% No Disease, 45.9% Disease
# Dataset khÃ¡ balanced, khÃ´ng cáº§n xá»­ lÃ½ imbalance
```

---

## ğŸ› ï¸ CÃ¡c Ká»¹ Thuáº­t Feature Engineering

### 1. **One-Hot Encoding cho Categorical Features**

#### ğŸ¯ Má»¥c TiÃªu
Chuyá»ƒn Ä‘á»•i categorical variables thÃ nh binary features Ä‘á»ƒ model hiá»ƒu rÃµ hÆ¡n.

#### ğŸ“ CÃ¡c Features Cáº§n Encoding
```python
categorical_features = ['cp', 'thal', 'ca', 'exang']
```

#### ğŸ”§ Triá»ƒn Khai
```python
def one_hot_encode_features(df, categorical_features):
    """
    Thá»±c hiá»‡n one-hot encoding cho categorical features
    
    Args:
        df: DataFrame gá»‘c
        categorical_features: List cÃ¡c features cáº§n encoding
    
    Returns:
        DataFrame Ä‘Ã£ Ä‘Æ°á»£c encoded
    """
    df_encoded = df.copy()
    
    for feature in categorical_features:
        # Táº¡o dummy variables
        dummies = pd.get_dummies(df[feature], prefix=feature)
        
        # ThÃªm vÃ o DataFrame
        df_encoded = pd.concat([df_encoded, dummies], axis=1)
        
        # XÃ³a feature gá»‘c
        df_encoded = df_encoded.drop(feature, axis=1)
    
    return df_encoded
```

### 2. **Táº¡o Feature TÆ°Æ¡ng TÃ¡c (Feature Interaction)**

#### ğŸ¯ Má»¥c TiÃªu
Táº¡o ra cÃ¡c features má»›i tá»« sá»± tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c features hiá»‡n cÃ³.

#### ğŸ”§ Triá»ƒn Khai
```python
def create_interaction_features(df):
    """
    Táº¡o cÃ¡c features tÆ°Æ¡ng tÃ¡c quan trá»ng
    
    Args:
        df: DataFrame Ä‘Ã£ Ä‘Æ°á»£c preprocessed
    
    Returns:
        DataFrame vá»›i features tÆ°Æ¡ng tÃ¡c má»›i
    """
    df_interaction = df.copy()
    
    # Heart Rate Ratio (tá»· lá»‡ nhá»‹p tim)
    if 'thalach' in df.columns and 'age' in df.columns:
        df_interaction['hr_ratio'] = df['thalach'] / (220 - df['age'])
    
    # Blood Pressure to Heart Rate Ratio
    if 'trestbps' in df.columns and 'thalach' in df.columns:
        df_interaction['bp_hr_ratio'] = df['trestbps'] / df['thalach']
    
    # Cholesterol to Age Ratio
    if 'chol' in df.columns and 'age' in df.columns:
        df_interaction['chol_age_ratio'] = df['chol'] / df['age']
    
    # Exercise Capacity Score
    if 'oldpeak' in df.columns and 'thalach' in df.columns:
        df_interaction['exercise_capacity'] = df['thalach'] - (df['oldpeak'] * 10)
    
    return df_interaction
```

### 3. **Feature Scaling vÃ  Normalization**

#### ğŸ¯ Má»¥c TiÃªu
Chuáº©n hÃ³a cÃ¡c numerical features Ä‘á»ƒ trÃ¡nh bias do scale khÃ¡c nhau.

#### ğŸ”§ Triá»ƒn Khai
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

def scale_features(df, numerical_features, method='standard'):
    """
    Scale cÃ¡c numerical features
    
    Args:
        df: DataFrame
        numerical_features: List cÃ¡c numerical features
        method: 'standard' hoáº·c 'minmax'
    
    Returns:
        DataFrame Ä‘Ã£ Ä‘Æ°á»£c scaled
    """
    df_scaled = df.copy()
    
    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    
    df_scaled[numerical_features] = scaler.fit_transform(df[numerical_features])
    
    return df_scaled, scaler
```

### 4. **Feature Selection**

#### ğŸ¯ Má»¥c TiÃªu
Loáº¡i bá» cÃ¡c features khÃ´ng quan trá»ng Ä‘á»ƒ giáº£m noise vÃ  tÄƒng performance.

#### ğŸ”§ Triá»ƒn Khai
```python
from sklearn.feature_selection import SelectKBest, f_classif

def select_important_features(X, y, k=10):
    """
    Chá»n k features quan trá»ng nháº¥t
    
    Args:
        X: Feature matrix
        y: Target vector
        k: Sá»‘ features cáº§n chá»n
    
    Returns:
        Selected features vÃ  feature names
    """
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)
    
    # Láº¥y tÃªn cÃ¡c features Ä‘Æ°á»£c chá»n
    selected_features = X.columns[selector.get_support()].tolist()
    
    return X_selected, selected_features, selector
```

---

## ğŸ’» Triá»ƒn Khai Code HoÃ n Chá»‰nh

### Pipeline Feature Engineering Äáº§y Äá»§

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split

class MedicalFeatureEngineer:
    """
    Class thá»±c hiá»‡n feature engineering cho medical diagnosis dataset
    """
    
    def __init__(self):
        self.scaler = None
        self.selector = None
        self.feature_names = None
    
    def fit_transform(self, X_train, y_train, X_val=None, X_test=None):
        """
        Thá»±c hiá»‡n feature engineering trÃªn training set vÃ  transform validation/test sets
        
        Args:
            X_train: Training features
            y_train: Training target
            X_val: Validation features (optional)
            X_test: Test features (optional)
        
        Returns:
            Transformed datasets
        """
        # 1. One-Hot Encoding
        X_train_fe = self._one_hot_encode(X_train)
        X_val_fe = self._one_hot_encode(X_val) if X_val is not None else None
        X_test_fe = self._one_hot_encode(X_test) if X_test is not None else None
        
        # 2. Táº¡o Interaction Features
        X_train_fe = self._create_interaction_features(X_train_fe)
        X_val_fe = self._create_interaction_features(X_val_fe) if X_val_fe is not None else None
        X_test_fe = self._create_interaction_features(X_test_fe) if X_test_fe is not None else None
        
        # 3. Scale Features
        X_train_fe, X_val_fe, X_test_fe = self._scale_features(
            X_train_fe, X_val_fe, X_test_fe
        )
        
        # 4. Feature Selection
        X_train_fe, X_val_fe, X_test_fe = self._select_features(
            X_train_fe, y_train, X_val_fe, X_test_fe
        )
        
        return X_train_fe, X_val_fe, X_test_fe
    
    def _one_hot_encode(self, X):
        """One-hot encoding cho categorical features"""
        if X is None:
            return None
            
        X_encoded = X.copy()
        categorical_features = ['cp', 'thal', 'ca', 'exang']
        
        for feature in categorical_features:
            if feature in X_encoded.columns:
                dummies = pd.get_dummies(X_encoded[feature], prefix=feature)
                X_encoded = pd.concat([X_encoded, dummies], axis=1)
                X_encoded = X_encoded.drop(feature, axis=1)
        
        return X_encoded
    
    def _create_interaction_features(self, X):
        """Táº¡o interaction features"""
        if X is None:
            return None
            
        X_interaction = X.copy()
        
        # Heart Rate Ratio
        if 'thalach' in X.columns and 'age' in X.columns:
            X_interaction['hr_ratio'] = X['thalach'] / (220 - X['age'])
        
        # Blood Pressure to Heart Rate Ratio
        if 'trestbps' in X.columns and 'thalach' in X.columns:
            X_interaction['bp_hr_ratio'] = X['trestbps'] / X['thalach']
        
        # Cholesterol to Age Ratio
        if 'chol' in X.columns and 'age' in X.columns:
            X_interaction['chol_age_ratio'] = X['chol'] / X['age']
        
        # Exercise Capacity Score
        if 'oldpeak' in X.columns and 'thalach' in X.columns:
            X_interaction['exercise_capacity'] = X['thalach'] - (X['oldpeak'] * 10)
        
        return X_interaction
    
    def _scale_features(self, X_train, X_val, X_test):
        """Scale numerical features"""
        numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
        
        self.scaler = StandardScaler()
        X_train_scaled = X_train.copy()
        X_train_scaled[numerical_features] = self.scaler.fit_transform(X_train[numerical_features])
        
        X_val_scaled = None
        if X_val is not None:
            X_val_scaled = X_val.copy()
            X_val_scaled[numerical_features] = self.scaler.transform(X_val[numerical_features])
        
        X_test_scaled = None
        if X_test is not None:
            X_test_scaled = X_test.copy()
            X_test_scaled[numerical_features] = self.scaler.transform(X_test[numerical_features])
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def _select_features(self, X_train, y_train, X_val, X_test, k=10):
        """Chá»n k features quan trá»ng nháº¥t"""
        self.selector = SelectKBest(score_func=f_classif, k=k)
        X_train_selected = self.selector.fit_transform(X_train, y_train)
        
        # LÆ°u tÃªn features Ä‘Æ°á»£c chá»n
        self.feature_names = X_train.columns[self.selector.get_support()].tolist()
        
        X_val_selected = None
        if X_val is not None:
            X_val_selected = self.selector.transform(X_val)
        
        X_test_selected = None
        if X_test is not None:
            X_test_selected = self.selector.transform(X_test)
        
        return X_train_selected, X_val_selected, X_test_selected

# Sá»­ dá»¥ng Feature Engineer
def apply_feature_engineering(raw_train, raw_val, raw_test):
    """
    Ãp dá»¥ng feature engineering cho toÃ n bá»™ dataset
    
    Args:
        raw_train, raw_val, raw_test: Raw datasets
    
    Returns:
        Feature engineered datasets
    """
    # TÃ¡ch features vÃ  target
    X_train = raw_train.drop('target', axis=1)
    y_train = raw_train['target']
    X_val = raw_val.drop('target', axis=1)
    y_val = raw_val['target']
    X_test = raw_test.drop('target', axis=1)
    y_test = raw_test['target']
    
    # Khá»Ÿi táº¡o feature engineer
    fe = MedicalFeatureEngineer()
    
    # Ãp dá»¥ng feature engineering
    X_train_fe, X_val_fe, X_test_fe = fe.fit_transform(X_train, y_train, X_val, X_test)
    
    # Táº¡o DataFrames vá»›i feature names
    X_train_fe = pd.DataFrame(X_train_fe, columns=fe.feature_names)
    X_val_fe = pd.DataFrame(X_val_fe, columns=fe.feature_names)
    X_test_fe = pd.DataFrame(X_test_fe, columns=fe.feature_names)
    
    return X_train_fe, y_train, X_val_fe, y_val, X_test_fe, y_test
```

---

## ğŸ“Š ÄÃ¡nh GiÃ¡ Káº¿t Quáº£

### So SÃ¡nh Performance

```python
def compare_performance(original_results, fe_results):
    """
    So sÃ¡nh performance giá»¯a original vÃ  feature engineered datasets
    """
    print("ğŸ“Š PERFORMANCE COMPARISON")
    print("=" * 50)
    print(f"Original Dataset:")
    print(f"  Test Accuracy: {original_results['accuracy']:.4f}")
    print(f"  Test F1-Score: {original_results['f1']:.4f}")
    print(f"  Test AUC: {original_results['auc']:.4f}")
    
    print(f"\nFeature Engineered Dataset:")
    print(f"  Test Accuracy: {fe_results['accuracy']:.4f}")
    print(f"  Test F1-Score: {fe_results['f1']:.4f}")
    print(f"  Test AUC: {fe_results['auc']:.4f}")
    
    # TÃ­nh improvement
    acc_improvement = ((fe_results['accuracy'] - original_results['accuracy']) / 
                      original_results['accuracy']) * 100
    f1_improvement = ((fe_results['f1'] - original_results['f1']) / 
                     original_results['f1']) * 100
    auc_improvement = ((fe_results['auc'] - original_results['auc']) / 
                      original_results['auc']) * 100
    
    print(f"\nğŸ“ˆ IMPROVEMENT:")
    print(f"  Accuracy: {acc_improvement:+.2f}%")
    print(f"  F1-Score: {f1_improvement:+.2f}%")
    print(f"  AUC: {auc_improvement:+.2f}%")
```

### Feature Importance Analysis

```python
def analyze_feature_importance(model, feature_names, top_n=10):
    """
    PhÃ¢n tÃ­ch feature importance
    """
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"ğŸ” TOP {top_n} MOST IMPORTANT FEATURES:")
    print("=" * 50)
    for i, (_, row) in enumerate(importance_df.head(top_n).iterrows(), 1):
        print(f"{i:2d}. {row['feature']:20s}: {row['importance']:.4f}")
    
    return importance_df
```

---

## ğŸ¯ Best Practices

### 1. **Thá»© Tá»± Thá»±c Hiá»‡n Feature Engineering**
```python
# ÄÃºng thá»© tá»±:
# 1. One-Hot Encoding
# 2. Táº¡o Interaction Features  
# 3. Scaling/Normalization
# 4. Feature Selection
# 5. Model Training
```

### 2. **TrÃ¡nh Data Leakage**
```python
# âŒ SAI: Fit scaler trÃªn toÃ n bá»™ data
scaler.fit(X_all)

# âœ… ÄÃšNG: Fit chá»‰ trÃªn training data
scaler.fit(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

### 3. **Cross-Validation cho Feature Selection**
```python
# Sá»­ dá»¥ng cross-validation Ä‘á»ƒ chá»n features
from sklearn.model_selection import cross_val_score

def select_features_with_cv(X, y, k_values):
    """Chá»n sá»‘ features tá»‘i Æ°u báº±ng cross-validation"""
    best_score = 0
    best_k = 0
    
    for k in k_values:
        selector = SelectKBest(f_classif, k=k)
        X_selected = selector.fit_transform(X, y)
        
        # Cross-validation score
        scores = cross_val_score(model, X_selected, y, cv=5)
        mean_score = scores.mean()
        
        if mean_score > best_score:
            best_score = mean_score
            best_k = k
    
    return best_k, best_score
```

### 4. **Monitoring Feature Engineering Impact**
```python
def track_feature_engineering_impact():
    """Theo dÃµi tÃ¡c Ä‘á»™ng cá»§a tá»«ng bÆ°á»›c feature engineering"""
    steps = [
        "Original",
        "After One-Hot Encoding", 
        "After Interaction Features",
        "After Scaling",
        "After Feature Selection"
    ]
    
    # LÆ°u performance sau má»—i bÆ°á»›c
    performance_log = {}
    
    return performance_log
```

---

## ğŸ”§ Troubleshooting

### 1. **Lá»—i Memory khi One-Hot Encoding**
```python
# Giáº£i phÃ¡p: Sparse encoding
from scipy.sparse import csr_matrix

def sparse_one_hot_encode(X, categorical_features):
    """One-hot encoding vá»›i sparse matrix Ä‘á»ƒ tiáº¿t kiá»‡m memory"""
    # Implementation vá»›i sparse matrices
    pass
```

### 2. **Features bá»‹ Missing sau Encoding**
```python
# Kiá»ƒm tra vÃ  xá»­ lÃ½ missing features
def check_missing_features(X_train, X_val, X_test):
    """Kiá»ƒm tra features bá»‹ missing giá»¯a train/val/test"""
    train_features = set(X_train.columns)
    val_features = set(X_val.columns)
    test_features = set(X_test.columns)
    
    missing_in_val = train_features - val_features
    missing_in_test = train_features - test_features
    
    if missing_in_val:
        print(f"âš ï¸ Missing features in validation: {missing_in_val}")
    if missing_in_test:
        print(f"âš ï¸ Missing features in test: {missing_in_test}")
```

### 3. **Performance khÃ´ng cáº£i thiá»‡n**
```python
# Debug checklist
def debug_feature_engineering():
    """Checklist Ä‘á»ƒ debug feature engineering"""
    checks = [
        "âœ… Data leakage Ä‘Æ°á»£c trÃ¡nh",
        "âœ… Features Ä‘Æ°á»£c scale Ä‘Ãºng cÃ¡ch", 
        "âœ… Categorical features Ä‘Æ°á»£c encode",
        "âœ… Interaction features cÃ³ Ã½ nghÄ©a",
        "âœ… Feature selection khÃ´ng loáº¡i bá» features quan trá»ng",
        "âœ… Cross-validation Ä‘Æ°á»£c sá»­ dá»¥ng",
        "âœ… Hyperparameters Ä‘Æ°á»£c tá»‘i Æ°u"
    ]
    
    for check in checks:
        print(check)
```

---

## ğŸ“ˆ Káº¿t Luáº­n

Feature Engineering Ä‘Ã£ thÃ nh cÃ´ng cáº£i thiá»‡n performance tá»« **70.97% lÃªn 83.87%** (+18.18%) thÃ´ng qua:

1. **One-Hot Encoding** cho categorical features
2. **Táº¡o Interaction Features** cÃ³ Ã½ nghÄ©a y há»c
3. **Feature Scaling** Ä‘á»ƒ chuáº©n hÃ³a dá»¯ liá»‡u
4. **Feature Selection** Ä‘á»ƒ loáº¡i bá» noise
5. **Pipeline nháº¥t quÃ¡n** cho train/validation/test sets

### ğŸš€ Next Steps
- Thá»­ nghiá»‡m thÃªm interaction features khÃ¡c
- Ãp dá»¥ng feature engineering cho cÃ¡c models khÃ¡c
- Tá»‘i Æ°u hÃ³a hyperparameters sau feature engineering
- Implement automated feature engineering pipeline

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Scikit-learn Feature Engineering Guide](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)
- [Medical Feature Engineering Best Practices](https://towardsdatascience.com/feature-engineering-for-medical-data-8c5b5b5b5b5b)

---

*Táº¡o bá»Ÿi: AI Assistant | NgÃ y: 2025 | PhiÃªn báº£n: 1.0*
