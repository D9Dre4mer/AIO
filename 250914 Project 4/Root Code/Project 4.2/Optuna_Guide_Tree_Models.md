# üå≥ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Optuna cho c√°c m√¥ h√¨nh c√¢y

## üìã M·ª•c l·ª•c
1. [Gi·ªõi thi·ªáu Optuna](#gi·ªõi-thi·ªáu-optuna)
2. [C√†i ƒë·∫∑t](#c√†i-ƒë·∫∑t)
3. [C·∫•u tr√∫c c∆° b·∫£n](#c·∫•u-tr√∫c-c∆°-b·∫£n)
4. [XGBoost v·ªõi Optuna](#xgboost-v·ªõi-optuna)
5. [LightGBM v·ªõi Optuna](#lightgbm-v·ªõi-optuna)
6. [Random Forest v·ªõi Optuna](#random-forest-v·ªõi-optuna)
7. [Gradient Boosting v·ªõi Optuna](#gradient-boosting-v·ªõi-optuna)
8. [AdaBoost v·ªõi Optuna](#adaboost-v·ªõi-optuna)
9. [Visualization](#visualization)
10. [Best Practices](#best-practices)

---

## üéØ Gi·ªõi thi·ªáu Optuna

**Optuna** l√† th∆∞ vi·ªán Python m·∫°nh m·∫Ω ƒë·ªÉ t·ªëi ∆∞u h√≥a hyperparameters t·ª± ƒë·ªông, ƒë·∫∑c bi·ªát hi·ªáu qu·∫£ v·ªõi c√°c m√¥ h√¨nh c√¢y.

### ∆Øu ƒëi·ªÉm:
- ‚úÖ **T·ª± ƒë·ªông t√¨m ki·∫øm**: Kh√¥ng c·∫ßn test th·ªß c√¥ng
- ‚úÖ **Pruning th√¥ng minh**: D·ª´ng s·ªõm c√°c trial kh√¥ng c√≥ tri·ªÉn v·ªçng
- ‚úÖ **Parallel processing**: Ch·∫°y song song nhi·ªÅu th√≠ nghi·ªám
- ‚úÖ **Visualization**: Hi·ªÉn th·ªã qu√° tr√¨nh t·ªëi ∆∞u h√≥a
- ‚úÖ **T∆∞∆°ng th√≠ch cao**: H·ªó tr·ª£ h·∫ßu h·∫øt c√°c m√¥ h√¨nh ML

---

## üîß C√†i ƒë·∫∑t

### C√†i ƒë·∫∑t c∆° b·∫£n:
```bash
pip install optuna
```

### C√†i ƒë·∫∑t v·ªõi c√°c t√πy ch·ªçn m·ªü r·ªông:
```bash
# V·ªõi visualization
pip install optuna[visualization]

# V·ªõi integration cho LightGBM
pip install optuna[lightgbm]

# V·ªõi integration cho XGBoost
pip install optuna[xgboost]

# C√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß
pip install optuna[all]
```

### Ki·ªÉm tra c√†i ƒë·∫∑t:
```python
import optuna
print(f"Optuna version: {optuna.__version__}")
```

---

## üèóÔ∏è C·∫•u tr√∫c c∆° b·∫£n

### Template c∆° b·∫£n:
```python
import optuna
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

def objective(trial):
    # 1. ƒê·ªãnh nghƒ©a hyperparameters
    param1 = trial.suggest_float('param1', 0.0, 1.0)
    param2 = trial.suggest_int('param2', 1, 100)
    
    # 2. T·∫°o model v·ªõi tham s·ªë
    model = YourModel(param1=param1, param2=param2)
    
    # 3. Train v√† evaluate
    scores = cross_val_score(model, X_train, y_train, cv=5)
    
    # 4. Return metric ƒë·ªÉ optimize
    return scores.mean()

# 5. T·∫°o study v√† optimize
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# 6. L·∫•y k·∫øt qu·∫£
print(f"Best score: {study.best_value}")
print(f"Best params: {study.best_params}")
```

---

## üöÄ XGBoost v·ªõi Optuna

### C√†i ƒë·∫∑t:
```bash
pip install xgboost optuna
```

### Code m·∫´u:
```python
import optuna
import xgboost as xgb
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

def xgb_objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'random_state': 42
    }
    
    model = xgb.XGBClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    return scores.mean()

# T·∫°o study
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(seed=42),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
)

# Optimize
study.optimize(xgb_objective, n_trials=100, timeout=3600)

# K·∫øt qu·∫£
print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")

# Train model cu·ªëi c√πng
best_model = xgb.XGBClassifier(**study.best_params)
best_model.fit(X_train, y_train)
```

---

## üí° LightGBM v·ªõi Optuna

### C√†i ƒë·∫∑t:
```bash
pip install lightgbm optuna
```

### Code m·∫´u:
```python
import optuna
import lightgbm as lgb
from sklearn.model_selection import cross_val_score

def lgb_objective(trial):
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 10, 300),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'random_state': 42,
        'verbose': -1
    }
    
    # S·ª≠ d·ª•ng LightGBM Dataset
    train_data = lgb.Dataset(X_train, label=y_train)
    model = lgb.train(params, train_data, num_boost_round=1000, valid_sets=[train_data], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
    
    # Predict v√† t√≠nh accuracy
    y_pred = model.predict(X_val, num_iteration=model.best_iteration)
    y_pred_binary = (y_pred > 0.5).astype(int)
    accuracy = accuracy_score(y_val, y_pred_binary)
    
    return accuracy

# T·∫°o study
study = optuna.create_study(direction='maximize')
study.optimize(lgb_objective, n_trials=100)

print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

---

## üå≤ Random Forest v·ªõi Optuna

### Code m·∫´u:
```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

def rf_objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
        'random_state': 42
    }
    
    model = RandomForestClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    return scores.mean()

# T·∫°o study
study = optuna.create_study(direction='maximize')
study.optimize(rf_objective, n_trials=100)

print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

---

## üìà Gradient Boosting v·ªõi Optuna

### Code m·∫´u:
```python
import optuna
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score

def gb_objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'random_state': 42
    }
    
    model = GradientBoostingClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    return scores.mean()

# T·∫°o study
study = optuna.create_study(direction='maximize')
study.optimize(gb_objective, n_trials=100)

print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

---

## üéØ AdaBoost v·ªõi Optuna

### Code m·∫´u:
```python
import optuna
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

def ada_objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 2.0, log=True),
        'algorithm': trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R']),
        'random_state': 42
    }
    
    # Base estimator v·ªõi hyperparameters
    base_estimator = DecisionTreeClassifier(
        max_depth=trial.suggest_int('max_depth', 1, 10),
        min_samples_split=trial.suggest_int('min_samples_split', 2, 20),
        random_state=42
    )
    
    model = AdaBoostClassifier(estimator=base_estimator, **params)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    return scores.mean()

# T·∫°o study
study = optuna.create_study(direction='maximize')
study.optimize(ada_objective, n_trials=100)

print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

---

## üìä Visualization

### C√†i ƒë·∫∑t visualization:
```bash
pip install optuna[visualization]
```

### C√°c lo·∫°i bi·ªÉu ƒë·ªì:

#### 1. Optimization History:
```python
import optuna.visualization as vis

# L·ªãch s·ª≠ t·ªëi ∆∞u h√≥a
vis.plot_optimization_history(study)
```

#### 2. Parameter Importance:
```python
# T·∫ßm quan tr·ªçng c·ªßa t·ª´ng parameter
vis.plot_param_importances(study)
```

#### 3. Parallel Coordinate:
```python
# Xem t·∫•t c·∫£ trials tr√™n 1 bi·ªÉu ƒë·ªì
vis.plot_parallel_coordinate(study)
```

#### 4. Slice Plot:
```python
# Xem t·ª´ng parameter ri√™ng l·∫ª
vis.plot_slice(study)
```

#### 5. Contour Plot:
```python
# Xem t∆∞∆°ng quan gi·ªØa 2 parameters
vis.plot_contour(study, params=['param1', 'param2'])
```

---

## üèÜ Best Practices

### 1. **Ch·ªçn Sampler ph√π h·ª£p:**
```python
# TPESampler - t·ªët nh·∫•t cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p
sampler = optuna.samplers.TPESampler(seed=42)

# RandomSampler - ƒë∆°n gi·∫£n, nhanh
sampler = optuna.samplers.RandomSampler(seed=42)

# GridSampler - khi bi·∫øt r√µ kh√¥ng gian t√¨m ki·∫øm
sampler = optuna.samplers.GridSampler(search_space)
```

### 2. **S·ª≠ d·ª•ng Pruning:**
```python
# MedianPruner - c√¢n b·∫±ng t·ªët
pruner = optuna.pruners.MedianPruner(
    n_startup_trials=5,      # S·ªë trials ƒë·∫ßu kh√¥ng prune
    n_warmup_steps=10,       # S·ªë steps warmup
    interval_steps=1         # T·∫ßn su·∫•t check
)

# SuccessiveHalvingPruner - cho large-scale
pruner = optuna.pruners.SuccessiveHalvingPruner()
```

### 3. **Timeout v√† n_trials:**
```python
# K·∫øt h·ª£p c·∫£ hai
study.optimize(objective, n_trials=1000, timeout=3600)  # 1 gi·ªù ho·∫∑c 1000 trials
```

### 4. **Logging v√† Callback:**
```python
def callback(study, trial):
    print(f"Trial {trial.number}: {trial.value:.4f}")

study.optimize(objective, n_trials=100, callbacks=[callback])
```

### 5. **Resume Study:**
```python
# L∆∞u study
study = optuna.create_study(study_name='my_study', storage='sqlite:///study.db')

# Resume t·ª´ checkpoint
study = optuna.load_study(study_name='my_study', storage='sqlite:///study.db')
```

---

## üîß Troubleshooting

### L·ªói th∆∞·ªùng g·∫∑p:

#### 1. **Memory Error:**
```python
# Gi·∫£m n_trials ho·∫∑c tƒÉng timeout
study.optimize(objective, n_trials=50, timeout=1800)
```

#### 2. **Slow Convergence:**
```python
# TƒÉng n_startup_trials
pruner = optuna.pruners.MedianPruner(n_startup_trials=20)
```

#### 3. **Overfitting:**
```python
# S·ª≠ d·ª•ng cross-validation
scores = cross_val_score(model, X_train, y_train, cv=5)
return scores.mean()
```

---

## üìù Template ho√†n ch·ªânh

```python
import optuna
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
import xgboost as xgb

def objective(trial):
    # ƒê·ªãnh nghƒ©a hyperparameters
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': 42
    }
    
    # T·∫°o model
    model = xgb.XGBClassifier(**params)
    
    # Cross-validation
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    
    return scores.mean()

# T·∫°o study
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(seed=42),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5)
)

# Optimize
study.optimize(objective, n_trials=100, timeout=3600)

# K·∫øt qu·∫£
print(f"Best accuracy: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")

# Train model cu·ªëi c√πng
best_model = xgb.XGBClassifier(**study.best_params)
best_model.fit(X_train, y_train)
```

---

## üéâ K·∫øt lu·∫≠n

Optuna l√† c√¥ng c·ª• m·∫°nh m·∫Ω gi√∫p t·ªëi ∆∞u h√≥a hyperparameters t·ª± ƒë·ªông cho c√°c m√¥ h√¨nh c√¢y. V·ªõi h∆∞·ªõng d·∫´n n√†y, b·∫°n c√≥ th·ªÉ:

- ‚úÖ C√†i ƒë·∫∑t v√† s·ª≠ d·ª•ng Optuna
- ‚úÖ √Åp d·ª•ng cho c√°c m√¥ h√¨nh c√¢y ph·ªï bi·∫øn
- ‚úÖ S·ª≠ d·ª•ng visualization ƒë·ªÉ ph√¢n t√≠ch
- ‚úÖ √Åp d·ª•ng best practices

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi Optuna! üöÄ**
