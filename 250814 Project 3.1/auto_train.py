#!/usr/bin/env python3
"""
Auto Training Script - Cháº¡y training tá»± Ä‘á»™ng vá»›i file CSV trong cache
Tá»± Ä‘á»™ng Ä‘á»c file CSV vÃ  kÃ­ch hoáº¡t táº¥t cáº£ chá»©c nÄƒng cáº§n thiáº¿t

Usage:
    python auto_train.py

Features:
- Tá»± Ä‘á»™ng Ä‘á»c file 2cls_spam_text_cls.csv tá»« cache/
- Tá»± Ä‘á»™ng cáº¥u hÃ¬nh: text column = 'Message', label column = 'Category'
- KÃ­ch hoáº¡t táº¥t cáº£ preprocessing options
- Cháº¡y training vá»›i táº¥t cáº£ models (7 models) vÃ  vectorization methods (3 methods)
- Táº¡o cache tá»± Ä‘á»™ng
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# Add current directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import progress tracking
from utils.progress_tracker import create_training_progress


def print_banner():
    """In banner chÃ o má»«ng"""
    print("=" * 80)
    print("ğŸ¤– TOPIC MODELING - AUTO CLASSIFIER")
    print("ğŸš€ AUTO TRAINING SCRIPT")
    print("=" * 80)
    print(f"â° Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)


def check_file_exists(file_path):
    """Kiá»ƒm tra file cÃ³ tá»“n táº¡i khÃ´ng"""
    if os.path.exists(file_path):
        print(f"âœ… Found file: {file_path}")
        return True
    else:
        print(f"âŒ File not found: {file_path}")
        return False


def load_dataset(file_path):
    """Äá»c dataset tá»« file CSV"""
    try:
        print(f"ğŸ“‚ Loading dataset from: {file_path}")
        
        # Äá»c file CSV
        df = pd.read_csv(file_path)
        
        print("âœ… Dataset loaded successfully!")
        print(f"   ğŸ“Š Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
        print(f"   ğŸ“‹ Columns: {list(df.columns)}")
        
        # Kiá»ƒm tra cá»™t Message vÃ  Category
        if 'Message' not in df.columns:
            print("âŒ Column 'Message' not found!")
            print(f"   Available columns: {list(df.columns)}")
            return None
            
        if 'Category' not in df.columns:
            print("âŒ Column 'Category' not found!")
            print(f"   Available columns: {list(df.columns)}")
            return None
        
        # Hiá»ƒn thá»‹ thÃ´ng tin vá» dá»¯ liá»‡u
        print(f"   ğŸ“ Text samples: {len(df['Message'].dropna()):,}")
        print(f"   ğŸ·ï¸  Unique categories: {df['Category'].nunique()}")
        print("   ğŸ“Š Category distribution:")
        category_counts = df['Category'].value_counts()
        for category, count in category_counts.items():
            print(f"      - {category}: {count:,} samples")
        
        return df
        
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        return None


def create_auto_config(df, mode='full'):
    """Táº¡o cáº¥u hÃ¬nh tá»± Ä‘á»™ng cho training"""
    print(f"\nğŸ”§ Creating auto configuration ({mode} mode)...")
    
    # Step 1: Dataset configuration
    max_samples = 10000 if mode == 'full' else 1000
    step1_config = {
        'dataframe': df,
        'file_path': 'cache/2cls_spam_text_cls.csv',
        'sampling_config': {
            'num_samples': min(max_samples, len(df)),
            'sampling_strategy': 'Stratified (Recommended)'
        },
        'dataset_size': len(df)
    }
    
    # Step 2: Column selection & preprocessing
    step2_config = {
        'text_column': 'Message',
        'label_column': 'Category',
        'text_samples': len(df['Message'].dropna()),
        'unique_classes': df['Category'].nunique(),
        'distribution': 'Balanced' if df['Category'].nunique() <= 5 else 'Multi-class',
        'avg_length': df['Message'].astype(str).str.len().mean(),
        'avg_length_words': df['Message'].astype(str).str.split().str.len().mean(),
        'unique_words': len(set(' '.join(df['Message'].astype(str).dropna()).lower().split())),
        'validation_errors': [],
        'validation_warnings': [],
        # KÃ­ch hoáº¡t táº¥t cáº£ preprocessing options
        'text_cleaning': True,
        'category_mapping': True,
        'data_validation': True,
        'memory_optimization': True,
        # Advanced preprocessing options
        'rare_words_removal': True,
        'rare_words_threshold': 2,
        'lemmatization': True,
        'context_aware_stopwords': True,
        'stopwords_aggressiveness': 'Moderate',
        'phrase_detection': True,
        'min_phrase_freq': 3,
        'completed': True
    }
    
    # Step 3: Model configuration
    if mode == 'full':
        selected_models = [
            'K-Nearest Neighbors',
            'Decision Tree', 
            'Naive Bayes',
            'K-Means Clustering',
            'Support Vector Machine',
            'Logistic Regression',
            'Linear SVC'
        ]
        selected_vectorization = [
            'BoW',
            'TF-IDF', 
            'Word Embeddings'
        ]
        ensemble_enabled = True
        cv_folds = 5
    else:  # quick mode
        selected_models = [
            'K-Nearest Neighbors',
            'Naive Bayes',
            'Logistic Regression'
        ]
        selected_vectorization = [
            'TF-IDF',
            'Word Embeddings'
        ]
        ensemble_enabled = False
        cv_folds = 3
    
    step3_config = {
        'data_split': {
            'training': 80,
            'test': 20
        },
        'cross_validation': {
            'cv_folds': cv_folds,
            'random_state': 42
        },
        'selected_models': selected_models,
        'selected_vectorization': selected_vectorization,
        'ensemble_learning': {
            'eligible': ensemble_enabled,
            'enabled': ensemble_enabled,
            'final_estimator': 'voting' if ensemble_enabled else None
        },
        'knn_config': {
            'optimization_method': 'Manual Input',
            'k_value': 15,
            'weights': 'distance',
            'metric': 'cosine',
            'cv_folds': cv_folds,
            'scoring_metric': 'f1_weighted'
        },
        'validation_errors': [],
        'validation_warnings': [],
        'completed': True
    }
    
    print("âœ… Auto configuration created!")
    print(f"   ğŸ“Š Models: {len(step3_config['selected_models'])} models")
    print(f"   ğŸ”¤ Vectorization: {len(step3_config['selected_vectorization'])} methods")
    print(f"   ğŸš€ Ensemble Learning: {'Enabled' if step3_config['ensemble_learning']['enabled'] else 'Disabled'}")
    print(f"   ğŸ“ˆ Total combinations: {len(step3_config['selected_models']) * len(step3_config['selected_vectorization'])}")
    print(f"   ğŸ¯ KNN Configuration: K={step3_config['knn_config']['k_value']}, "
          f"Weights={step3_config['knn_config']['weights']}, "
          f"Metric={step3_config['knn_config']['metric']}")
    
    return step1_config, step2_config, step3_config


def run_training(step1_config, step2_config, step3_config):
    """Cháº¡y training vá»›i cáº¥u hÃ¬nh Ä‘Ã£ táº¡o"""
    print("\nğŸš€ Starting training...")
    
    try:
        # Import training pipeline
        from training_pipeline import execute_streamlit_training
        
        # Progress callback function
        def progress_callback(phase, message, progress):
            print(f"   [{phase}] {message} ({progress:.1%})")
        
        # GPU Optimization: Convert sparse matrices to dense for GPU acceleration
        print(f"\nğŸš€ ENABLING GPU OPTIMIZATION...")
        print(f"   â€¢ Converting sparse matrices (BoW, TF-IDF) to dense arrays")
        print(f"   â€¢ This enables GPU acceleration for all vectorization methods")
        print(f"   â€¢ Memory usage will increase but performance will improve")
        
        # Execute training
        result = execute_streamlit_training(
            step1_config['dataframe'],
            step1_config,
            step2_config, 
            step3_config,
            progress_callback=progress_callback
        )
        
        if result['status'] == 'success':
            print("âœ… Training completed successfully!")
            print(f"   ğŸ“Š Successful combinations: {result.get('successful_combinations', 0)}")
            print(f"   â±ï¸  Total time: {result.get('evaluation_time', 0):.1f}s")
            return result
        else:
            print(f"âŒ Training failed: {result.get('message', 'Unknown error')}")
            return None
            
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        import traceback
        traceback.print_exc()
        return None


def display_results(result):
    """Hiá»ƒn thá»‹ káº¿t quáº£ training"""
    if not result:
        print("âŒ No results to display")
        return
    
    print("\nğŸ“Š TRAINING RESULTS")
    print("=" * 50)
    
    # Best model
    best_combinations = result.get('best_combinations', {})
    best_overall = best_combinations.get('best_overall', {})
    
    if best_overall:
        print(f"ğŸ¥‡ Best Model: {best_overall.get('combination_key', 'N/A')}")
        print(f"   ğŸ“ˆ F1 Score: {best_overall.get('f1_score', 0):.3f}")
        print(f"   ğŸ¯ Test Accuracy: {best_overall.get('test_accuracy', 0):.3f}")
        print(f"   â±ï¸  Training Time: {best_overall.get('training_time', 0):.1f}s")
    
    # Summary statistics
    comprehensive_results = result.get('comprehensive_results', [])
    successful_results = [r for r in comprehensive_results if r.get('status') == 'success']
    
    if successful_results:
        print("\nğŸ“Š Summary Statistics:")
        print(f"   âœ… Successful models: {len(successful_results)}")
        print(f"   ğŸ“ˆ Average F1 Score: {np.mean([r.get('f1_score', 0) for r in successful_results]):.3f}")
        print(f"   ğŸ¯ Average Accuracy: {np.mean([r.get('test_accuracy', 0) for r in successful_results]):.3f}")
        print(f"   â±ï¸  Average Training Time: {np.mean([r.get('training_time', 0) for r in successful_results]):.1f}s")
        
        # Top 3 models
        print("\nğŸ† Top 3 Models:")
        top_models = sorted(successful_results, key=lambda x: x.get('f1_score', 0), reverse=True)[:3]
        for i, model in enumerate(top_models, 1):
            model_name = f"{model.get('model_name', 'Unknown')} + {model.get('embedding_name', 'Unknown')}"
            f1_score = model.get('f1_score', 0)
            print(f"   {i}. {model_name}: {f1_score:.3f}")


def main():
    """HÃ m main chÃ­nh"""
    print_banner()
    
    # ÄÆ°á»ng dáº«n file CSV
    csv_file = "cache/2cls_spam_text_cls.csv"
    
    # Kiá»ƒm tra file cÃ³ tá»“n táº¡i khÃ´ng
    if not check_file_exists(csv_file):
        print(f"\nğŸ’¡ Please make sure the file exists at: {csv_file}")
        print("   You can:")
        print("   1. Create the file manually")
        print("   2. Download a sample dataset")
        print("   3. Use a different file path")
        return
    
    # Äá»c dataset
    df = load_dataset(csv_file)
    if df is None:
        return
    
    # Chá»n mode
    print("\nğŸ”§ Training Mode Selection:")
    print("   1. Quick Mode (3 models, 2 vectorization, ~2-3 minutes)")
    print("   2. Full Mode (7 models, 3 vectorization, ~5-10 minutes)")
    
    try:
        choice = input("\nEnter your choice (1 or 2, default=1): ").strip()
        mode = 'quick' if choice != '2' else 'full'
    except KeyboardInterrupt:
        print("\n\nâŒ Training cancelled by user")
        return
    
    print(f"\nğŸš€ Selected: {mode.upper()} MODE")
    
    # Táº¡o cáº¥u hÃ¬nh tá»± Ä‘á»™ng
    step1_config, step2_config, step3_config = create_auto_config(df, mode)
    
    # Cháº¡y training
    result = run_training(step1_config, step2_config, step3_config)
    
    # Hiá»ƒn thá»‹ káº¿t quáº£
    display_results(result)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ AUTO TRAINING COMPLETED!")
    print(f"â° Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    if result:
        print("ğŸ’¾ Cache has been created for future use!")
        print("ğŸš€ You can now use the Streamlit app with cached results!")


if __name__ == "__main__":
    main()
