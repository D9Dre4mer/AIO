"""
New Model Trainer using modular architecture with validation and cross-validation support
"""

from typing import Dict, Any, Union, Tuple, List
import numpy as np
from scipy import sparse

# Will receive instances via constructor
from .base.metrics import ModelMetrics


class NewModelTrainer:
    """New model trainer using modular architecture with validation and cross-validation"""
    
    def __init__(self, test_size: float = 0.2, validation_size: float = 0.2, 
                 cv_folds: int = 5, cv_stratified: bool = True,
                 model_factory=None, validation_manager=None):
        """Initialize new model trainer with validation and cross-validation support"""
        # Models are now registered in models/__init__.py
        
        # Set validation parameters
        self.test_size = test_size
        self.validation_size = validation_size
        
        # Set cross-validation parameters
        self.cv_folds = cv_folds
        self.cv_stratified = cv_stratified
        
        # Store instances
        self.model_factory = model_factory
        self.validation_manager = validation_manager
        
        # Update validation manager with new CV parameters if provided
        if self.validation_manager:
            self.validation_manager.set_cv_parameters(cv_folds, cv_stratified)
    
    def cross_validate_model(
        self,
        model_name: str,
        X: Union[np.ndarray, sparse.csr_matrix],
        y: np.ndarray,
        metrics: List[str] = None,
        **model_params
    ) -> Dict[str, Any]:
        """Perform cross-validation on a specific model"""
        
        # Create model instance
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        model = self.model_factory.create_model(model_name, **model_params)
        
        # Perform cross-validation
        if not self.validation_manager:
            raise ValueError("Validation manager not set. Please provide validation_manager in constructor.")
        cv_results = self.validation_manager.cross_validate_model(model, X, y, metrics)
        
        # Print summary
        self.validation_manager.print_cv_summary(cv_results)
        
        # Get recommendations
        recommendations = self.validation_manager.get_cv_recommendations(cv_results)
        if recommendations:
            print("\nüí° Recommendations:")
            for rec in recommendations:
                print(f"  {rec}")
        
        return cv_results
    
    def cross_validate_all_models(
        self,
        X: Union[np.ndarray, sparse.csr_matrix],
        y: np.ndarray,
        metrics: List[str] = None
    ) -> Dict[str, Dict[str, Any]]:
        """Perform cross-validation on all available models"""
        
        results = {}
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        available_models = self.model_factory.get_available_models()
        
        print(f"üîÑ Cross-Validating {len(available_models)} models...")
        
        for model_name in available_models:
            try:
                print(f"\n{'='*50}")
                print(f"üìä {model_name.upper()} - Cross-Validation")
                print(f"{'='*50}")
                
                cv_result = self.cross_validate_model(model_name, X, y, metrics)
                results[model_name] = cv_result
                
            except Exception as e:
                print(f"‚ùå Error with {model_name}: {e}")
                continue
        
        return results
    
    def compare_cv_results(
        self,
        cv_results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Compare cross-validation results across models"""
        
        comparison = {}
        
        for model_name, result in cv_results.items():
            overall = result['overall_results']
            comparison[model_name] = {
                'accuracy_mean': overall['accuracy_mean'],
                'accuracy_std': overall['accuracy_std'],
                'accuracy_range': overall['accuracy_max'] - overall['accuracy_min'],
                'stability_score': 1.0 - overall['accuracy_std']  # Higher is more stable
            }
        
        # Sort by accuracy (descending)
        sorted_by_accuracy = sorted(
            comparison.items(),
            key=lambda x: x[1]['accuracy_mean'],
            reverse=True
        )
        
        # Sort by stability (descending)
        sorted_by_stability = sorted(
            comparison.items(),
            key=lambda x: x[1]['stability_score'],
            reverse=True
        )
        
        return {
            'model_comparison': comparison,
            'sorted_by_accuracy': sorted_by_accuracy,
            'sorted_by_stability': sorted_by_stability,
            'best_accuracy_model': sorted_by_accuracy[0] if sorted_by_accuracy else None,
            'most_stable_model': sorted_by_stability[0] if sorted_by_stability else None
        }
    
    def print_cv_comparison(self, cv_results: Dict[str, Dict[str, Any]]) -> None:
        """Print comprehensive cross-validation comparison"""
        
        print("\n" + "="*60)
        print("üèÜ CROSS-VALIDATION MODEL COMPARISON")
        print("="*60)
        
        comparison = self.compare_cv_results(cv_results)
        
        print("\nüìä Model Performance Ranking (by Mean Accuracy):")
        for i, (model_name, metrics) in enumerate(comparison['sorted_by_accuracy'], 1):
            acc_mean = metrics['accuracy_mean']
            acc_std = metrics['accuracy_std']
            stability = metrics['stability_score']
            
            stability_indicator = "‚úÖ" if stability > 0.9 else "‚ö†Ô∏è" if stability > 0.8 else "‚ùå"
            
            print(f"{i:2d}. {model_name:15s}: "
                  f"Acc: {acc_mean:.4f} ¬± {acc_std:.4f} "
                  f"{stability_indicator} (Stability: {stability:.3f})")
        
        print(f"\nüéØ Best Accuracy Model: {comparison['best_accuracy_model'][0]}")
        print(f"üõ°Ô∏è Most Stable Model: {comparison['most_stable_model'][0]}")
        
        # Stability analysis
        print("\nüìà Stability Analysis:")
        for model_name, metrics in comparison['model_comparison'].items():
            stability = metrics['stability_score']
            if stability > 0.9:
                print(f"‚úÖ {model_name}: Very stable ({stability:.3f})")
            elif stability > 0.8:
                print(f"‚ö†Ô∏è  {model_name}: Moderately stable ({stability:.3f})")
            else:
                print(f"‚ùå {model_name}: Unstable ({stability:.3f})")
    
    # Existing methods (keep for backward compatibility)
    def train_validate_test_model(
        self,
        model_name: str,
        X: Union[np.ndarray, sparse.csr_matrix],
        y: np.ndarray,
        X_val: Union[np.ndarray, sparse.csr_matrix] = None,
        y_val: np.ndarray = None,
        X_test: Union[np.ndarray, sparse.csr_matrix] = None,
        y_test: np.ndarray = None,
        **model_params
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float, float, Dict[str, Any]]:
        """Train, validate and test a specific model with 3-way split"""
        
        # Use provided split data or create new split
        if X_val is not None and y_val is not None and X_test is not None and y_test is not None:
            # Use provided split data
            X_train, y_train = X, y
            print(f"üìä Using provided data split:")
            print(f"   ‚Ä¢ Train: {X_train.shape[0] if hasattr(X_train, 'shape') else len(X_train)} | Val: {X_val.shape[0] if hasattr(X_val, 'shape') else len(X_val)} | Test: {X_test.shape[0] if hasattr(X_test, 'shape') else len(X_test)}")
        else:
            # Split data into train/validation/test
            if not self.validation_manager:
                raise ValueError("Validation manager not set. Please provide validation_manager in constructor.")
            X_train, X_val, X_test, y_train, y_val, y_test = self.validation_manager.split_data(
                X, y, stratify=y
            )
            
            # Print split summary
            split_info = self.validation_manager.get_split_info(X, y)
            self.validation_manager.print_split_summary(split_info)
        
        # Create model instance
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        model = self.model_factory.create_model(model_name, **model_params)
        
        # Train model
        print(f"\nüöÄ Training {model_name} model...")
        model.fit(X_train, y_train)
        
        # Validate model
        print(f"üîç Validating {model_name} model...")
        val_metrics = model.validate(X_val, y_val)
        val_accuracy = val_metrics['accuracy']
        y_val_pred = model.predict(X_val)
        
        # Test model
        print(f"üß™ Testing {model_name} model...")
        y_test_pred = model.predict(X_test)
        test_metrics = ModelMetrics.compute_classification_metrics(y_test, y_test_pred)
        test_accuracy = test_metrics['accuracy']
        
        return y_test_pred, y_val_pred, y_test, val_accuracy, test_accuracy, test_metrics
    
    def train_validate_test_all_models(
        self,
        X: Union[np.ndarray, sparse.csr_matrix],
        y: np.ndarray
    ) -> Dict[str, Dict[str, Any]]:
        """Train, validate and test all available models"""
        
        results = {}
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        available_models = self.model_factory.get_available_models()
        
        print(f"üöÄ Training {len(available_models)} models with validation...")
        
        for model_name in available_models:
            try:
                print(f"\n{'='*50}")
                print(f"üìä {model_name.upper()}")
                print(f"{'='*50}")
                
                y_test_pred, y_val, y_test, val_acc, test_acc, test_metrics = \
                    self.train_validate_test_model(model_name, X, y)
                
                results[model_name] = {
                    'predictions': y_test_pred,
                    'validation_accuracy': val_acc,
                    'test_accuracy': test_acc,
                    'test_metrics': test_metrics,
                    'validation_predictions': y_val,
                    'test_ground_truth': y_test
                }
                
                print(f"‚úÖ {model_name}: Validation Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}")
                
            except Exception as e:
                print(f"‚ùå Error with {model_name}: {e}")
                continue
        
        return results
    
    def get_model_comparison_with_validation(
        self,
        results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Compare performance of all models including validation"""
        
        # Extract accuracies
        model_comparison = {}
        for model_name, result in results.items():
            model_comparison[model_name] = {
                'validation_accuracy': result['validation_accuracy'],
                'test_accuracy': result['test_accuracy'],
                'overfitting_score': result['validation_accuracy'] - result['test_accuracy']
            }
        
        # Sort by test accuracy
        sorted_by_test = sorted(
            model_comparison.items(),
            key=lambda x: x[1]['test_accuracy'],
            reverse=True
        )
        
        # Sort by validation accuracy
        sorted_by_val = sorted(
            model_comparison.items(),
            key=lambda x: x[1]['validation_accuracy'],
            reverse=True
        )
        
        return {
            'model_comparison': model_comparison,
            'sorted_by_test': sorted_by_test,
            'sorted_by_validation': sorted_by_val,
            'best_test_model': sorted_by_test[0] if sorted_by_test else None,
            'best_validation_model': sorted_by_val[0] if sorted_by_val else None
        }
    
    def print_validation_summary(self, results: Dict[str, Dict[str, Any]]) -> None:
        """Print comprehensive validation summary"""
        
        print("\n" + "="*60)
        print("üìä COMPREHENSIVE VALIDATION SUMMARY")
        print("="*60)
        
        comparison = self.get_model_comparison_with_validation(results)
        
        print("\nüèÜ Model Performance Ranking (by Test Accuracy):")
        for i, (model_name, metrics) in enumerate(comparison['sorted_by_test'], 1):
            overfitting = metrics['overfitting_score']
            overfitting_indicator = "‚ö†Ô∏è" if overfitting > 0.05 else "‚úÖ"
            print(f"{i:2d}. {model_name:15s}: "
                  f"Val: {metrics['validation_accuracy']:.4f}, "
                  f"Test: {metrics['test_accuracy']:.4f} "
                  f"{overfitting_indicator} (Overfitting: {overfitting:+.4f})")
        
        print(f"\nüéØ Best Test Model: {comparison['best_test_model'][0]}")
        print(f"üîç Best Validation Model: {comparison['best_validation_model'][0]}")
        
        # Overfitting analysis
        print("\nüìà Overfitting Analysis:")
        for model_name, metrics in comparison['model_comparison'].items():
            overfitting = metrics['overfitting_score']
            if overfitting > 0.05:
                print(f"‚ö†Ô∏è  {model_name}: High overfitting ({overfitting:+.4f})")
            elif overfitting < -0.05:
                print(f"üìâ {model_name}: Underfitting ({overfitting:+.4f})")
            else:
                print(f"‚úÖ {model_name}: Good generalization ({overfitting:+.4f})")
    
    # Keep existing methods for backward compatibility
    def train_and_test_model(self, *args, **kwargs):
        """Backward compatibility method - returns (y_pred, accuracy, report)"""
        y_test_pred, y_val, y_test, val_acc, test_acc, test_metrics = \
            self.train_validate_test_model(*args, **kwargs)
        return y_test_pred, test_acc, test_metrics
    
    def train_and_test_all_models(self, *args, **kwargs):
        """Backward compatibility method"""
        return self.train_validate_test_all_models(*args, **kwargs)
    
    def get_model_comparison(self, *args, **kwargs):
        """Backward compatibility method"""
        return self.get_model_comparison_with_validation(*args, **kwargs)
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Get information about a specific model"""
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        return self.model_factory.get_model_info(model_name)
    
    def list_available_models(self) -> list:
        """List all available models"""
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        return self.model_factory.get_available_models()
    
    def suggest_models_for_task(
        self, 
        task_type: str = None, 
        data_type: str = None
    ) -> list:
        """Suggest models for specific task and data type"""
        if not self.model_factory:
            raise ValueError("Model factory not set. Please provide model_factory in constructor.")
        return self.model_factory.suggest_models(task_type, data_type)
